{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#current-release-info","title":"Current release info","text":"Name Downloads Version Platforms"},{"location":"#conda-forge-feedstock","title":"conda-forge feedstock","text":"<p>Conda-forge feedstock</p>"},{"location":"#pyramids-gis-utility-package","title":"pyramids - GIS utility package","text":"<p>pyramids is a GIS utility package using gdal, ....</p> <p>pyramids</p> <p></p>"},{"location":"#main-features","title":"Main Features","text":"<ul> <li>GIS modules to enable the modeler to fully prepare the meteorological inputs and do all the preprocessing   needed to build the model (align rasters with the DEM), in addition to various methods to manipulate and   convert different forms of distributed data (rasters, NetCDF, shapefiles)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Conda (conda-forge): Installing <code>pyramids</code> from the <code>conda-forge</code> channel can be achieved by:</li> </ul> <pre><code>conda install -c conda-forge pyramids\n</code></pre> <p>It is possible to list all the versions of <code>pyramids</code> available on your platform with:</p> <pre><code>conda search pyramids --channel conda-forge\n</code></pre> <ul> <li>pip (PyPI):</li> </ul> <p>to install the last release, you can easily use pip</p> <pre><code>pip install pyramids-gis\n</code></pre> <ul> <li>From source (latest):</li> </ul> <p>to install the last development to time, you can install the library from GitHub</p> <pre><code>pip install git+https://github.com/Serapieum-of-alex/pyramids\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#minimal-example-open-a-dataset-and-inspect-metadata","title":"Minimal example: open a dataset and inspect metadata","text":"<pre><code>from pyramids.dataset import Dataset\n\n# Use your own raster path (GeoTIFF/ASC/NetCDF supported); here we show a relative test file\npath = \"tests/data/geotiff/dem.tif\"  # adjust path as needed\n\nds = Dataset.read_file(path)\nprint(ds.width, ds.height, ds.transform)\nprint(ds.meta)\n\n# Access array data\narr = ds.read()\nprint(arr.shape, arr.dtype)\n\n# Save a single band to a new GeoTIFF (writes alongside input by default)\nout = \"./dem_copy.tif\"\nds.to_file(out)\nprint(\"Saved to\", out)\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Explore the Tutorials for end-to-end workflows.</li> <li>See How it works for architecture and data flow.</li> <li>Browse the API Reference for details of classes and functions.</li> </ul>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Mostafa Farrag moah.farag@gmail.com</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Citizen Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#1-purpose","title":"1. Purpose","text":"<p>A primary goal of Hapi is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof).</p> <p>This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior.</p> <p>We invite all those who participate in Hapi to help us create safe and positive experiences for everyone.</p>"},{"location":"CODE_OF_CONDUCT/#2-open-sourceculturetech-citizenship","title":"2. Open [Source/Culture/Tech] Citizenship","text":"<p>A supplemental goal of this Code of Conduct is to increase open [source/culture/tech] citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community.</p> <p>Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society.</p> <p>If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know.</p>"},{"location":"CODE_OF_CONDUCT/#3-expected-behavior","title":"3. Expected Behavior","text":"<p>The following behaviors are expected and requested of all community members:</p> <ul> <li>Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community.</li> <li>Exercise consideration and respect in your speech and actions.</li> <li>Attempt collaboration before conflict.</li> <li>Refrain from demeaning, discriminatory, or harassing behavior and speech.</li> <li>Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential.</li> <li>Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#4-unacceptable-behavior","title":"4. Unacceptable Behavior","text":"<p>The following behaviors are considered harassment and are unacceptable within our community:</p> <ul> <li>Violence, threats of violence or violent language directed against another person.</li> <li>Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language.</li> <li>Posting or displaying sexually explicit or violent material.</li> <li>Posting or threatening to post other people's personally identifying information (\"doxing\").</li> <li>Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability.</li> <li>Inappropriate photography or recording.</li> <li>Inappropriate physical contact. You should have someone's consent before touching them.</li> <li>Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances.</li> <li>Deliberate intimidation, stalking or following (online or in person).</li> <li>Advocating for, or encouraging, any of the above behavior.</li> <li>Sustained disruption of community events, including talks and presentations.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#5-weapons-policy","title":"5. Weapons Policy","text":"<p>No weapons will be allowed at Hapi events, community spaces, or in other spaces covered by the scope of this Code of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. Community members are further expected to comply with all state and local laws on this matter.</p>"},{"location":"CODE_OF_CONDUCT/#6-consequences-of-unacceptable-behavior","title":"6. Consequences of Unacceptable Behavior","text":"<p>Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated.</p> <p>Anyone asked to stop unacceptable behavior is expected to comply immediately.</p> <p>If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event).</p>"},{"location":"CODE_OF_CONDUCT/#7-reporting-guidelines","title":"7. Reporting Guidelines","text":"<p>If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. .</p> <p>Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress.</p>"},{"location":"CODE_OF_CONDUCT/#8-addressing-grievances","title":"8. Addressing Grievances","text":"<p>If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify  with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies.</p>"},{"location":"CODE_OF_CONDUCT/#9-scope","title":"9. Scope","text":"<p>We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues--online and in-person--as well as in all one-on-one communications pertaining to community business.</p> <p>This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members.</p>"},{"location":"CODE_OF_CONDUCT/#10-contact-info","title":"10. Contact info","text":""},{"location":"CODE_OF_CONDUCT/#11-license-and-attribution","title":"11. License and attribution","text":"<p>The Citizen Code of Conduct is distributed by Stumptown Syndicate under a Creative Commons Attribution-ShareAlike license.</p> <p>Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy.</p> <p>Revision 2.3. Posted 6 March 2017.</p> <p>Revision 2.2. Posted 4 February 2016.</p> <p>Revision 2.1. Posted 23 June 2014.</p> <p>Revision 2.0, adopted by the Stumptown Syndicate board on 10 January 2013. Posted 17 March 2013.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#gnu-general-public-license","title":"GNU GENERAL PUBLIC LICENSE","text":"<p>Version 3, 29 June 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-use-with-the-gnu-affero-general-public-license","title":"13. Use with the GNU Affero General Public License.","text":"<p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"change-log/","title":"Change log","text":""},{"location":"change-log/#073-2025-08-02","title":"0.7.3 (2025-08-02)","text":""},{"location":"change-log/#dev","title":"Dev","text":"<ul> <li>add pixi configuration to the pyproject.toml file.</li> <li>relax the limits for the dependencies in the pyproject.toml file.</li> <li>gdal is installed from conda-forge channel.</li> <li>update the workflow to use the new pixi configuration.</li> </ul>"},{"location":"change-log/#072-2025-01-12","title":"0.7.2 (2025-01-12)","text":""},{"location":"change-log/#dev_1","title":"Dev","text":"<ul> <li>replace the setup.py with pyproject.toml</li> <li>automatic search for the gdal_plugins_path in the conda environment, and setting the <code>GDAL_DRIVER_PATH</code> environment variable.</li> <li>add python 3.12 to CI.</li> </ul>"},{"location":"change-log/#071-2024-12-07","title":"0.7.1 (2024-12-07)","text":"<ul> <li>update <code>cleopatra</code> package version to 0.5.1 and update the api to use the new version.</li> <li>update the miniconda workflow in ci.</li> <li>update gdal to 3.10 and update the DataSource to Dataset in the <code>FeatureCollection.file_name</code>.</li> <li>add <code>libgdal-netcdf</code> and <code>libgdal-hdf4</code> to the conda dependencies.</li> </ul>"},{"location":"change-log/#070-2024-06-01","title":"0.7.0 (2024-06-01)","text":"<ul> <li>install viz, dev, all packages through pip.</li> <li>create a separate module for the netcdf files.</li> <li>add configuration file and module for setting gdal configurations.</li> </ul>"},{"location":"change-log/#abstractdataset","title":"AbstractDataset","text":"<ul> <li>add <code>meta_data</code> property to return the metadata of the dataset.</li> <li>add <code>access</code> property to indicate the access mode of the dataset.</li> </ul>"},{"location":"change-log/#dataset","title":"Dataset","text":"<ul> <li>add extra parameter <code>file_i</code> to the <code>read_file</code> method to read a specific file in a compressed file.</li> <li>initialize the <code>GDAL_TIFF_INTERNAL_MASK</code> configuration to <code>No</code></li> <li>the add the <code>access</code> parameter to the constructor to set the access mode of the dataset.</li> <li>add the <code>band_units</code> property to return the units of the bands.</li> <li>the <code>__str__</code> and the <code>__repr__</code> methods return string numpy like data type (instead of the gdal constant) of the dataset.</li> <li>add <code>meta_data</code> property setter to set any key:value as a metadata of the dataset.</li> <li>add <code>scale</code> and <code>offset</code> properties to set the scale and offset of the bands.</li> <li>add <code>copy</code> method to copy the dataset to memory.</li> <li>add <code>get_attribute_table</code>set_attribute_table` method to get/set the attribute table of a specific band.</li> <li>the <code>plot</code> method uses the rgb bands defined in the dataset plotting (if exist).</li> <li>add <code>create</code> method to create a new dataset from scratch.</li> <li>add <code>write_array</code> method to write an array to an existing dataset.</li> <li>add <code>get_mask</code> method to get the mask of a dataset band.</li> <li>add <code>band_color</code> method to get the color assigned to a specific band (RGB).</li> <li>add <code>get_band_by_color</code> method to get the band index by its color.</li> <li>add <code>get_histogram</code> method to get/calculate  the histogram of a specific band.</li> <li>the <code>read_array</code> method takes and extra parameter <code>window</code> to lazily read a <code>window</code> of the raster, the window is [xoff, yoff, x-window, y-window], the <code>window</code> can also be a geodataframe.</li> <li>add <code>get_block_arrangement</code> method divide the raster into tiles based on the block size.</li> <li>add tiff file writing options (compression/tile/tile_length)</li> <li>add <code>close</code> method to flush to desk and close a dataset.</li> <li>add <code>add_band</code> method to add an array as a band to an existing dataset.</li> <li>rename <code>pivot_point</code> to <code>top_left_corner</code> in the <code>create</code> method.</li> <li>the <code>to_file</code> method return a <code>Dataset</code> object pointing to the saved dataset rather than the need to re-read the saved dataset after you save it.</li> </ul>"},{"location":"change-log/#datacube","title":"Datacube","text":"<ul> <li>the <code>Datacube</code> is moved to a separate module <code>datacube</code>.</li> </ul>"},{"location":"change-log/#netcdf","title":"NetCDF","text":"<ul> <li>move all the netcdf related functions to a separate module <code>netcdf</code>.</li> </ul>"},{"location":"change-log/#featurecollection","title":"FeatureCollection","text":"<ul> <li>rename the <code>pivot_point</code> to <code>top_left_corner</code></li> </ul>"},{"location":"change-log/#deprecated","title":"Deprecated","text":"<p>*Cropping a raster using a polygon is done now directly using gdal.wrap nand the the <code>_crop_with_polygon_by_rasterizing</code> is deprecated. * rename the interpolation method <code>nearest neighbour</code> to <code>nearest neighbor</code>.</p>"},{"location":"change-log/#060-2024-02-24","title":"0.6.0 (2024-02-24)","text":"<ul> <li>move the dem module to a separate package \"digital-rivers\".</li> </ul>"},{"location":"change-log/#056-2024-01-09","title":"0.5.6 (2024-01-09)","text":""},{"location":"change-log/#dataset_1","title":"Dataset","text":"<ul> <li>create <code>create_overviews</code>, <code>recreate_overview</code>, <code>read_overview_array</code> methods, and <code>overview_count</code> attribute to handle overviews.</li> <li>The <code>plot</code> method takes an extra parameters <code>overviews</code> and <code>overview_index</code> to enable plotting overviews instead of the real values in the bands.</li> </ul>"},{"location":"change-log/#055-2024-01-04","title":"0.5.5 (2024-01-04)","text":""},{"location":"change-log/#dataset_2","title":"Dataset","text":"<ul> <li>Count domain cells for a specific band.</li> </ul>"},{"location":"change-log/#054-2023-12-31","title":"0.5.4 (2023-12-31)","text":""},{"location":"change-log/#dataset_3","title":"Dataset","text":"<ul> <li>fix the un-updated array dimension bug in the crop method when the mask is a vector mask and the touch parameter is True.</li> </ul>"},{"location":"change-log/#053-2023-12-28","title":"0.5.3 (2023-12-28)","text":""},{"location":"change-log/#dataset_4","title":"Dataset","text":"<ul> <li>Introduce a new parameter touch to the crop method in the Dataset to enable considering the cells that most of the cell lies inside the mask, not only the cells that lie entirely inside the mask.</li> <li>Introduce a new parameter inplace to the crop method in the Dataset to enable replacing the dataset object with the new cropped dataset.</li> <li>Adjust the stats method to take a mask to calculate the stats inside this mask.</li> </ul>"},{"location":"change-log/#052-2023-12-27","title":"0.5.2 (2023-12-27)","text":""},{"location":"change-log/#dataset_5","title":"Dataset","text":"<ul> <li>add _iloc method to get the gdal band object by index.</li> <li>add stats method to calculate the statistics of the raster bands.</li> </ul>"},{"location":"change-log/#051-2023-11-27","title":"0.5.1 (2023-11-27)","text":""},{"location":"change-log/#dataset_6","title":"Dataset","text":"<ul> <li>revert the convert_longitude method to not use the gdal_wrap method as it is not working with the new version of gdal (newer tan 3.7.1).</li> <li>bump up versions.</li> </ul>"},{"location":"change-log/#050-2023-10-01","title":"0.5.0 (2023-10-01)","text":""},{"location":"change-log/#dataset_7","title":"Dataset","text":"<ul> <li>The dtype attribute is not initialized in the init, but initialized when using the dtype property.</li> <li>Create band_names setter method.</li> <li>add gdal_dtype, numpy_dtype, and dtype attribute to change between the dtype (data type general name), and the corresponding data type in numpy and gdal.</li> <li>Create color_table attribute, getter &amp; setter property to set a symbology (assign color to different values in each band).</li> <li>The read_array method returns array with the same type as the dtype of the first band in the raster.</li> <li>add a setter method to the band_names property.</li> <li>The methods (create_driver_from_scratch, get_band_names) is converted to private method.</li> <li>The no_data_value check the dtype before setting any value.</li> <li>The convert_longitude used the gdal.Wrap method instead of making the calculation step by step.</li> <li>The to_polygon is converted to _band_to_polygon private method used in the clusters method.</li> <li>The to_geodataframe is converted to to_feature_collection. change the backend of the function to use the crop function is a vector mask is given.</li> <li>the to_crs takes an extra parameter \"inplace\".</li> <li>The locate_points is converted to map_to_array_coordinates.</li> <li>Create array_to_map_coordinates to translate the array indices into real map coordinates.</li> </ul>"},{"location":"change-log/#datacube_1","title":"DataCube","text":"<ul> <li>rename the read_separate_files to read_multiple_files, and enable it to use regex strings to filter files in a given directory.</li> <li>rename read_dataset to open_datacube.</li> <li>rename the data attribute to values</li> </ul>"},{"location":"change-log/#featurecollection_1","title":"FeatureCollection","text":"<ul> <li>Add a pivot_point attribute to return the top left corner/first coordinates of the polygon.</li> <li>Add a layers_count property to return the number of layers in the file.</li> <li>Add a layer_names property to return the layers names.</li> <li>Add a column property to return column names.</li> <li>Add the file_name property to store the file name.</li> <li>Add the dtypes property to retrieve the data types of the columns in the file.</li> <li>Rename bounds to total_bounds.</li> <li>The _gdf_to_ds can convert the GeoDataFrame to a ogr.DataSource and to a gdal.Dataset.</li> <li>The create_point method returns a shapely point object or a GeoDataFrame if an epsg number is given.</li> </ul>"},{"location":"change-log/#042-2023-04-27","title":"0.4.2 (2023-04-27)","text":"<ul> <li>fix bug in plotting dataset without specifying the band</li> <li>fix bug in passing ot not passing band index in case of multi band rasters</li> <li>change the bounds in to_dataset method to total_bounds tp get the bbox of the whole geometries in the gdf</li> <li>add convert_longitude method to convert longitude to range between -180 and 180</li> </ul>"},{"location":"change-log/#041-2023-04-23","title":"0.4.1 (2023-04-23)","text":"<ul> <li>adjust all spatial operation functions to work with multi-band rasters.</li> <li>use gdal exceptions to capture runtime error of not finding the the file.</li> <li>add cluster method to dataset class.</li> <li>time_stamp attribute returns None if there is no time_stamp.</li> <li>restructure the no_data_value related functions.</li> <li>plot function can plot rgb image for multi-band rasters.</li> <li>to_file detect the driver type from the extension in the path.</li> </ul>"},{"location":"change-log/#040-2023-04-11","title":"0.4.0 (2023-04-11)","text":"<ul> <li>Restructure the whole package to two main objects Dataset and FeatureCollection</li> <li>Add class for multiple Dataset \"DataCube\".</li> <li>Link both Dataset and FeatureCollection to convert between raster and vector data types.</li> <li>Remove rasterio and netcdf from dependencies and depend only on gdal.</li> <li>Test read rasters/netcdf from virtual file systems (aws, compressed)</li> <li>Add dunder methods for all classes.</li> <li>add plotting functionality and cleopatra (plotting package) as an optional package.</li> <li>remove loops and replace it with ufunc from numpy.</li> </ul>"},{"location":"change-log/#033-2023-02-06","title":"0.3.3 (2023-02-06)","text":"<ul> <li>fix bug in reading the ogr drivers catalog for the vector class</li> <li>fix bug in creating rasterLike in the asciiToRaster method</li> </ul>"},{"location":"change-log/#032-2023-01-29","title":"0.3.2 (2023-01-29)","text":"<ul> <li>refactor code</li> <li>add documentation</li> <li>fix creating memory driver with compression in _createDataset</li> </ul>"},{"location":"change-log/#031-2023-01-25","title":"0.3.1 (2023-01-25)","text":"<ul> <li>add pyarrow to use parquet data type for saving dataframes and geodataframes</li> <li>add H3 indexing package, and add new module indexing with functions to convert geometries to indices back and forth.</li> <li>fix bug in calculating pivot point of the netcdf file</li> <li>rasterToDataFrame function will create geometries of the cells only based on the add_geometry parameter.</li> </ul>"},{"location":"change-log/#030-2023-01-23","title":"0.3.0 (2023-01-23)","text":"<ul> <li>add array module to deal with any array operations.</li> <li>add openDataset, getEPSG create SpatialReference, and setNoDataValue utility function, getCellCoords, ...</li> <li>add rasterToPolygon, PolygonToRaster, rasterToGeoDataFrame, conversion between ogr DataSource and GeoDataFrame.</li> </ul>"},{"location":"change-log/#0211-2023-01-14","title":"0.2.11 (2023-01-14)","text":"<ul> <li>add utils module for functions dealing with compressing files, and others utility functions</li> </ul>"},{"location":"change-log/#0211-2022-12-27","title":"0.2.11 (2022-12-27)","text":"<ul> <li>fix bug in pypi package names in requirements.txt file</li> </ul>"},{"location":"change-log/#0210-2022-12-25","title":"0.2.10 (2022-12-25)","text":"<ul> <li>lock numpy version to 1.23.5 as conda-forge can not install 1.24.0</li> </ul>"},{"location":"change-log/#029-2022-12-19","title":"0.2.9 (2022-12-19)","text":"<ul> <li>Use environment.yaml and requirements.txt instead of pyproject.toml and replace poetry env by conda env</li> </ul>"},{"location":"change-log/#010-2022-05-24","title":"0.1.0 (2022-05-24)","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.</p> <p>Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure any install or build dependencies are removed before the end of the layer when doing a    build.</li> <li>Update the README.md with details of changes to the interface, this includes new environment    variables, exposed ports, useful file locations and container parameters.</li> <li>Increase the version numbers in any examples files and the README.md to the new version that this    Pull Request would represent. The versioning scheme we use is SemVer.</li> <li>You may merge the Pull Request in once you have the sign-off of two other developers, or if you    do not have permission to do that, you may request the second reviewer to merge it for you.</li> </ol>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contributing/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"contributing/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"contributing/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"contributing/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"installation/","title":"Installation","text":"<p>This page explains how to install pyramids-gis and its dependencies using Pixi/conda or pip. The instructions and versions below are aligned with the project\u2019s pyproject.toml.</p> <p>Package name: pyramids-gis Current version: 0.7.3 Supported Python versions: 3.11 \u2013 3.13 (requires Python &gt;=3.11,&lt;4)</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":""},{"location":"installation/#core-runtime-pypi","title":"Core runtime (PyPI)","text":"<ul> <li>numpy &gt;=2.0.0</li> <li>pandas &gt;=2.0.0</li> <li>geopandas &gt;=1.0.0</li> <li>Shapely &gt;=2.0.0</li> <li>pyproj &gt;=3.7.0</li> <li>PyYAML &gt;=6.0.0</li> <li>loguru &gt;=0.7.2</li> <li>hpc-utils &gt;=0.1.5</li> </ul>"},{"location":"installation/#gis-stack-recommended-via-conda-forge","title":"GIS stack (recommended via conda-forge)","text":"<ul> <li>GDAL &gt;=3.10,&lt;4</li> <li>libgdal-netcdf &gt;=3.10,&lt;4</li> <li>libgdal-hdf4 &gt;=3.10,&lt;4</li> </ul>"},{"location":"installation/#optional-extras","title":"Optional extras","text":"<ul> <li>viz: cleopatra &gt;=0.5.1</li> <li>dev: nbval, pre-commit, pytest, coverage, build, twine, etc.</li> <li>docs: mkdocs, mkdocs-material, mkdocstrings, mike, etc.</li> </ul>"},{"location":"installation/#recommended-pixiconda-environment","title":"Recommended: Pixi/Conda environment","text":"<p>This repository includes a Pixi configuration to create fully-solvable environments with the right GDAL build from conda-forge.</p> <p>Prerequisites: Install Pixi (https://pixi.sh/) or have conda/mamba with the conda-forge channel available.</p>"},{"location":"installation/#using-pixi","title":"Using Pixi","text":"<p>From the project root:</p> <pre><code>pixi run main          # runs the main test suite to ensure the env is solvable\npixi shell             # enter the Pixi environment\n</code></pre> <p>Pixi environments provided: - default: includes dev + viz extras - docs: documentation toolchain - py311 / py312 / py313: pinned Python versions</p> <p>To install the package in editable mode inside the Pixi environment:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#using-condamamba-directly","title":"Using conda/mamba directly","text":"<p>Create and activate an environment (example with Python 3.12):</p> <pre><code>mamba create -n pyramids -c conda-forge python=3.12 gdal libgdal-netcdf libgdal-hdf4\nmamba activate pyramids-gis\n</code></pre> <p>Then install the package from PyPI (release):</p> <pre><code>pip install pyramids-gis==0.7.3\n</code></pre> <p>Optionally include extras (examples):</p> <pre><code>pip install \"pyramids-gis[viz]\"        # installs cleopatra\npip install \"pyramids-gis[dev]\"        # developer tools\npip install \"pyramids-gis[docs]\"       # docs toolchain\n</code></pre>"},{"location":"installation/#installing-with-pip-only-advanced","title":"Installing with pip only (advanced)","text":"<p>Installing GDAL wheels via pip can be platform-specific. We strongly recommend installing GDAL from conda-forge first, then using pip for pyramids-gis:</p> <pre><code>conda install -c conda-forge gdal libgdal-netcdf libgdal-hdf4\npip install pyramids-gis\n</code></pre> <p>If you insist on a pip-only approach, consult the GDAL wheel guidance for your platform and ensure gdal is available at runtime before installing pyramids-gis.</p>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>Clone the repository and install:</p> <pre><code>git clone https://github.com/Serapieum-of-alex/pyramids.git\ncd pyramids\npython -m pip install .\n</code></pre> <p>Editable (development) install:</p> <pre><code>git clone https://github.com/Serapieum-of-alex/pyramids.git\ncd pyramids\npip install -e .[dev]\n</code></pre> <p>Install directly from GitHub (latest main):</p> <pre><code>pip install \"git+https://github.com/Serapieum-of-alex/pyramids.git\"\n</code></pre> <p>Install a specific tagged release from GitHub:</p> <pre><code>pip install \"git+https://github.com/Serapieum-of-alex/pyramids.git@v0.7.3\"\n</code></pre>"},{"location":"installation/#quick-check","title":"Quick check","text":"<p>After installation, open Python and run:</p> <pre><code>import pyramids\nprint(pyramids.__version__)\n</code></pre> <p>You can also run the test suite if you installed dev tools:</p> <pre><code>pytest -m \"not plot\" -q\n</code></pre>"},{"location":"installation/#notes","title":"Notes","text":"<ul> <li>Supported Python versions are 3.11\u20133.13.</li> <li>Prefer conda-forge for GDAL and related libraries.</li> <li>Documentation: https://pyramids-gis.readthedocs.io/</li> <li>Source repository: https://github.com/Serapieum-of-alex/pyramids</li> </ul>"},{"location":"adr/","title":"Architecture Decision Records (ADRs)","text":"<p>This section collects significant architectural decisions for the pyramids project, using the MADR format.</p> <ul> <li>ADR-0001: Docs stack</li> <li>ADR-0002: Type checking approach</li> <li>ADR-0003: I/O format choices</li> <li>ADR-0004: Dependency graphing approach</li> <li>ADR-0005: Versioned documentation strategy</li> </ul> <p>Each ADR includes context, decision, and consequences. New ADRs should follow the numbering convention and be immutable once accepted.</p>"},{"location":"adr/0001-docs-stack/","title":"ADR-0001: Documentation stack","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/0001-docs-stack/#context","title":"Context","text":"<p>We need a robust, contributor-friendly documentation stack that supports API reference, diagrams, tutorials, and versioned publishing.</p>"},{"location":"adr/0001-docs-stack/#decision","title":"Decision","text":"<ul> <li>Use MkDocs with the Material theme as the primary docs framework.</li> <li>Use mkdocstrings[python] for auto-generating API reference from docstrings.</li> <li>Prefer Mermaid fenced blocks for diagrams; allow PlantUML if complexity requires it.</li> <li>Use mike for versioned documentation on GitHub Pages.</li> </ul>"},{"location":"adr/0001-docs-stack/#consequences","title":"Consequences","text":"<ul> <li>Contributors document code with Google-style docstrings.</li> <li>Diagrams render client-side in the site; heavy diagrams can be split or pre-rendered if needed.</li> <li>CI builds the docs on pushes and releases, publishing with mike.</li> </ul>"},{"location":"adr/0002-type-checking/","title":"ADR-0002: Type checking approach","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/0002-type-checking/#context","title":"Context","text":"<p>The project benefits from static analysis to improve reliability and developer experience, but runtime dependencies (e.g., GDAL) and large legacy surfaces make full coverage challenging.</p>"},{"location":"adr/0002-type-checking/#decision","title":"Decision","text":"<ul> <li>Adopt gradual typing with standard Python type hints.</li> <li>Allow optional external checkers (mypy or pyright) for contributors; not enforced in CI yet.</li> <li>Prefer Google-style docstrings with type annotations to improve mkdocstrings output.</li> </ul>"},{"location":"adr/0002-type-checking/#consequences","title":"Consequences","text":"<ul> <li>Type hints will be added progressively, focusing on public APIs first.</li> <li>We can later enable strictness per-module and add a type coverage report.</li> </ul>"},{"location":"adr/0003-io-formats/","title":"ADR-0003: I/O format choices","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/0003-io-formats/#context","title":"Context","text":"<p><code>pyramids</code> processes raster and vector geospatial data and must interoperate with common GIS tools and libraries.</p>"},{"location":"adr/0003-io-formats/#decision","title":"Decision","text":"<ul> <li>Raster: prioritize GeoTIFF for read/write; support ASCII Grid for simple interchange; support NetCDF for multi-dimensional datasets when appropriate.</li> <li>Vector: prioritize GeoJSON for web interoperability; support Shapefile and GeoPackage for desktop GIS.</li> </ul>"},{"location":"adr/0003-io-formats/#consequences","title":"Consequences","text":"<ul> <li>Users can rely on standard formats across workflows.</li> <li>Additional drivers may be enabled via GDAL installation; docs should clarify environment requirements.</li> </ul>"},{"location":"adr/0004-dependency-graphing/","title":"ADR-0004: Dependency graphing approach","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/0004-dependency-graphing/#context","title":"Context","text":"<p>We want visibility into internal module dependencies, cycles, and layering to guide refactoring and documentation.</p>"},{"location":"adr/0004-dependency-graphing/#decision","title":"Decision","text":"<ul> <li>Represent a high-level dependency graph using Mermaid in the docs (modules only).</li> <li>Provide a simple, deterministic diagram generation step that exports Mermaid sources to artifacts for CI publishing.</li> <li>Optionally evaluate pydeps/pyan/grimp later for automated graphs; keep manual graph for now to avoid heavy dependencies.</li> </ul>"},{"location":"adr/0004-dependency-graphing/#consequences","title":"Consequences","text":"<ul> <li>Readers have a clear mental model of package layering.</li> <li>Automated cycle detection can be added in future CI without blocking current workflows.</li> </ul>"},{"location":"adr/0005-versioning-docs/","title":"ADR-0005: Versioned documentation strategy","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-16</li> </ul>"},{"location":"adr/0005-versioning-docs/#context","title":"Context","text":"<p>We want readers to access documentation matching their installed version and preview changes on PRs.</p>"},{"location":"adr/0005-versioning-docs/#decision","title":"Decision","text":"<ul> <li>Use <code>mike</code> to publish versioned docs to GitHub Pages.</li> <li>Track <code>main</code> as default; publish releases under their tag and alias <code>latest</code>.</li> <li>Optionally publish PR previews under a <code>develop</code> alias.</li> </ul>"},{"location":"adr/0005-versioning-docs/#consequences","title":"Consequences","text":"<ul> <li>Contributors can update docs alongside code and see previews.</li> <li>Users can browse historical docs aligned with releases.</li> </ul>"},{"location":"adr/0006-docker-image/","title":"ADR-0006: Docker image","text":"<ul> <li>Status: Accepted</li> <li>Date: 2025-08-24</li> </ul>"},{"location":"adr/0006-docker-image/#context","title":"Context","text":"<p>We need a reproducible, hermetic container that runs the pyramids package across platforms without requiring local installations of GDAL/PROJ/NetCDF or compiler toolchains. The image should: - Be fast to rebuild via Docker layer caching. - Bundle all native geospatial dependencies inside the image/runtime environment. - Be CI-friendly and cross-platform (supporting amd64/arm64 builds via Buildx when needed). - Avoid ad-hoc apt installs in the final image and avoid conda complexity; rely on a locked environment.</p>"},{"location":"adr/0006-docker-image/#decision","title":"Decision","text":"<ul> <li>Use a multi-stage Dockerfile driven by Pixi environments:</li> <li>build stage: <code>FROM ghcr.io/prefix-dev/pixi:bookworm-slim</code><ul> <li>Build arg <code>ENV_NAME</code> (defaults to <code>default</code>) selects which Pixi environment to build.</li> <li>Environment path: <code>/app/.pixi/envs/${ENV_NAME}</code> (exported as <code>PIXI_ENV_DIR</code>).</li> <li>Copy <code>pyproject.toml</code>, <code>pixi.lock</code>, and <code>README.md</code> first for caching; then <code>src/</code>.</li> <li>Ensure non-editable install by rewriting any <code>editable = true</code> to <code>editable = false</code> in <code>pyproject.toml</code> during build.</li> <li>Run <code>pixi install -e \"${ENV_NAME}\"</code> to resolve and build the environment per <code>pixi.lock</code>.</li> </ul> </li> <li>production stage: <code>FROM debian:bookworm-slim</code><ul> <li>Copy only the built environment from the build stage into a neutral prefix <code>VENV_DIR=/opt/venv</code>.</li> <li>Set runtime environment variables so native deps resolve correctly:</li> <li><code>PATH=\"${VENV_DIR}/bin:${PATH}</code></li> <li><code>LD_LIBRARY_PATH=\"${VENV_DIR}/lib:${LD_LIBRARY_PATH}</code></li> <li><code>PROJ_LIB=\"${VENV_DIR}/share/proj\"</code>, <code>GDAL_DATA=\"${VENV_DIR}/share/gdal\"</code>, <code>PYTHONNOUSERSITE=1</code></li> <li>Verify at build time that <code>pyramids</code>, <code>osgeo.gdal</code>, <code>shapely</code>, and <code>pyproj</code> import and that Python runs from <code>${VENV_DIR}</code>.</li> <li>Default <code>CMD</code> uses <code>${VENV_DIR}/bin/python</code> to import <code>pyramids</code> and print the Python version as a sanity check.</li> </ul> </li> <li>Python version and native library versions are pinned by <code>pixi.lock</code> (not by a Docker <code>PYTHON_VERSION</code> build arg).</li> <li>No additional <code>apt-get</code> installs are performed in the final image; all binaries and data files come from the bundled environment.</li> <li>No non-root user or explicit <code>WORKDIR</code> is set in the current design; revisit if required for security or UX.</li> </ul>"},{"location":"adr/0006-docker-image/#consequences","title":"Consequences","text":"<ul> <li>Reproducible builds and runs across developer machines and CI without local GDAL/PROJ installs.</li> <li>Hermetic runtime: GDAL/PROJ/GEOS, etc., come from the copied environment; env vars ensure correct resolution.</li> <li>Good layer caching: dependency resolution happens in the build stage and is cached until <code>pyproject.toml</code>/<code>pixi.lock</code> change; source changes rebuild quickly.</li> <li>Users switch environments with <code>--build-arg ENV_NAME=&lt;name&gt;</code>; Python version comes from the selected Pixi env.</li> <li>Image remains moderately sized because we copy only the resolved environment into a slim Debian base; no extra system packages are added.</li> <li>Absence of a non-root user means the container runs as root by default; consider adding an unprivileged user in a future revision if needed.</li> </ul>"},{"location":"architecture/diagrams/","title":"Architecture Diagrams","text":"<p>Below are core diagrams describing the system at multiple levels.</p>"},{"location":"architecture/diagrams/#c4-system-context","title":"C4: System Context","text":"<pre><code>flowchart LR\n  user(User) --&gt;|Provides GIS data paths &amp; commands| pyramids{{pyramids package}}\n  ext1[(Raster files\\nGeoTIFF/ASC/NetCDF)] --&gt; pyramids\n  ext2[(Vector files\\nShapefile/GeoJSON/GPKG)] --&gt; pyramids\n  pyramids --&gt; out1[(Processed rasters\\nGeoTIFF/ASC)]\n  pyramids --&gt; out2[(Processed vectors\\nGeoJSON/GPKG)]</code></pre>"},{"location":"architecture/diagrams/#c4-containers","title":"C4: Containers","text":"<pre><code>flowchart TB\n  subgraph Runtime Process\n    A[Dataset]:::c --&gt; B[_io]\n    C[Datacube]:::c --&gt; B\n    D[FeatureCollection]:::c --&gt; B\n    A --&gt; E[_utils]\n    C --&gt; E\n    D --&gt; E\n  end\n  classDef c fill:#eef,stroke:#88f</code></pre>"},{"location":"architecture/diagrams/#c4-components","title":"C4: Components","text":"<pre><code>flowchart LR\n  io[_io: read_file, to_ascii, path parsing]\n  utils[_utils: geometry/index helpers]\n  ds[dataset.Dataset]\n  abs[abstract_dataset.AbstractDataset]\n  dc[datacube.Datacube]\n  fc[featurecollection.FeatureCollection]\n\n  abs --&gt; ds\n  ds --&gt; io\n  dc --&gt; ds\n  fc --&gt; io\n  ds --&gt; utils\n  fc --&gt; utils</code></pre>"},{"location":"architecture/diagrams/#uml-class-raster-core","title":"UML Class: Raster Core","text":"<pre><code>classDiagram\n  class AbstractDataset {\n    &lt;&lt;abstract&gt;&gt;\n    +read_file(path, read_only)\n    +to_file(path, band)\n  }\n  class Dataset {\n    +read_file(path, read_only, file_i)\n    +to_file(path, band, tile_length)\n    +read()\n  }\n  AbstractDataset &lt;|-- Dataset</code></pre>"},{"location":"architecture/diagrams/#uml-class-vector-core","title":"UML Class: Vector Core","text":"<pre><code>classDiagram\n  class FeatureCollection {\n    +read_file(path)\n    +to_file(path, driver)\n  }</code></pre>"},{"location":"architecture/diagrams/#sequence-read-raster-from-zip","title":"Sequence: Read Raster from Zip","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant DS as Dataset\n  participant IO as _io\n  U-&gt;&gt;DS: Dataset.read_file(\"dem.zip!dem.tif\")\n  DS-&gt;&gt;IO: _parse_path(path)\n  IO--&gt;&gt;DS: zip path + inner file\n  DS-&gt;&gt;IO: read_file(...)\n  IO--&gt;&gt;DS: array + meta\n  DS--&gt;&gt;U: Dataset instance</code></pre>"},{"location":"architecture/diagrams/#sequence-save-raster-to-geotiff","title":"Sequence: Save Raster to GeoTIFF","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant DS as Dataset\n  U-&gt;&gt;DS: to_file(\"out.tif\")\n  DS--&gt;&gt;U: writes file</code></pre>"},{"location":"architecture/diagrams/#sequence-build-datacube-from-folder","title":"Sequence: Build Datacube from Folder","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant DC as Datacube\n  participant DS as Dataset\n  U-&gt;&gt;DC: from_folder(\"./rasters/*.tif\")\n  DC-&gt;&gt;DS: open each file\n  DS--&gt;&gt;DC: Dataset objects\n  DC--&gt;&gt;U: Datacube</code></pre>"},{"location":"architecture/diagrams/#sequence-zonal-statistics","title":"Sequence: Zonal Statistics","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant DS as Dataset\n  participant FC as FeatureCollection\n  U-&gt;&gt;FC: read_file(polygons.gpkg)\n  U-&gt;&gt;DS: read_file(raster.tif)\n  FC-&gt;&gt;DS: zonal_stats(raster)\n  DS--&gt;&gt;U: table</code></pre>"},{"location":"architecture/diagrams/#sequence-align-and-resample","title":"Sequence: Align and Resample","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant DS as Dataset\n  U-&gt;&gt;DS: align_to(reference)\n  DS--&gt;&gt;U: aligned dataset</code></pre>"},{"location":"architecture/diagrams/#dependency-graph-modules","title":"Dependency Graph (Modules)","text":"<pre><code>flowchart LR\n  abstract_dataset --&gt; dataset\n  _io --&gt; dataset\n  _utils --&gt; dataset\n  dataset --&gt; datacube\n  _io --&gt; featurecollection\n  _utils --&gt; featurecollection</code></pre> <ul> <li>brief class diagram for the <code>Dataset</code> class and related components:</li> </ul> <pre><code>classDiagram\n    %% configuration class\n    class config_Config {\n        +__init__(config_file)\n        +load_config()\n        +initialize_gdal()\n        +dynamic_env_variables()\n        +set_env_conda()\n        +set_env_os()\n    }\n\n    %% abstract base class for rasters\n    class abstract_dataset_AbstractDataset {\n        +__init__(src, access)\n        +values() np.ndarray\n        +rows() int\n        +columns() int\n        +shape() (bands, rows, cols)\n        +geotransform()\n        +top_left_corner()\n        +epsg() int\n        +crs()\n        +cell_size() int\n        +no_data_value\n        +meta_data()\n    }\n\n    %% concrete raster class\n    class dataset_Dataset {\n        +__init__(src, access)\n        +read_file(path, read_only)\n        +create_from_array(array, top_left_corner, cell_size, epsg)\n        +read_array(band, window)\n        +to_file(path, driver)\n        +align(alignment_src, data_src)\n        +resample(cell_size, method)\n        +crop(window)\n        +plot(title, ticks_spacing, cmap, color_scale, vmin, cbar_label)\n    }\n\n    %% NetCDF: raster class specialised for NetCDF variables\n    class netcdf_NetCDF {\n        +__init__(src, access)\n        +get_variable_names()\n        +get_variables()\n        +time_stamp()\n        +lat()\n        +lon()\n    }\n\n    %% DataCube: stack of rasters\n    class datacube_Datacube {\n        +__init__(src, time_length, files)\n        +create_cube(src, dataset_length)\n        +read_multiple_files(path, with_order, regex_string, date, file_name_data_fmt, start, end, fmt, extension)\n        +base() Dataset\n        +files() list\n        +time_length() int\n        +shape() (time, rows, cols)\n        +rows() int\n        +columns() int\n    }\n\n    %% FeatureCollection for vector data\n    class featurecollection_FeatureCollection {\n        +__init__(gdf)\n        +GetXYCoords()\n        +GetPointCoords()\n        +GetLineCoords()\n        +GetPolyCoords()\n        +Explode()\n        +CombineGeometrics()\n        +GCSDistance()\n        +ReprojectPoints()\n        +AddSpatialReference()\n        +WriteShapefile()\n    }\n\n    %% Driver catalog\n    class _utils_Catalog {\n        +__init__(raster_driver)\n        +get_driver(driver)\n        +get_gdal_name(driver)\n        +get_driver_by_extension(extension)\n        +get_extension(driver)\n        +exists(driver)\n    }\n\n    %% error classes\n    class _errors_ReadOnlyError\n    class _errors_DatasetNoFoundError\n    class _errors_NoDataValueError\n    class _errors_AlignmentError\n    class _errors_DriverNotExistError\n    class _errors_FileFormatNotSupported\n    class _errors_OptionalPackageDoesNotExist\n    class _errors_FailedToSaveError\n    class _errors_OutOfBoundsError\n\n    %% inheritance relations\n    abstract_dataset_AbstractDataset &lt;|-- dataset_Dataset\n    dataset_Dataset &lt;|-- netcdf_NetCDF\n\n    %% composition/usage relations\n    datacube_Datacube --&gt; dataset_Dataset : \"base raster\"\n    abstract_dataset_AbstractDataset ..&gt; _utils_Catalog : \"uses Catalog constant\"\n    abstract_dataset_AbstractDataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    dataset_Dataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    featurecollection_FeatureCollection ..&gt; _utils_Catalog : \"uses drivers\"\n    dataset_Dataset ..&gt; _errors_ReadOnlyError : \"raises\"\n    dataset_Dataset ..&gt; _errors_AlignmentError : \"raises\"\n    dataset_Dataset ..&gt; _errors_NoDataValueError : \"raises\"\n    dataset_Dataset ..&gt; _errors_FailedToSaveError : \"raises\"\n    dataset_Dataset ..&gt; _errors_OutOfBoundsError : \"raises\"\n    datacube_Datacube ..&gt; _errors_DatasetNoFoundError : \"raises\"\n    featurecollection_FeatureCollection ..&gt; _errors_DriverNotExistError : \"raises\"\n    netcdf_NetCDF ..&gt; _errors_OptionalPackageDoesNotExist : \"raises\"\n    config_Config ..&gt; dataset_Dataset : \"initialises raster settings\"\n</code></pre> <ul> <li>Detailed class diagram:</li> </ul> <pre><code>classDiagram\n    %% configuration class\n    class config_Config {\n        +__init__(config_file)\n        +load_config()\n        +initialize_gdal()\n        +set_env_conda()\n        +dynamic_env_variables()\n        +setup_logging()\n        +set_error_handler()\n    }\n\n    %% abstract base class for rasters\n    class abstract_dataset_AbstractDataset {\n        +__init__(src, access)\n        +__str__()\n        +__repr__()\n        +access()\n        +raster()\n        +raster(value)\n        +values()\n        +rows()\n        +columns()\n        +shape()\n        +geotransform()\n        +top_left_corner()\n        +epsg()\n        +epsg(value)\n        +crs()\n        +crs(value)\n        +cell_size()\n        +no_data_value()\n        +no_data_value(value)\n        +meta_data()\n        +meta_data(value)\n        +block_size()\n        +block_size(value)\n        +file_name()\n        +driver_type()\n        +read_file(path, read_only)\n        +read_array(band, window)\n        +_read_block(band, window)\n        +plot(band, exclude_value, rgb, surface_reflectance, cutoff, overview, overview_index, **kwargs)\n    }\n\n    %% concrete raster class\n    class dataset_Dataset {\n        +__init__(src, access)\n        +__str__()\n        +__repr__()\n        +access()\n        +raster()\n        +raster(value)\n        +values()\n        +rows()\n        +columns()\n        +shape()\n        +geotransform()\n        +epsg()\n        +epsg(value)\n        +crs()\n        +crs(value)\n        +cell_size()\n        +band_count()\n        +band_names()\n        +band_names(name_list)\n        +band_units()\n        +band_units(value)\n        +no_data_value()\n        +no_data_value(value)\n        +meta_data()\n        +meta_data(value)\n        +block_size()\n        +block_size(value)\n        +file_name()\n        +driver_type()\n        +scale()\n        +scale(value)\n        +offset()\n        +offset(value)\n        +read_file(path, read_only)\n        +create_from_array(arr, top_left_corner, cell_size, epsg)\n        +read_array(band, window)\n        +_read_block(band, window)\n        +plot(band, exclude_value, rgb, surface_reflectance, cutoff, overview, overview_index, **kwargs)\n        +to_file(path, driver, band)\n        +to_crs(to_epsg, method, maintain_alignment)\n        +resample(cell_size, method)\n        +align(alignment_src)\n        +crop(mask, touch)\n        +merge(src, dst, no_data_value, init, n)\n        +apply(ufunc)\n        +overlay(classes_map, exclude_value)\n    }\n\n    %% NetCDF: raster class specialised for NetCDF variables\n    class netcdf_NetCDF {\n        +__init__(src, access)\n        +__str__()\n        +__repr__()\n        +lon()\n        +lat()\n        +x()\n        +y()\n        +get_y_lat_dimension_array(pivot_y, cell_size, rows)\n        +get_x_lon_dimension_array(pivot_x, cell_size, columns)\n        +variables()\n        +no_data_value()\n        +no_data_value(value)\n        +file_name()\n        +time_stamp()\n        +read_file(path, read_only, open_as_multi_dimensional)\n        +_get_time_variable()\n        +_get_lat_lon()\n        +_read_variable(var)\n        +get_variable_names()\n        +_read_md_array(variable_name)\n        +get_variables(read_only)\n        +is_subset()\n        +is_md_array()\n        +create_main_dimension(group, dim_name, dtype, values)\n        +create_from_array(arr, geo, bands_values, epsg, no_data_value, driver_type, path, variable_name)\n        +_create_netcdf_from_array(arr, variable_name, cols, rows, bands, bands_values, geo, epsg, no_data_value, driver_type, path)\n        +_add_md_array_to_group(dst_group, var_name, src_mdarray)\n        +add_variable(dataset, variable_name)\n        +remove_variable(variable_name)\n    }\n\n    %% DataCube: stack of rasters\n    class datacube_Datacube {\n        +__init__(src, time_length, files)\n        +__str__()\n        +__repr__()\n        +base()\n        +files()\n        +time_length()\n        +rows()\n        +columns()\n        +shape()\n        +create_cube(src, dataset_length)\n        +read_multiple_files(path, with_order, regex_string, date, file_name_data_fmt, start, end, fmt, extension)\n        +open_datacube(band)\n        +values()\n        +values(val)\n        +__getitem__(key)\n        +__setitem__(key, value)\n        +__len__()\n        +__iter__()\n        +head(n)\n        +tail(n)\n        +first()\n        +last()\n        +iloc(i)\n        +plot(band, exclude_value, **kwargs)\n        +to_file(path, driver, band)\n        +to_crs(to_epsg, method, maintain_alignment)\n        +crop(mask, inplace, touch)\n        +align(alignment_src)\n        +merge(src, dst, no_data_value, init, n)\n        +apply(ufunc)\n        +overlay(classes_map, exclude_value)\n    }\n\n    %% FeatureCollection for vector data\n    class featurecollection_FeatureCollection {\n        +__init__(gdf)\n        +__str__()\n        +feature()\n        +epsg()\n        +total_bounds()\n        +top_left_corner()\n        +layers_count()\n        +layer_names()\n        +column()\n        +file_name()\n        +dtypes()\n        +read_file(path)\n        +create_ds(driver, path)\n        +_create_driver(driver, path)\n        +_copy_driver_to_memory(ds, name)\n        +to_file(path, driver)\n        +_gdf_to_ds(inplace, gdal_dataset)\n        +_ds_to_gdf_with_io(inplace)\n        +_ds_to_gdf_in_memory(inplace)\n        +GetXYCoords()\n        +GetPointCoords()\n        +GetLineCoords()\n        +GetPolyCoords()\n        +Explode()\n        +MultiGeomHandler()\n        +GetCoords()\n        +XY()\n        +CreatePolygon()\n        +CreatePoint()\n        +CombineGeometrics()\n        +GCSDistance()\n        +ReprojectPoints()\n        +ReprojectPoints_2()\n        +AddSpatialReference()\n        +PolygonCenterPoint()\n        +WriteShapefile()\n    }\n\n    %% Driver catalog\n    class _utils_Catalog {\n        +__init__(raster_driver)\n        +get_driver(driver)\n        +get_gdal_name(driver)\n        +get_driver_by_extension(extension)\n        +get_extension(driver)\n        +exists(driver)\n    }\n\n    %% error classes\n    class _errors_ReadOnlyError\n    class _errors_DatasetNoFoundError\n    class _errors_NoDataValueError\n    class _errors_AlignmentError\n    class _errors_DriverNotExistError\n    class _errors_FileFormatNotSupported\n    class _errors_OptionalPackageDoesNotExist\n    class _errors_FailedToSaveError\n    class _errors_OutOfBoundsError\n\n    %% inheritance relations\n    abstract_dataset_AbstractDataset &lt;|-- dataset_Dataset\n    dataset_Dataset &lt;|-- netcdf_NetCDF\n\n    %% composition/usage relations\n    datacube_Datacube --&gt; dataset_Dataset : \"base raster\"\n    abstract_dataset_AbstractDataset ..&gt; _utils_Catalog : \"uses Catalog constant\"\n    abstract_dataset_AbstractDataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    dataset_Dataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    featurecollection_FeatureCollection ..&gt; _utils_Catalog : \"uses drivers\"\n    dataset_Dataset ..&gt; _errors_ReadOnlyError : \"raises\"\n    dataset_Dataset ..&gt; _errors_AlignmentError : \"raises\"\n    dataset_Dataset ..&gt; _errors_NoDataValueError : \"raises\"\n    dataset_Dataset ..&gt; _errors_FailedToSaveError : \"raises\"\n    dataset_Dataset ..&gt; _errors_OutOfBoundsError : \"raises\"\n    datacube_Datacube ..&gt; _errors_DatasetNoFoundError : \"raises\"\n    featurecollection_FeatureCollection ..&gt; _errors_DriverNotExistError : \"raises\"\n    netcdf_NetCDF ..&gt; _errors_OptionalPackageDoesNotExist : \"raises\"\n    config_Config ..&gt; dataset_Dataset : \"initialises raster settings\"\n</code></pre>"},{"location":"how-to/docker/","title":"Docker image and GitHub Container Registry (GHCR)","text":"<p>The image is a Library/runtime container - Purpose: Provide a fully set up environment with all native deps (GDAL/PROJ/etc.) so users can run arbitrary commands. - Endpoint: Keep it minimal; print version/help or drop into a shell for interactive use.</p> <p>This guide explains everything you need to work with the provided Dockerfile: - What the Dockerfile does and its stages - How to build the image locally - How to run the container - How to push the image to GitHub Container Registry (GHCR) - How to delete images from GHCR when needed</p> <p>Requirements - Docker installed (Docker Desktop on Windows/macOS, or Docker Engine on Linux) - Optional: GitHub CLI (<code>gh</code>) for advanced GHCR management - Optional for pushing/deleting: a GitHub Personal Access Token (classic) with at least <code>read:packages</code> and <code>write:packages</code> scopes. For deleting, also <code>delete:packages</code>.</p>"},{"location":"how-to/docker/#understand-the-dockerfile","title":"Understand the Dockerfile","text":"<p>Path: <code>./Dockerfile</code></p> <p>The Dockerfile is multi-stage and optimized to build a ready-to-run environment using [Pixi]. It has two stages:</p> <p>1) build (FROM <code>ghcr.io/prefix-dev/pixi:bookworm-slim</code>)    - Installs the project into an isolated Pixi environment (at <code>/app/.pixi/envs/${ENV_NAME}</code>).    - Honors <code>ENV_NAME</code> so you can choose which Pixi environment to build (defaults to <code>default</code>).    - Ensures a non-editable install by rewriting any <code>editable = true</code> to <code>editable = false</code> in <code>pyproject.toml</code> during the build.    - Benefits from Docker layer caching of Pixi downloads and the environment for faster rebuilds.</p> <p>2) production (FROM <code>debian:bookworm-slim</code>)    - Copies only the built Pixi environment into a neutral runtime prefix at <code>/opt/venv</code> (<code>VENV_DIR</code>).    - Adds <code>${VENV_DIR}/bin</code> to <code>PATH</code> and sets <code>LD_LIBRARY_PATH</code>, <code>PROJ_LIB</code>, <code>GDAL_DATA</code>, and <code>PYTHONNOUSERSITE</code> so GDAL/PROJ, etc. work out of the box.    - Runs a verification step during the image build that imports <code>pyramids</code>, <code>osgeo.gdal</code>, <code>shapely</code>, and <code>pyproj</code>, and asserts Python runs from <code>${VENV_DIR}</code>.    - Default <code>CMD</code> invokes <code>${VENV_DIR}/bin/python</code> to import <code>pyramids</code> and print the Python version.</p> <p>Key build arguments and environment variables: - <code>ARG ENV_NAME=default</code> \u2192 set with <code>--build-arg ENV_NAME=&lt;name&gt;</code> to pick a Pixi environment. - <code>ENV PIXI_ENV_DIR=/app/.pixi/envs/${ENV_NAME}</code> \u2192 where the build-stage environment is created. - <code>ENV VENV_DIR=/opt/venv</code> \u2192 runtime prefix in the final image where the environment is copied. - <code>ENV PATH=\"${VENV_DIR}/bin:${PATH}\"</code> \u2192 ensures the environment\u2019s Python and tools are first on PATH at runtime. - <code>ENV LD_LIBRARY_PATH</code>, <code>ENV PROJ_LIB</code>, <code>ENV GDAL_DATA</code>, <code>ENV PYTHONNOUSERSITE=1</code> \u2192 set to make GDAL/PROJ work and keep the runtime hermetic (details below).</p> <p>If you need multiple platforms (e.g., Apple Silicon/arm64 vs. amd64), you can use Buildx (see below).</p>"},{"location":"how-to/docker/#docker-runtime-environment-variables","title":"Docker runtime environment variables","text":"<ul> <li> <p>LD_LIBRARY_PATH</p> <ul> <li>Purpose: Ensures the dynamic linker can find shared libraries (e.g., GDAL, PROJ, GEOS) bundled inside the environment embedded in the image.</li> <li>Effect: Points the loader to the environment\u2019s lib directory first so extensions depending on native libraries resolve correctly at runtime.</li> </ul> </li> <li> <p>PROJ_LIB</p> <ul> <li>Purpose: Tells PROJ where to find its datum shift grids and CRS resource files.</li> <li>Effect: Enables coordinate transformations and reprojections that require PROJ\u2019s data files.</li> </ul> </li> <li> <p>GDAL_DATA</p> <ul> <li>Purpose: Points GDAL to its data directory containing coordinate system definitions, driver metadata, and supporting resources.</li> <li>Effect: Ensures GDAL utilities and Python bindings can locate EPSG definitions and other essential data.</li> </ul> </li> <li> <p>PYTHONNOUSERSITE=1</p> <ul> <li>Purpose: Prevents Python from loading packages from the user\u2019s site-packages directory.</li> <li>Effect: Produces a hermetic runtime by using only the packages inside the image\u2019s environment, avoiding accidental contamination from host-level Python packages.</li> </ul> </li> </ul>"},{"location":"how-to/docker/#build-the-image","title":"Build the image","text":"<p>Typical local build (PowerShell or any shell):</p> <pre><code># In the repo root\nIMAGE=\"pyramids\"\n# Build using the default Pixi environment\ndocker build -t \"$IMAGE:latest\" .\n</code></pre> <p>Choose a Pixi environment with <code>ENV_NAME</code> (if you have multiple, e.g., <code>py311</code>):</p> <pre><code>docker build --build-arg ENV_NAME=default -t pyramids:default .\n# or\n# docker build --build-arg ENV_NAME=py311 -t pyramids:py311 .\n</code></pre> <p>Tag with a version too:</p> <pre><code>VERSION=\"0.1.0\"\ndocker build -t pyramids:latest -t \"pyramids:$VERSION\" .\n</code></pre> <p>Multi-arch build (optional) with Buildx:</p> <pre><code># Create and use a builder once (if needed)\ndocker buildx create --use --name multi\n# Build for linux/amd64 (default on most PCs) and/or linux/arm64 (Apple Silicon)\ndocker buildx build --platform linux/amd64 -t pyramids:latest .\n# For arm64 as well:\n# docker buildx build --platform linux/amd64,linux/arm64 -t pyramids:latest .\n</code></pre>"},{"location":"how-to/docker/#run-the-container","title":"Run the container","text":"<p>Basic run:</p> <pre><code>docker run --rm pyramids:latest\n</code></pre> <p>Interactive shell inside the container:</p> <pre><code>docker run --rm -it pyramids:latest bash\n</code></pre> <p>Mount a local folder (e.g., to access data):</p> <pre><code># Replace /path/to/data with your real path (e.g., use $(wslpath -a 'C:\\data') on WSL)\ndocker run --rm -it -v /path/to/data:/data pyramids:latest bash\n# Example using Windows path in WSL:\n# docker run --rm -it -v \"$(wslpath -a 'C:\\data')\":/data pyramids:latest bash\n</code></pre> <p>Note: Environment variables like <code>GDAL_DATA</code> and <code>PROJ_LIB</code> are already set inside the image. The Pixi environment is in the PATH by default.</p>"},{"location":"how-to/docker/#push-to-github-container-registry-ghcr","title":"Push to GitHub Container Registry (GHCR)","text":"<p>Image naming convention for GHCR: <code>ghcr.io/&lt;owner&gt;/&lt;repo&gt;[:tag]</code>.</p> <p>For this repository, a convenient image name at release time is:</p> <pre><code>ghcr.io/serapieum-of-alex/pyramids\n</code></pre> <p>Example (adjust owner/repo):</p> <pre><code>OWNER=\"Serapieum-of-alex\"\nREPO=\"pyramids\"\nIMAGE=\"ghcr.io/${OWNER,,}/${REPO,,}\"\nVERSION=\"&lt;your-version-here&gt;\"\n\n# Tag the local image to GHCR\ndocker tag pyramids:latest  \"$IMAGE:latest\"\ndocker tag pyramids:latest  \"$IMAGE:$VERSION\"\n\n# Login to GHCR (use a PAT with write:packages)\n# Option A: interactive (paste PAT when prompted)\ndocker login ghcr.io -u &lt;your-github-username&gt;\n# Option B: non-interactive using GHCR_PAT environment variable\n# echo \"$GHCR_PAT\" | docker login ghcr.io -u &lt;your-github-username&gt; --password-stdin\n\n# Push\ndocker push \"$IMAGE:$VERSION\"\ndocker push \"$IMAGE:latest\"\n</code></pre> <p>Automated publish on GitHub Release - This repository includes a workflow: <code>.github/workflows/docker-release.yml</code>. - When a GitHub Release is created, it:   - Builds the Docker image from <code>Dockerfile</code>.   - Tags it with the release version (and <code>latest</code> if not a pre-release) using <code>docker/metadata-action</code>.   - Pushes to <code>ghcr.io/&lt;owner&gt;/&lt;repo&gt;</code> using the repository\u2019s <code>GITHUB_TOKEN</code>.</p> <p>To trigger: create a new release (or use the \"Run workflow\" button for manual dispatch if enabled).</p>"},{"location":"how-to/docker/#delete-images-from-ghcr","title":"Delete images from GHCR","text":"<p>There are two common approaches: web UI and <code>gh</code> CLI.</p> <p>A) Web UI 1. Go to your GitHub org/user \u2192 Packages \u2192 Find the container package named after the repo (e.g., <code>pyramids</code>). 2. Open the package \u2192 Versions \u2192 Delete the version(s) you want. You may need <code>delete:packages</code> scope.</p> <p>B) GitHub CLI (<code>gh</code>)</p> <p>The API endpoints differ for user vs organization. Examples below assume the image name is <code>pyramids</code> under organization <code>Serapieum-of-alex</code>.</p> <p>List versions (organization):</p> <pre><code>ORG=\"Serapieum-of-alex\"\nPKG=\"pyramids\"  # package name in GHCR equals the lowercased repo name by default\n\ngh api -H \"Accept: application/vnd.github+json\" \\\n  \"/orgs/$ORG/packages/container/$PKG/versions\"\n# Optionally pretty-print with jq:\n# gh api -H \"Accept: application/vnd.github+json\" \"/orgs/$ORG/packages/container/$PKG/versions\" | jq '.[] | {id,name,metadata}'\n</code></pre> <p>Delete a specific version by ID (organization):</p> <pre><code>VERSION_ID=123456\n\ngh api -X DELETE -H \"Accept: application/vnd.github+json\" \\\n  \"/orgs/$ORG/packages/container/$PKG/versions/$VERSION_ID\"\n</code></pre> <p>For user-owned packages, replace <code>/orgs/{org}</code> with <code>/user</code>:</p> <pre><code>PKG=\"pyramids\"\n\n# List\ngh api -H \"Accept: application/vnd.github+json\" \"/user/packages/container/$PKG/versions\"\n\n# Delete\nVERSION_ID=123456\ngh api -X DELETE -H \"Accept: application/vnd.github+json\" \"/user/packages/container/$PKG/versions/$VERSION_ID\"\n</code></pre> <p>Notes: - You must authenticate <code>gh</code> (run <code>gh auth login</code>) with a token that has <code>read:packages</code>, <code>write:packages</code>, and <code>delete:packages</code> to delete. - Deleting \"versions\" removes specific tags. Deleting the whole package is also possible via the UI if you need a full reset.</p>"},{"location":"how-to/docker/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>denied: permission: Check you\u2019re logged into GHCR and your token has the right scopes.</li> <li>image name invalid: Ensure the name is lowercase for GHCR (owner and repo must be lowercase in the full image reference).</li> <li>multi-arch build fails: Use <code>docker buildx</code> and ensure QEMU emulation is enabled if cross-building.</li> <li>slow builds: The Dockerfile uses Pixi cache; make sure Docker BuildKit is enabled (default in recent Docker releases).</li> </ul>"},{"location":"how-to/docker/#references","title":"References","text":"<ul> <li>GitHub Container Registry: https://docs.github.com/packages/working-with-a-github-packages-registry/working-with-the-container-registry</li> <li>docker/build-push-action: https://github.com/docker/build-push-action</li> <li>docker/login-action: https://github.com/docker/login-action</li> <li>docker/metadata-action: https://github.com/docker/metadata-action</li> <li>Pixi: https://pixi.sh</li> </ul>"},{"location":"how-to/faq/","title":"FAQ &amp; Troubleshooting","text":""},{"location":"how-to/faq/#installation-issues-gdal","title":"Installation issues (GDAL)","text":"<ul> <li>Symptom: Errors importing GDAL or installing wheels.</li> <li>Tips:</li> <li>Prefer conda-forge: <code>conda install -c conda-forge gdal pyramids</code></li> <li>Ensure Python version matches available GDAL builds.</li> </ul>"},{"location":"how-to/faq/#file-not-found-or-unsupported-format","title":"File not found or unsupported format","text":"<ul> <li>Check the path. Windows users: escape backslashes in Python strings or use raw strings, e.g., <code>r\"C:\\\\path\\\\file.tif\"</code>.</li> <li>Verify the file extension is supported (GeoTIFF/ASC/NetCDF for rasters; GeoJSON/Shapefile/GPKG for vectors).</li> </ul>"},{"location":"how-to/faq/#different-raster-sizes-in-a-datacube","title":"Different raster sizes in a datacube","text":"<ul> <li>Ensure all rasters share the same extent, resolution, and CRS before stacking.</li> <li>Align inputs using the reference raster and resample if needed.</li> </ul>"},{"location":"how-to/faq/#writing-outputs-fails","title":"Writing outputs fails","text":"<ul> <li>Confirm write permissions in the destination folder.</li> <li>For ASCII export, ensure <code>no_data_value</code> and <code>cell_size</code> are valid (see API docs for <code>_io.to_ascii</code>).</li> </ul>"},{"location":"how-to/faq/#crs-or-transform-confusion","title":"CRS or transform confusion","text":"<ul> <li>Use the <code>meta</code> and <code>transform</code> attributes on <code>Dataset</code> to inspect georeferencing.</li> <li>Reproject or align datasets prior to arithmetic operations.</li> </ul>"},{"location":"how-to/faq/#performance-tips","title":"Performance tips","text":"<ul> <li>Work with windows/tiles for large rasters.</li> <li>Use compressed GeoTIFFs when appropriate and keep I/O on local SSD.</li> </ul>"},{"location":"how-to/recipes/","title":"How-to Recipes","text":"<p>Practical snippets for common tasks with pyramids.</p>"},{"location":"how-to/recipes/#read-a-geotiff-and-print-stats","title":"Read a GeoTIFF and print stats","text":"<pre><code>from pyramids.dataset import Dataset\n\nds = Dataset.read_file(\"tests\\\\data\\\\geotiff\\\\dem.tif\")\narr = ds.read()\nprint(arr.min(), arr.max(), arr.mean())\n</code></pre>"},{"location":"how-to/recipes/#convert-a-raster-to-ascii","title":"Convert a raster to ASCII","text":"<pre><code>from pyramids.dataset import Dataset\n\nds = Dataset.read_file(\"tests\\\\data\\\\geotiff\\\\dem.tif\")\nds.to_file(\"out.asc\")\n</code></pre>"},{"location":"how-to/recipes/#crop-a-raster-to-bounding-box","title":"Crop a raster to bounding box","text":"<pre><code>from pyramids.dataset import Dataset\n\nxmin, ymin, xmax, ymax = 6.8, 50.3, 7.2, 50.6  # example bbox\nbbox = (xmin, ymin, xmax, ymax)\nsrc = \"tests\\\\data\\\\geotiff\\\\dem.tif\"\n\nds = Dataset.read_file(src)\n# ds.crop_bbox is illustrative; see actual API for cropping/windowing\n# cropped = ds.crop_bbox(bbox)\n# cropped.to_file(\"dem_cropped.tif\")\n</code></pre>"},{"location":"how-to/recipes/#read-multiple-rasters-into-a-datacube","title":"Read multiple rasters into a datacube","text":"<pre><code>from pyramids.datacube import Datacube\n\ndc = Datacube.read_multiple_files(\"tests\\\\data\\\\geotiff\\\\rhine\", with_order=True, regex_string=r\"\\\\d+\", date=False)\nprint(dc)\n</code></pre>"},{"location":"how-to/recipes/#zonal-statistics-with-a-polygon-layer","title":"Zonal statistics with a polygon layer","text":"<pre><code>from pyramids.dataset import Dataset\nfrom pyramids.featurecollection import FeatureCollection\n\nraster = Dataset.read_file(\"tests\\\\data\\\\geotiff\\\\dem.tif\")\npolys = FeatureCollection.read_file(\"tests\\\\data\\\\geometries\\\\polygons.geojson\")\n# table = raster.zonal_stats(polys)  # replace with the actual method name in your API\n# print(table.head())\n</code></pre>"},{"location":"overview/codebase-map/","title":"Codebase Map","text":"<p>This page summarizes the main modules, key classes, and the public API surface of the <code>pyramids</code> package.</p>"},{"location":"overview/codebase-map/#packages-and-modules","title":"Packages and Modules","text":"<ul> <li>pyramids.abstract_dataset</li> <li>Abstract base class that defines the core dataset interface (read, write, metadata, windowing, reprojection scaffolding).</li> <li>pyramids.dataset</li> <li>Concrete raster dataset implementation with rich I/O, transformations, tiling, pyramid operations, and export utilities.</li> <li>pyramids.datacube</li> <li>Temporal/spatial multi-band or multi-file orchestration (data cube), aggregations, slicing, alignment.</li> <li>pyramids.featurecollection</li> <li>Vector data abstraction for feature collections built on GeoPandas; read/write, selection, join, spatial ops.</li> <li>pyramids._io</li> <li>Internal helpers for reading/serializing data and archives (zip/gzip/tar parsing, ASCII export), and file path parsing.</li> <li>pyramids._utils</li> <li>Utility functions (array ops, geometry helpers, indexing helpers) used across datasets and cubes.</li> <li>pyramids._errors</li> <li>Error/exception types used across the package.</li> </ul>"},{"location":"overview/codebase-map/#key-public-classes","title":"Key Public Classes","text":"<ul> <li>pyramids.dataset.Dataset</li> <li>pyramids.datacube.Datacube</li> <li>pyramids.featurecollection.FeatureCollection</li> </ul>"},{"location":"overview/codebase-map/#representative-public-methods","title":"Representative Public Methods","text":"<ul> <li>Dataset</li> <li>read_file(path, read_only=True, file_i=0)</li> <li>to_file(path, band=0, tile_length=None)</li> <li>read(...), write(...), crop/align/stack helpers (see API Reference)</li> <li>Datacube</li> <li>constructors/helpers to build from folders, patterns, and indexing; slicing and aggregation (see API Reference)</li> <li>FeatureCollection</li> <li>read_file(path), to_file(path, driver=\"geojson\"), selection/clip/buffer (see API Reference)</li> </ul>"},{"location":"overview/codebase-map/#data-flow-high-level","title":"Data Flow (High Level)","text":"<ul> <li>External data (GeoTIFF/ASC/NetCDF/Vector) -&gt; _io parsers -&gt; Dataset/FeatureCollection objects</li> <li>Datasets -&gt; combined via Datacube for temporal/spatial operations</li> <li>Outputs -&gt; Dataset/FeatureCollection to_file(), ASCII/GeoTIFF/GeoJSON exports</li> </ul> <p>See the Architecture section for diagrams and deeper internals, and the API Reference for exhaustive signatures.</p>"},{"location":"overview/codebase-map/#class-dependency-graph","title":"Class &amp; Dependency Graph","text":"<p>Below is a high-level Mermaid class dependency diagram showing the main modules and their primary classes, plus key dependencies between them.</p> <pre><code>classDiagram\n  class AbstractDataset {\n    &lt;&lt;abstract&gt;&gt;\n    +read_file(path, read_only)\n    +to_file(path, band)\n  }\n  class Dataset {\n    +read_file(path, read_only, file_i)\n    +to_file(path, band, tile_length)\n    +read()\n  }\n  class Datacube {\n    +read_multiple_files(...)\n    +open_datacube(...)\n    +to_file(...)\n  }\n  class FeatureCollection {\n    +read_file(path)\n    +to_file(path, driver)\n  }\n  class IO {\n    &lt;&lt;module&gt;&gt;\n  }\n  class Utils {\n    &lt;&lt;module&gt;&gt;\n  }\n\n  AbstractDataset &lt;|-- Dataset\n  Datacube ..&gt; Dataset : uses\n  FeatureCollection ..&gt; IO : uses\n  Dataset ..&gt; IO : uses\n  Dataset ..&gt; Utils : uses\n  FeatureCollection ..&gt; Utils : uses</code></pre> <p>Notes: - Module nodes like <code>_io</code> and <code>_utils</code> represent internal helper modules used by multiple classes. - Method lists are illustrative; see the API Reference for complete signatures.</p>"},{"location":"overview/how-it-works/","title":"How it works","text":"<p>This overview explains the system boundaries and data flow of the pyramids package.</p>"},{"location":"overview/how-it-works/#system-context-c4-context","title":"System Context (C4: Context)","text":"<pre><code>flowchart LR\n  user(User) --&gt;|Provides GIS data paths &amp; commands| pyramids{{pyramids package}}\n  ext1[(Raster files\\nGeoTIFF/ASC/NetCDF)] --&gt; pyramids\n  ext2[(Vector files\\nShapefile/GeoJSON/GeoPackage)] --&gt; pyramids\n  pyramids --&gt; out1[(Processed rasters\\nGeoTIFF/ASC)]\n  pyramids --&gt; out2[(Processed vectors\\nGeoJSON/GPKG)]</code></pre>"},{"location":"overview/how-it-works/#runtime-containers-c4-containers","title":"Runtime Containers (C4: Containers)","text":"<pre><code>flowchart TB\n  subgraph Process\n    A[Dataset]:::c --&gt; B[_io]:::c\n    C[Datacube]:::c --&gt; B\n    D[FeatureCollection]:::c --&gt; B\n    A --&gt; E[_utils]\n    C --&gt; E\n    D --&gt; E\n  end\n  classDef c fill:#eef,stroke:#88f</code></pre>"},{"location":"overview/how-it-works/#components-c4-components","title":"Components (C4: Components)","text":"<pre><code>flowchart LR\n  io[_io: read_file, to_ascii, path parsing]\\nutils[_utils: geometry/index helpers]\n  ds[dataset.Dataset]\\nabs[abstract_dataset.AbstractDataset]\n  dc[datacube.Datacube]\\nfc[featurecollection.FeatureCollection]\n\n  abs --&gt; ds\n  ds --&gt; io\n  dc --&gt; ds\n  fc --&gt; io\n  ds --&gt; utils\n  fc --&gt; utils</code></pre>"},{"location":"overview/how-it-works/#data-flow","title":"Data Flow","text":"<ol> <li>Input paths are parsed; archives (.zip/.gz/.tar) are handled in <code>_io</code>.</li> <li>Raster inputs are loaded into <code>Dataset</code>; vector inputs into <code>FeatureCollection</code>.</li> <li><code>Datacube</code> orchestrates collections of datasets for temporal/spatial ops.</li> <li>Results are exported via <code>to_file</code> (GeoTIFF/ASCII/GeoJSON, etc.).</li> </ol> <p>See the diagrams page for UML and sequence flows.</p>"},{"location":"reference/datacube/","title":"DataCube Class","text":""},{"location":"reference/datacube/#pyramids.datacube.Datacube","title":"<code>pyramids.datacube.Datacube</code>","text":"<p>DataCube.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>class Datacube:\n    \"\"\"DataCube.\"\"\"\n\n    files: List[str]\n    data: np.ndarray\n\n    \"\"\"\n    files:\n        list of geotiff files' names\n    \"\"\"\n\n    def __init__(\n        self,\n        src: Dataset,\n        time_length: int,\n        files: List[str] = None,\n    ):\n        \"\"\"Construct Datacube object.\"\"\"\n        self._base = src\n        self._files = files\n        self._time_length = time_length\n\n        pass\n\n    def __str__(self):\n        \"\"\"__str__.\"\"\"\n        message = f\"\"\"\n            Files: {len(self.files)}\n            Cell size: {self._base.cell_size}\n            EPSG: {self._base.epsg}\n            Dimension: {self.rows} * {self.columns}\n            Mask: {self._base.no_data_value[0]}\n        \"\"\"\n        return message\n\n    def __repr__(self):\n        \"\"\"__repr__.\"\"\"\n        message = f\"\"\"\n            Files: {len(self.files)}\n            Cell size: {self._base.cell_size}\n            EPSG: {self._base.epsg}\n            Dimension: {self.rows} * {self.columns}\n            Mask: {self._base.no_data_value[0]}\n        \"\"\"\n        return message\n\n    @property\n    def base(self) -&gt; Dataset:\n        \"\"\"base.\n\n        Base Dataset\n        \"\"\"\n        return self._base\n\n    @property\n    def files(self):\n        \"\"\"Files.\"\"\"\n        return self._files\n\n    @property\n    def time_length(self) -&gt; int:\n        \"\"\"Length of the dataset.\"\"\"\n        return self._time_length\n\n    @property\n    def rows(self):\n        \"\"\"Number of rows.\"\"\"\n        return self._base.rows\n\n    @property\n    def shape(self):\n        \"\"\"Number of rows.\"\"\"\n        return self.time_length, self.rows, self.columns\n\n    @property\n    def columns(self):\n        \"\"\"Number of columns.\"\"\"\n        return self._base.columns\n\n    @classmethod\n    def create_cube(cls, src: Dataset, dataset_length: int) -&gt; \"Datacube\":\n        \"\"\"Create Datacube.\n\n            - Create a Datacube from a sample raster.\n\n        Args:\n            src (Dataset):\n                Raster object.\n            dataset_length (int):\n                Length of the dataset.\n\n        Returns:\n            Datacube: Datacube object.\n        \"\"\"\n        return cls(src, dataset_length)\n\n    @classmethod\n    def read_multiple_files(\n        cls,\n        path: Union[str, List[str]],\n        with_order: bool = False,\n        regex_string: str = r\"\\d{4}.\\d{2}.\\d{2}\",\n        date: bool = True,\n        file_name_data_fmt: str = None,\n        start: str = None,\n        end: str = None,\n        fmt: str = \"%Y-%m-%d\",\n        extension: str = \".tif\",\n    ) -&gt; \"Datacube\":\n        r\"\"\"read_multiple_files.\n\n            - Read rasters from a folder (or list of files) and create a 3D array with the same 2D dimensions as the\n              first raster and length equal to the number of files.\n\n            - All rasters should have the same dimensions.\n            - If you want to read the rasters with a certain order, the raster file names should contain a date\n              that follows a consistent format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD), e.g. \"MSWEP_1979.01.01.tif\".\n\n        Args:\n            path (str | List[str]):\n                Path of the folder that contains all the rasters, or a list containing the paths of the rasters to read.\n            with_order (bool):\n                True if the raster names follow a certain order. Then the raster names should have a date that follows\n                the same format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD). For example:\n\n                ```python\n                &gt;&gt;&gt; \"MSWEP_1979.01.01.tif\"\n                &gt;&gt;&gt; \"MSWEP_1979.01.02.tif\"\n                &gt;&gt;&gt; ...\n                &gt;&gt;&gt; \"MSWEP_1979.01.20.tif\"\n\n                ```\n\n            regex_string (str):\n                A regex string used to locate the date in the file names. Default is r\"\\d{4}.\\d{2}.\\d{2}\". For example:\n\n                ```python\n                &gt;&gt;&gt; fname = \"MSWEP_YYYY.MM.DD.tif\"\n                &gt;&gt;&gt; regex_string = r\"\\d{4}.\\d{2}.\\d{2}\"\n                ```\n\n                - Or:\n\n                ```python\n                &gt;&gt;&gt; fname = \"MSWEP_YYYY_M_D.tif\"\n                &gt;&gt;&gt; regex_string = r\"\\d{4}_\\d{1}_\\d{1}\"\n                ```\n\n                - If there is a number at the beginning of the name:\n\n                ```python\n                &gt;&gt;&gt; fname = \"1_MSWEP_YYYY_M_D.tif\"\n                &gt;&gt;&gt; regex_string = r\"\\d+\"\n                ```\n\n            date (bool):\n                True if the number in the file name is a date. Default is True.\n            file_name_data_fmt (str):\n                If the file names contain a date and you want to read them ordered. Default is None. For example:\n\n                ```python\n                &gt;&gt;&gt; \"MSWEP_YYYY.MM.DD.tif\"\n                &gt;&gt;&gt; file_name_data_fmt = \"%Y.%m.%d\"\n                ```\n\n            start (str):\n                Start date if you want to read the input raster for a specific period only and not all rasters. If not\n                given, all rasters in the given path will be read.\n            end (str):\n                End date if you want to read the input rasters for a specific period only. If not given, all rasters in\n                the given path will be read.\n            fmt (str):\n                Format of the given date in the start/end parameter.\n            extension (str):\n                The extension of the files you want to read from the given path. Default is \".tif\".\n\n        Returns:\n            Datacube:\n                Instance of the Datacube class.\n\n        Examples:\n            - Read all rasters in a folder:\n\n              ```python\n              &gt;&gt;&gt; from pyramids.datacube import Datacube\n              &gt;&gt;&gt; raster_folder = \"examples/GIS/data/raster-folder\"\n              &gt;&gt;&gt; prec = Datacube.read_multiple_files(raster_folder)\n\n              ```\n\n            - Read from a pre-collected list without ordering:\n\n              ```python\n              &gt;&gt;&gt; import glob\n              &gt;&gt;&gt; search_criteria = \"*.tif\"\n              &gt;&gt;&gt; file_list = glob.glob(os.path.join(raster_folder, search_criteria))\n              &gt;&gt;&gt; prec = Datacube.read_multiple_files(file_list, with_order=False)\n\n              ```\n        \"\"\"\n        if not isinstance(path, str) and not isinstance(path, list):\n            raise TypeError(f\"path input should be string/list type, given{type(path)}\")\n\n        if isinstance(path, str):\n            # check whither the path exists or not\n            if not os.path.exists(path):\n                raise FileNotFoundError(\"The path you have provided does not exist\")\n            # get a list of all files\n            files = os.listdir(path)\n            files = [i for i in files if i.endswith(extension)]\n            # files = glob.glob(os.path.join(path, \"*.tif\"))\n            # check whether there are files or not inside the folder\n            if len(files) &lt; 1:\n                raise FileNotFoundError(\"The path you have provided is empty\")\n        else:\n            files = path[:]\n\n        # to sort the files in the same order as the first number in the name\n        if with_order:\n            match_str_fn = lambda x: re.search(regex_string, x)\n            list_dates = list(map(match_str_fn, files))\n\n            if None in list_dates:\n                raise ValueError(\n                    \"The date format/separator given does not match the file names\"\n                )\n            if date:\n                if file_name_data_fmt is None:\n                    raise ValueError(\n                        f\"To read the raster with a certain order (with_order = {with_order}, then you have to enter \"\n                        f\"the value of the parameter file_name_data_fmt(given: {file_name_data_fmt})\"\n                    )\n                fn = lambda x: dt.datetime.strptime(x.group(), file_name_data_fmt)\n            else:\n                fn = lambda x: int(x.group())\n            list_dates = list(map(fn, list_dates))\n\n            df = pd.DataFrame()\n            df[\"files\"] = files\n            df[\"date\"] = list_dates\n            df.sort_values(\"date\", inplace=True, ignore_index=True)\n            files = df.loc[:, \"files\"].values\n\n        if start is not None or end is not None:\n            if date:\n                start = dt.datetime.strptime(start, fmt)\n                end = dt.datetime.strptime(end, fmt)\n\n                files = (\n                    df.loc[start &lt;= df[\"date\"], :]\n                    .loc[df[\"date\"] &lt;= end, \"files\"]\n                    .values\n                )\n            else:\n                files = (\n                    df.loc[start &lt;= df[\"date\"], :]\n                    .loc[df[\"date\"] &lt;= end, \"files\"]\n                    .values\n                )\n\n        if not isinstance(path, list):\n            # add the path to all the files\n            files = [f\"{path}/{i}\" for i in files]\n        # create a 3d array with the 2d dimension of the first raster and the len\n        # of the number of rasters in the folder\n        sample = Dataset.read_file(files[0])\n\n        return cls(sample, len(files), files)\n\n    def open_datacube(self, band: int = 0) -&gt; None:\n        \"\"\"Open the datacube.\n\n        Read values from the given band as arrays for all files.\n\n        Args:\n            band (int): Index of the band you want to read. Default is 0.\n\n        Returns:\n            None: Loads values into the internal 3D array [time, rows, cols] in-place.\n        \"\"\"\n        # check the given band number\n        if not hasattr(self, \"base\"):\n            raise ValueError(\n                \"please use the read_multiple_files method to get the files (tiff/ascii) in the\"\n                \"dataset directory\"\n            )\n        if band &gt; self.base.band_count - 1:\n            raise ValueError(\n                f\"the raster has only {self.base.band_count} check the given band number\"\n            )\n        # fill the array with no_data_value data\n        self._values = np.ones(\n            (\n                self.time_length,\n                self.base.rows,\n                self.base.columns,\n            )\n        )\n        self._values[:, :, :] = np.nan\n\n        for i, file_i in enumerate(self.files):\n            # read the tif file\n            raster_i = gdal.Open(f\"{file_i}\")\n            self._values[i, :, :] = raster_i.GetRasterBand(band + 1).ReadAsArray()\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        \"\"\"Values.\n\n        - The attribute where the dataset array is stored.\n        - the 3D numpy array, [dataset length, rows, cols], [dataset length, lons, lats]\n        \"\"\"\n        return self._values\n\n    @values.setter\n    def values(self, val):\n        \"\"\"Values.\n\n        - setting the data (array) does not allow different dimension from the dimension that has been\n        defined in creating the dataset.\n        \"\"\"\n        # if the attribute is defined before check the dimension\n        if hasattr(self, \"values\"):\n            if self._values.shape != val.shape:\n                raise ValueError(\n                    f\"The dimension of the new data: {val.shape}, differs from the dimension of the \"\n                    f\"original dataset: {self._values.shape}, please redefine the base Dataset and \"\n                    f\"dataset_length first\"\n                )\n\n        self._values = val\n\n    @values.deleter\n    def values(self):\n        self._values = None\n\n    def __getitem__(self, key):\n        \"\"\"Getitem.\"\"\"\n        if not hasattr(self, \"values\"):\n            raise AttributeError(\"Please use the read_dataset method to read the data\")\n        return self._values[key, :, :]\n\n    def __setitem__(self, key, value: np.ndarray):\n        \"\"\"Setitem.\"\"\"\n        if not hasattr(self, \"values\"):\n            raise AttributeError(\"Please use the read_dataset method to read the data\")\n        self._values[key, :, :] = value\n\n    def __len__(self):\n        \"\"\"Length of the Datacube.\"\"\"\n        return self._values.shape[0]\n\n    def __iter__(self):\n        \"\"\"Iterate over the Datacube.\"\"\"\n        return iter(self._values[:])\n\n    def head(self, n: int = 5):\n        \"\"\"First 5 Datasets.\"\"\"\n        return self._values[:n, :, :]\n\n    def tail(self, n: int = -5):\n        \"\"\"Last 5 Datasets.\"\"\"\n        return self._values[n:, :, :]\n\n    def first(self):\n        \"\"\"First Dataset.\"\"\"\n        return self._values[0, :, :]\n\n    def last(self):\n        \"\"\"Last Dataset.\"\"\"\n        return self._values[-1, :, :]\n\n    def iloc(self, i) -&gt; Dataset:\n        \"\"\"iloc.\n\n            - Access dataset array using index.\n\n        Args:\n            i (int):\n                Index of the dataset to access.\n\n        Returns:\n            Dataset: Dataset object.\n        \"\"\"\n        if not hasattr(self, \"values\"):\n            raise DatasetNoFoundError(\"please read the dataset first\")\n        arr = self._values[i, :, :]\n        dst = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self.base.raster, 0)\n        dst.GetRasterBand(1).WriteArray(arr)\n        return Dataset(dst)\n\n    def plot(self, band: int = 0, exclude_value: Any = None, **kwargs: Any) -&gt; \"ArrayGlyph\":\n        r\"\"\"Read Array.\n\n            - read the values stored in a given band.\n\n        Args:\n            band (int):\n                The band you want to get its data. Default is 0.\n            exclude_value (Any):\n                Value to exclude from the plot. Default is None.\n            **kwargs:\n                | Parameter                  | Type                  | Description |\n                |----------------------------|-----------------------|-------------|\n                | points                     | array                 | 3-column array: col 1 = value to display, col 2 = row index, col 3 = column index. Columns 2 and 3 indicate the location of the point. |\n                | point_color                | str                   | Color of the points. |\n                | point_size                 | Any                   | Size of the points. |\n                | pid_color                  | str                   | Color of the annotation of the point. Default is blue. |\n                | pid_size                   | Any                   | Size of the point annotation. |\n                | figsize                    | tuple, optional       | Figure size. Default is `(8, 8)`. |\n                | title                      | str, optional         | Title of the plot. Default is `'Total Discharge'`. |\n                | title_size                 | int, optional         | Title size. Default is `15`. |\n                | orientation                | str, optional         | Orientation of the color bar (`horizontal` or `vertical`). Default is `'vertical'`. |\n                | rotation                   | number, optional      | Rotation of the color bar label. Default is `-90`. |\n                | cbar_length                | float, optional       | Ratio to control the height of the color bar. Default is `0.75`. |\n                | ticks_spacing              | int, optional         | Spacing in the color bar ticks. Default is `2`. |\n                | cbar_label_size            | int, optional         | Size of the color bar label. Default is `12`. |\n                | cbar_label                 | str, optional         | Label of the color bar. Default is `'Discharge m\u00b3/s'`. |\n                | color_scale                | int, optional         | Color scaling mode (default = `1`): 1 = normal scale, 2 = power scale, 3 = SymLogNorm scale, 4 = PowerNorm scale, 5 = BoundaryNorm scale. |\n                | gamma                      | float, optional       | Value needed for `color_scale=2`. Default is `1/2`. |\n                | line_threshold             | float, optional       | Value needed for `color_scale=3`. Default is `0.0001`. |\n                | line_scale                 | float, optional       | Value needed for `color_scale=3`. Default is `0.001`. |\n                | bounds                     | list                  | Discrete bounds for `color_scale=4`. Default is `None`. |\n                | midpoint                   | float, optional       | Value needed for `color_scale=5`. Default is `0`. |\n                | cmap                       | str, optional         | Color map style. Default is `'coolwarm_r'`. |\n                | display_cell_value         | bool                  | Whether to display the values of the cells as text. |\n                | num_size                   | int, optional         | Size of the numbers plotted on top of each cell. Default is `8`. |\n                | background_color_threshold | float \\| int, optional| Threshold for deciding number color: if value &gt; threshold \u2192 black; else white. If `None`, uses `max_value/2`. Default is `None`. |\n\n\n        Returns:\n            ArrayGlyph: A plotting/animation handle (from cleopatra.ArrayGlyph).\n        \"\"\"\n        import_cleopatra(\n            \"The current funcrion uses cleopatra package to for plotting, please install it manually, for more info \"\n            \"check https://github.com/Serapieum-of-alex/cleopatra\"\n        )\n        from cleopatra.array_glyph import ArrayGlyph\n\n        data = self.values\n\n        exclude_value = (\n            [self.base.no_data_value[band], exclude_value]\n            if exclude_value is not None\n            else [self.base.no_data_value[band]]\n        )\n\n        cleo = ArrayGlyph(data, exclude_value=exclude_value)\n        time = list(range(self.time_length))\n        cleo.animate(time, **kwargs)\n        return cleo\n\n    def to_file(\n        self, path: Union[str, List[str]], driver: str = \"geotiff\", band: int = 0\n    ):\n        \"\"\"Save to geotiff format.\n\n            saveRaster saves a raster to a path\n\n        Args:\n            path (Union[str, List[str]]):\n                a path includng the name of the raster and extention.\n            driver (str):\n                driver = \"geotiff\".\n            band (int):\n                band index, needed only in case of ascii drivers. Default is 1.\n\n        Examples:\n            - Save to a file:\n\n              ```python\n              &gt;&gt;&gt; raster_obj = Dataset.read_file(\"path/to/file/***.tif\")\n              &gt;&gt;&gt; output_path = \"examples/GIS/data/save_raster_test.tif\"\n              &gt;&gt;&gt; raster_obj.to_file(output_path)\n\n              ```\n        \"\"\"\n        ext = CATALOG.get_extension(driver)\n\n        if isinstance(path, str):\n            if not Path(path).exists():\n                Path(path).mkdir(parents=True, exist_ok=True)\n            path = [f\"{path}/{i}.{ext}\" for i in range(self.time_length)]\n        else:\n            if not len(path) == self.time_length:\n                raise ValueError(\n                    f\"Length of the given paths: {len(path)} does not equal number of rasters in the data cube: {self.time_length}\"\n                )\n            if not Path(path[0]).parent.exists():\n                Path(path[0]).parent.mkdir(parents=True, exist_ok=True)\n\n        for i in range(self.time_length):\n            src = self.iloc(i)\n            src.to_file(path[i], band=band)\n\n    def to_crs(\n        self,\n        to_epsg: int = 3857,\n        method: str = \"nearest neighbor\",\n        maintain_alignment: int = False,\n    ) -&gt; None:\n        \"\"\"to_epsg.\n\n            - to_epsg reprojects a raster to any projection (default the WGS84 web mercator projection,\n            without resampling) The function returns a GDAL in-memory file object, where you can ReadAsArray etc.\n\n        Args:\n            to_epsg (int):\n                Reference number to the new projection (https://epsg.io/)\n                (default 3857 the reference no of WGS84 web mercator).\n            method (str):\n                Resampling technique. Default is \"Nearest\". See https://gisgeography.com/raster-resampling/.\n                \"Nearest\" for nearest neighbor, \"cubic\" for cubic convolution, \"bilinear\" for bilinear.\n            maintain_alignment (bool):\n                True to maintain the number of rows and columns of the raster the same after reprojection.\n                Default is False.\n\n        Returns:\n            None: Updates the datacube values and base in place after reprojection.\n\n        Examples:\n            - Reproject dataset to EPSG:3857:\n\n              ```python\n              &gt;&gt;&gt; from pyramids.dataset import Dataset\n              &gt;&gt;&gt; src = Dataset.read_file(\"path/raster_name.tif\")\n              &gt;&gt;&gt; projected_raster = src.to_crs(to_epsg=3857)\n\n              ```\n        \"\"\"\n        for i in range(self.time_length):\n            src = self.iloc(i)\n            dst = src.to_crs(\n                to_epsg, method=method, maintain_alignment=maintain_alignment\n            )\n            arr = dst.read_array()\n            if i == 0:\n                # create the array\n                array = (\n                    np.ones(\n                        (\n                            self.time_length,\n                            arr.shape[0],\n                            arr.shape[1],\n                        )\n                    )\n                    * np.nan\n                )\n            array[i, :, :] = arr\n\n        self._values = array\n        # use the last src as\n        self._base = dst\n\n    def crop(\n        self, mask: Union[Dataset, str], inplace: bool = False, touch: bool = True\n    ) -&gt; Union[None, Dataset]:\n        \"\"\"crop.\n\n            crop matches the location of nodata value from src raster to dst raster. Mask is where the NoDatavalue will\n            be taken and the location of this value. src_dir is path to the folder where rasters exist where we need to\n            put the NoDataValue of the mask in RasterB at the same locations.\n\n        Args:\n            mask (Dataset):\n                Dataset object of the mask raster to crop the rasters (to get the NoData value and its location in the\n                array). Mask should include the name of the raster and the extension like \"data/dem.tif\", or you can\n                read the mask raster using gdal and use it as the first parameter to the function.\n            inplace (bool):\n                True to make the changes in place.\n            touch (bool):\n                Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask.\n                Default is True.\n\n        Returns:\n            Union[None, \"Datacube\"]: New rasters have the values from rasters in B_input_path with the NoDataValue in\n            the same locations as raster A.\n\n        Examples:\n            - Crop aligned rasters using a DEM mask:\n\n              ```python\n              &gt;&gt;&gt; dem_path = \"examples/GIS/data/acc4000.tif\"\n              &gt;&gt;&gt; src_path = \"examples/GIS/data/aligned_rasters/\"\n              &gt;&gt;&gt; out_path = \"examples/GIS/data/crop_aligned_folder/\"\n              &gt;&gt;&gt; Datacube.crop(dem_path, src_path, out_path)\n\n              ```\n        \"\"\"\n        for i in range(self.time_length):\n            src = self.iloc(i)\n            dst = src.crop(mask, touch=touch)\n            arr = dst.read_array()\n            if i == 0:\n                # create the array\n                array = (\n                    np.ones(\n                        (self.time_length, arr.shape[0], arr.shape[1]),\n                    )\n                    * np.nan\n                )\n\n            array[i, :, :] = arr\n\n        if inplace:\n            self._values = array\n            # use the last src as\n            self._base = dst\n        else:\n            dataset = Datacube(dst, time_length=self.time_length)\n            dataset._values = array\n            return dataset\n\n    # # TODO: merge ReprojectDataset and ProjectRaster they are almost the same\n    # # TODO: still needs to be tested\n    # @staticmethod\n    # def to_epsg(\n    #         src: gdal.Datacube,\n    #         to_epsg: int = 3857,\n    #         cell_size: int = [],\n    #         method: str = \"Nearest\",\n    #\n    # ) -&gt; gdal.Datacube:\n    #     \"\"\"to_epsg.\n    #\n    #         - to_epsg reprojects and resamples a folder of rasters to any projection\n    #         (default the WGS84 web mercator projection, without resampling)\n    #\n    #     Parameters\n    #     ----------\n    #     src: [gdal dataset]\n    #         gdal dataset object (src=gdal.Open(\"dem.tif\"))\n    #     to_epsg: [integer]\n    #          reference number to the new projection (https://epsg.io/)\n    #         (default 3857 the reference no of WGS84 web mercator )\n    #     cell_size: [integer]\n    #          number to resample the raster cell size to a new cell size\n    #         (default empty so raster will not be resampled)\n    #     method: [String]\n    #         resampling technique default is \"Nearest\"\n    #         https://gisgeography.com/raster-resampling/\n    #         \"Nearest\" for nearest neighbor,\"cubic\" for cubic convolution,\n    #         \"bilinear\" for bilinear\n    #\n    #     Returns\n    #     -------\n    #     raster: [gdal Datacube]\n    #          a GDAL in-memory file object, where you can ReadAsArray etc.\n    #     \"\"\"\n    #     if not isinstance(src, gdal.Datacube):\n    #         raise TypeError(\n    #             \"src should be read using gdal (gdal dataset please read it using gdal\"\n    #             f\" library) given {type(src)}\"\n    #         )\n    #     if not isinstance(to_epsg, int):\n    #         raise TypeError(\n    #             \"please enter correct integer number for to_epsg more information \"\n    #             f\"https://epsg.io/, given {type(to_epsg)}\"\n    #         )\n    #     if not isinstance(method, str):\n    #         raise TypeError(\n    #             \"please enter correct method more information see \" \"docmentation \"\n    #         )\n    #\n    #     if cell_size:\n    #         assert isinstance(cell_size, int) or isinstance(\n    #             cell_size, float\n    #         ), \"please enter an integer or float cell size\"\n    #\n    #     if method == \"Nearest\":\n    #         method = gdal.GRA_NearestNeighbour\n    #     elif method == \"cubic\":\n    #         method = gdal.GRA_Cubic\n    #     elif method == \"bilinear\":\n    #         method = gdal.GRA_Bilinear\n    #\n    #     src_proj = src.GetProjection()\n    #     src_gt = src.GetGeoTransform()\n    #     src_x = src.RasterXSize\n    #     src_y = src.RasterYSize\n    #     dtype = src.GetRasterBand(1).DataType\n    #     # spatial ref\n    #     src_sr = osr.SpatialReference(wkt=src_proj)\n    #     src_epsg = src_sr.GetAttrValue(\"AUTHORITY\", 1)\n    #\n    #     # distination\n    #     # spatial ref\n    #     dst_epsg = osr.SpatialReference()\n    #     dst_epsg.ImportFromEPSG(to_epsg)\n    #     # transformation factors\n    #     tx = osr.CoordinateTransformation(src_sr, dst_epsg)\n    #\n    #     # incase the source crs is GCS and longitude is in the west hemisphere gdal\n    #     # reads longitude fron 0 to 360 and transformation factor wont work with valeus\n    #     # greater than 180\n    #     if src_epsg == \"4326\" and src_gt[0] &gt; 180:\n    #         lng_new = src_gt[0] - 360\n    #         # transform the right upper corner point\n    #         (ulx, uly, ulz) = tx.TransformPoint(lng_new, src_gt[3])\n    #         # transform the right lower corner point\n    #         (lrx, lry, lrz) = tx.TransformPoint(\n    #             lng_new + src_gt[1] * src_x, src_gt[3] + src_gt[5] * src_y\n    #         )\n    #     else:\n    #         # transform the right upper corner point\n    #         (ulx, uly, ulz) = tx.TransformPoint(src_gt[0], src_gt[3])\n    #         # transform the right lower corner point\n    #         (lrx, lry, lrz) = tx.TransformPoint(\n    #             src_gt[0] + src_gt[1] * src_x, src_gt[3] + src_gt[5] * src_y\n    #         )\n    #\n    #     if not cell_size:\n    #         # the result raster has the same pixcel size as the source\n    #         # check if the coordinate system is GCS convert the distance from angular to metric\n    #         if src_epsg == \"4326\":\n    #             coords_1 = (src_gt[3], src_gt[0])\n    #             coords_2 = (src_gt[3], src_gt[0] + src_gt[1])\n    #             #            pixel_spacing=geopy.distance.vincenty(coords_1, coords_2).m\n    #             pixel_spacing = FeatureCollection.GCSDistance(coords_1, coords_2)\n    #         else:\n    #             pixel_spacing = src_gt[1]\n    #     else:\n    #         # if src_epsg.GetAttrValue('AUTHORITY', 1) != \"4326\":\n    #         #     assert (cell_size &gt; 1), \"please enter cell size greater than 1\"\n    #         # if the user input a cell size resample the raster\n    #         pixel_spacing = cell_size\n    #\n    #     # create a new raster\n    #     cols = int(np.round(abs(lrx - ulx) / pixel_spacing))\n    #     rows = int(np.round(abs(uly - lry) / pixel_spacing))\n    #     dst = Dataset._create_dataset(cols, rows, 1, dtype, driver=\"MEM\")\n    #\n    #     # new geotransform\n    #     new_geo = (ulx, pixel_spacing, src_gt[2], uly, src_gt[4], -pixel_spacing)\n    #     # set the geotransform\n    #     dst.SetGeoTransform(new_geo)\n    #     # set the projection\n    #     dst.SetProjection(dst_epsg.ExportToWkt())\n    #     # set the no data value\n    #     no_data_value = src.GetRasterBand(1).GetNoDataValue()\n    #     dst = Dataset._set_no_data_value(dst, no_data_value)\n    #     # perform the projection &amp; resampling\n    #     gdal.ReprojectImage(\n    #         src, dst, src_sr.ExportToWkt(), dst_epsg.ExportToWkt(), method\n    #     )\n    #\n    #     return dst\n\n    def align(self, alignment_src: Dataset) -&gt; None:\n        \"\"\"matchDataAlignment.\n\n        This function matches the coordinate system and the number of rows and columns between two rasters. Raster A\n        is the source of the coordinate system, number of rows, number of columns, and cell size. The result will be\n        a raster with the same structure as Raster A but with values from Raster B using nearest neighbor interpolation.\n\n        Args:\n            alignment_src (Dataset):\n                Dataset to use as the spatial template (CRS, rows, columns).\n\n        Returns:\n            None:\n                Updates the datacube values in place to match the alignment of alignment_src.\n\n        Examples:\n            - Align all rasters in the datacube to a DEM raster:\n\n              ```python\n              &gt;&gt;&gt; dem_path = \"01GIS/inputs/4000/acc4000.tif\"\n              &gt;&gt;&gt; prec_in_path = \"02Precipitation/CHIRPS/Daily/\"\n              &gt;&gt;&gt; prec_out_path = \"02Precipitation/4km/\"\n              &gt;&gt;&gt; Dataset.align(dem_path, prec_in_path, prec_out_path)\n\n              ```\n        \"\"\"\n        if not isinstance(alignment_src, Dataset):\n            raise TypeError(\"alignment_src input should be a Dataset object\")\n\n        for i in range(self.time_length):\n            src = self.iloc(i)\n            dst = src.align(alignment_src)\n            arr = dst.read_array()\n            if i == 0:\n                # create the array\n                array = (\n                    np.ones(\n                        (self.time_length, arr.shape[0], arr.shape[1]),\n                    )\n                    * np.nan\n                )\n\n            array[i, :, :] = arr\n\n        self._values = array\n        # use the last src as\n        self._base = dst\n\n    @staticmethod\n    def merge(\n        src: List[str],\n        dst: str,\n        no_data_value: Union[float, int, str] = \"0\",\n        init: Union[float, int, str] = \"nan\",\n        n: Union[float, int, str] = \"nan\",\n    ) -&gt; None:\n        \"\"\"merge.\n\n            Merges a group of rasters into one raster.\n\n        Args:\n            src (List[str]):\n                List of paths to all input rasters.\n            dst (str):\n                Path to the output raster.\n            no_data_value (float | int | str):\n                Assign a specified nodata value to output bands.\n            init (float | int | str):\n                Pre-initialize the output image bands with these values. However, it is not\n                marked as the nodata value in the output file. If only one value is given, the same value is used\n                in all the bands.\n            n (float | int | str):\n                Ignore pixels from files being merged in with this pixel value.\n\n        Returns:\n            None\n        \"\"\"\n        # run the command\n        # cmd = \"gdal_merge.py -o merged_image_1.tif\"\n        # subprocess.call(cmd.split() + file_list)\n        # vrt = gdal.BuildVRT(\"merged.vrt\", file_list)\n        # src = gdal.Translate(\"merged_image.tif\", vrt)\n\n        parameters = (\n            [\"\", \"-o\", dst]\n            + src\n            + [\n                \"-co\",\n                \"COMPRESS=LZW\",\n                \"-init\",\n                str(init),\n                \"-a_nodata\",\n                str(no_data_value),\n                \"-n\",\n                str(n),\n            ]\n        )  # '-separate'\n        gdal_merge.main(parameters)\n\n    def apply(self, ufunc: Callable) -&gt; None:\n        \"\"\"apply.\n\n        Apply a function on each raster in the datacube.\n\n        Args:\n            ufunc (Callable):\n                Callable universal function (builtin or user defined). See\n                https://numpy.org/doc/stable/reference/ufuncs.html\n                To create a ufunc from a normal function: https://numpy.org/doc/stable/reference/generated/numpy.frompyfunc.html\n\n        Returns:\n            None\n\n        Examples:\n            - Apply a simple modulo operation to each value:\n\n              ```python\n              &gt;&gt;&gt; def func(val):\n              &gt;&gt;&gt;    return val%2\n              &gt;&gt;&gt; ufunc = np.frompyfunc(func, 1, 1)\n              &gt;&gt;&gt; dataset.apply(ufunc)\n\n              ```\n        \"\"\"\n        if not callable(ufunc):\n            raise TypeError(\"The Second argument should be a function\")\n        arr = self.values\n        no_data_value = self.base.no_data_value[0]\n        # execute the function on each raster\n        arr[~np.isclose(arr, no_data_value, rtol=0.001)] = ufunc(\n            arr[~np.isclose(arr, no_data_value, rtol=0.001)]\n        )\n\n    def overlay(\n        self,\n        classes_map,\n        exclude_value: Union[float, int] = None,\n    ) -&gt; Dict[List[float], List[float]]:\n        \"\"\"Overlay.\n\n        Args:\n            classes_map (Dataset):\n                Dataset object for the raster that has classes to overlay with.\n            exclude_value (float | int, optional):\n                Values to exclude from extracted values. Defaults to None.\n\n        Returns:\n            Dict[List[float], List[float]]:\n                Dictionary with a list of values in the basemap as keys and for each key a list of all the\n                intersected values in the maps from the path.\n        \"\"\"\n        values = {}\n        for i in range(self.time_length):\n            src = self.iloc(i)\n            dict_i = src.overlay(classes_map, exclude_value)\n\n            # these are the distinct values from the BaseMap which are keys in the\n            # values dict with each one having a list of values\n            classes = list(dict_i.keys())\n\n            for class_i in classes:\n                if class_i not in values.keys():\n                    values[class_i] = list()\n\n                values[class_i] = values[class_i] + dict_i[class_i]\n\n        return values\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.data","title":"<code>data</code>  <code>instance-attribute</code>","text":"files <p>list of geotiff files' names</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.base","title":"<code>base</code>  <code>property</code>","text":"<p>base.</p> <p>Base Dataset</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.files","title":"<code>files</code>  <code>property</code>","text":"<p>Files.</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.time_length","title":"<code>time_length</code>  <code>property</code>","text":"<p>Length of the dataset.</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.rows","title":"<code>rows</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Number of rows.</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Number of columns.</p>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.values","title":"<code>values</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Values.</p> <ul> <li>The attribute where the dataset array is stored.</li> <li>the 3D numpy array, [dataset length, rows, cols], [dataset length, lons, lats]</li> </ul>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__init__","title":"<code>__init__(src, time_length, files=None)</code>","text":"<p>Construct Datacube object.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __init__(\n    self,\n    src: Dataset,\n    time_length: int,\n    files: List[str] = None,\n):\n    \"\"\"Construct Datacube object.\"\"\"\n    self._base = src\n    self._files = files\n    self._time_length = time_length\n\n    pass\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__str__","title":"<code>__str__()</code>","text":"<p>str.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __str__(self):\n    \"\"\"__str__.\"\"\"\n    message = f\"\"\"\n        Files: {len(self.files)}\n        Cell size: {self._base.cell_size}\n        EPSG: {self._base.epsg}\n        Dimension: {self.rows} * {self.columns}\n        Mask: {self._base.no_data_value[0]}\n    \"\"\"\n    return message\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__repr__","title":"<code>__repr__()</code>","text":"<p>repr.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __repr__(self):\n    \"\"\"__repr__.\"\"\"\n    message = f\"\"\"\n        Files: {len(self.files)}\n        Cell size: {self._base.cell_size}\n        EPSG: {self._base.epsg}\n        Dimension: {self.rows} * {self.columns}\n        Mask: {self._base.no_data_value[0]}\n    \"\"\"\n    return message\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.create_cube","title":"<code>create_cube(src, dataset_length)</code>  <code>classmethod</code>","text":"<p>Create Datacube.</p> <pre><code>- Create a Datacube from a sample raster.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Dataset</code> <p>Raster object.</p> required <code>dataset_length</code> <code>int</code> <p>Length of the dataset.</p> required <p>Returns:</p> Name Type Description <code>Datacube</code> <code>Datacube</code> <p>Datacube object.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>@classmethod\ndef create_cube(cls, src: Dataset, dataset_length: int) -&gt; \"Datacube\":\n    \"\"\"Create Datacube.\n\n        - Create a Datacube from a sample raster.\n\n    Args:\n        src (Dataset):\n            Raster object.\n        dataset_length (int):\n            Length of the dataset.\n\n    Returns:\n        Datacube: Datacube object.\n    \"\"\"\n    return cls(src, dataset_length)\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.read_multiple_files","title":"<code>read_multiple_files(path, with_order=False, regex_string='\\\\d{4}.\\\\d{2}.\\\\d{2}', date=True, file_name_data_fmt=None, start=None, end=None, fmt='%Y-%m-%d', extension='.tif')</code>  <code>classmethod</code>","text":"<p>read_multiple_files.</p> <pre><code>- Read rasters from a folder (or list of files) and create a 3D array with the same 2D dimensions as the\n  first raster and length equal to the number of files.\n\n- All rasters should have the same dimensions.\n- If you want to read the rasters with a certain order, the raster file names should contain a date\n  that follows a consistent format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD), e.g. \"MSWEP_1979.01.01.tif\".\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | List[str]</code> <p>Path of the folder that contains all the rasters, or a list containing the paths of the rasters to read.</p> required <code>with_order</code> <code>bool</code> <p>True if the raster names follow a certain order. Then the raster names should have a date that follows the same format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD). For example:</p> <pre><code>&gt;&gt;&gt; \"MSWEP_1979.01.01.tif\"\n&gt;&gt;&gt; \"MSWEP_1979.01.02.tif\"\n&gt;&gt;&gt; ...\n&gt;&gt;&gt; \"MSWEP_1979.01.20.tif\"\n</code></pre> <code>False</code> <code>regex_string</code> <code>str</code> <p>A regex string used to locate the date in the file names. Default is r\"\\d{4}.\\d{2}.\\d{2}\". For example:</p> <pre><code>&gt;&gt;&gt; fname = \"MSWEP_YYYY.MM.DD.tif\"\n&gt;&gt;&gt; regex_string = r\"\\d{4}.\\d{2}.\\d{2}\"\n</code></pre> <ul> <li>Or:</li> </ul> <pre><code>&gt;&gt;&gt; fname = \"MSWEP_YYYY_M_D.tif\"\n&gt;&gt;&gt; regex_string = r\"\\d{4}_\\d{1}_\\d{1}\"\n</code></pre> <ul> <li>If there is a number at the beginning of the name:</li> </ul> <pre><code>&gt;&gt;&gt; fname = \"1_MSWEP_YYYY_M_D.tif\"\n&gt;&gt;&gt; regex_string = r\"\\d+\"\n</code></pre> <code>'\\\\d{4}.\\\\d{2}.\\\\d{2}'</code> <code>date</code> <code>bool</code> <p>True if the number in the file name is a date. Default is True.</p> <code>True</code> <code>file_name_data_fmt</code> <code>str</code> <p>If the file names contain a date and you want to read them ordered. Default is None. For example:</p> <pre><code>&gt;&gt;&gt; \"MSWEP_YYYY.MM.DD.tif\"\n&gt;&gt;&gt; file_name_data_fmt = \"%Y.%m.%d\"\n</code></pre> <code>None</code> <code>start</code> <code>str</code> <p>Start date if you want to read the input raster for a specific period only and not all rasters. If not given, all rasters in the given path will be read.</p> <code>None</code> <code>end</code> <code>str</code> <p>End date if you want to read the input rasters for a specific period only. If not given, all rasters in the given path will be read.</p> <code>None</code> <code>fmt</code> <code>str</code> <p>Format of the given date in the start/end parameter.</p> <code>'%Y-%m-%d'</code> <code>extension</code> <code>str</code> <p>The extension of the files you want to read from the given path. Default is \".tif\".</p> <code>'.tif'</code> <p>Returns:</p> Name Type Description <code>Datacube</code> <code>Datacube</code> <p>Instance of the Datacube class.</p> <p>Examples:</p> <ul> <li>Read all rasters in a folder:</li> </ul> <pre><code>&gt;&gt;&gt; from pyramids.datacube import Datacube\n&gt;&gt;&gt; raster_folder = \"examples/GIS/data/raster-folder\"\n&gt;&gt;&gt; prec = Datacube.read_multiple_files(raster_folder)\n</code></pre> <ul> <li>Read from a pre-collected list without ordering:</li> </ul> <pre><code>&gt;&gt;&gt; import glob\n&gt;&gt;&gt; search_criteria = \"*.tif\"\n&gt;&gt;&gt; file_list = glob.glob(os.path.join(raster_folder, search_criteria))\n&gt;&gt;&gt; prec = Datacube.read_multiple_files(file_list, with_order=False)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>@classmethod\ndef read_multiple_files(\n    cls,\n    path: Union[str, List[str]],\n    with_order: bool = False,\n    regex_string: str = r\"\\d{4}.\\d{2}.\\d{2}\",\n    date: bool = True,\n    file_name_data_fmt: str = None,\n    start: str = None,\n    end: str = None,\n    fmt: str = \"%Y-%m-%d\",\n    extension: str = \".tif\",\n) -&gt; \"Datacube\":\n    r\"\"\"read_multiple_files.\n\n        - Read rasters from a folder (or list of files) and create a 3D array with the same 2D dimensions as the\n          first raster and length equal to the number of files.\n\n        - All rasters should have the same dimensions.\n        - If you want to read the rasters with a certain order, the raster file names should contain a date\n          that follows a consistent format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD), e.g. \"MSWEP_1979.01.01.tif\".\n\n    Args:\n        path (str | List[str]):\n            Path of the folder that contains all the rasters, or a list containing the paths of the rasters to read.\n        with_order (bool):\n            True if the raster names follow a certain order. Then the raster names should have a date that follows\n            the same format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD). For example:\n\n            ```python\n            &gt;&gt;&gt; \"MSWEP_1979.01.01.tif\"\n            &gt;&gt;&gt; \"MSWEP_1979.01.02.tif\"\n            &gt;&gt;&gt; ...\n            &gt;&gt;&gt; \"MSWEP_1979.01.20.tif\"\n\n            ```\n\n        regex_string (str):\n            A regex string used to locate the date in the file names. Default is r\"\\d{4}.\\d{2}.\\d{2}\". For example:\n\n            ```python\n            &gt;&gt;&gt; fname = \"MSWEP_YYYY.MM.DD.tif\"\n            &gt;&gt;&gt; regex_string = r\"\\d{4}.\\d{2}.\\d{2}\"\n            ```\n\n            - Or:\n\n            ```python\n            &gt;&gt;&gt; fname = \"MSWEP_YYYY_M_D.tif\"\n            &gt;&gt;&gt; regex_string = r\"\\d{4}_\\d{1}_\\d{1}\"\n            ```\n\n            - If there is a number at the beginning of the name:\n\n            ```python\n            &gt;&gt;&gt; fname = \"1_MSWEP_YYYY_M_D.tif\"\n            &gt;&gt;&gt; regex_string = r\"\\d+\"\n            ```\n\n        date (bool):\n            True if the number in the file name is a date. Default is True.\n        file_name_data_fmt (str):\n            If the file names contain a date and you want to read them ordered. Default is None. For example:\n\n            ```python\n            &gt;&gt;&gt; \"MSWEP_YYYY.MM.DD.tif\"\n            &gt;&gt;&gt; file_name_data_fmt = \"%Y.%m.%d\"\n            ```\n\n        start (str):\n            Start date if you want to read the input raster for a specific period only and not all rasters. If not\n            given, all rasters in the given path will be read.\n        end (str):\n            End date if you want to read the input rasters for a specific period only. If not given, all rasters in\n            the given path will be read.\n        fmt (str):\n            Format of the given date in the start/end parameter.\n        extension (str):\n            The extension of the files you want to read from the given path. Default is \".tif\".\n\n    Returns:\n        Datacube:\n            Instance of the Datacube class.\n\n    Examples:\n        - Read all rasters in a folder:\n\n          ```python\n          &gt;&gt;&gt; from pyramids.datacube import Datacube\n          &gt;&gt;&gt; raster_folder = \"examples/GIS/data/raster-folder\"\n          &gt;&gt;&gt; prec = Datacube.read_multiple_files(raster_folder)\n\n          ```\n\n        - Read from a pre-collected list without ordering:\n\n          ```python\n          &gt;&gt;&gt; import glob\n          &gt;&gt;&gt; search_criteria = \"*.tif\"\n          &gt;&gt;&gt; file_list = glob.glob(os.path.join(raster_folder, search_criteria))\n          &gt;&gt;&gt; prec = Datacube.read_multiple_files(file_list, with_order=False)\n\n          ```\n    \"\"\"\n    if not isinstance(path, str) and not isinstance(path, list):\n        raise TypeError(f\"path input should be string/list type, given{type(path)}\")\n\n    if isinstance(path, str):\n        # check whither the path exists or not\n        if not os.path.exists(path):\n            raise FileNotFoundError(\"The path you have provided does not exist\")\n        # get a list of all files\n        files = os.listdir(path)\n        files = [i for i in files if i.endswith(extension)]\n        # files = glob.glob(os.path.join(path, \"*.tif\"))\n        # check whether there are files or not inside the folder\n        if len(files) &lt; 1:\n            raise FileNotFoundError(\"The path you have provided is empty\")\n    else:\n        files = path[:]\n\n    # to sort the files in the same order as the first number in the name\n    if with_order:\n        match_str_fn = lambda x: re.search(regex_string, x)\n        list_dates = list(map(match_str_fn, files))\n\n        if None in list_dates:\n            raise ValueError(\n                \"The date format/separator given does not match the file names\"\n            )\n        if date:\n            if file_name_data_fmt is None:\n                raise ValueError(\n                    f\"To read the raster with a certain order (with_order = {with_order}, then you have to enter \"\n                    f\"the value of the parameter file_name_data_fmt(given: {file_name_data_fmt})\"\n                )\n            fn = lambda x: dt.datetime.strptime(x.group(), file_name_data_fmt)\n        else:\n            fn = lambda x: int(x.group())\n        list_dates = list(map(fn, list_dates))\n\n        df = pd.DataFrame()\n        df[\"files\"] = files\n        df[\"date\"] = list_dates\n        df.sort_values(\"date\", inplace=True, ignore_index=True)\n        files = df.loc[:, \"files\"].values\n\n    if start is not None or end is not None:\n        if date:\n            start = dt.datetime.strptime(start, fmt)\n            end = dt.datetime.strptime(end, fmt)\n\n            files = (\n                df.loc[start &lt;= df[\"date\"], :]\n                .loc[df[\"date\"] &lt;= end, \"files\"]\n                .values\n            )\n        else:\n            files = (\n                df.loc[start &lt;= df[\"date\"], :]\n                .loc[df[\"date\"] &lt;= end, \"files\"]\n                .values\n            )\n\n    if not isinstance(path, list):\n        # add the path to all the files\n        files = [f\"{path}/{i}\" for i in files]\n    # create a 3d array with the 2d dimension of the first raster and the len\n    # of the number of rasters in the folder\n    sample = Dataset.read_file(files[0])\n\n    return cls(sample, len(files), files)\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.open_datacube","title":"<code>open_datacube(band=0)</code>","text":"<p>Open the datacube.</p> <p>Read values from the given band as arrays for all files.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Index of the band you want to read. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Loads values into the internal 3D array [time, rows, cols] in-place.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def open_datacube(self, band: int = 0) -&gt; None:\n    \"\"\"Open the datacube.\n\n    Read values from the given band as arrays for all files.\n\n    Args:\n        band (int): Index of the band you want to read. Default is 0.\n\n    Returns:\n        None: Loads values into the internal 3D array [time, rows, cols] in-place.\n    \"\"\"\n    # check the given band number\n    if not hasattr(self, \"base\"):\n        raise ValueError(\n            \"please use the read_multiple_files method to get the files (tiff/ascii) in the\"\n            \"dataset directory\"\n        )\n    if band &gt; self.base.band_count - 1:\n        raise ValueError(\n            f\"the raster has only {self.base.band_count} check the given band number\"\n        )\n    # fill the array with no_data_value data\n    self._values = np.ones(\n        (\n            self.time_length,\n            self.base.rows,\n            self.base.columns,\n        )\n    )\n    self._values[:, :, :] = np.nan\n\n    for i, file_i in enumerate(self.files):\n        # read the tif file\n        raster_i = gdal.Open(f\"{file_i}\")\n        self._values[i, :, :] = raster_i.GetRasterBand(band + 1).ReadAsArray()\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Getitem.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"Getitem.\"\"\"\n    if not hasattr(self, \"values\"):\n        raise AttributeError(\"Please use the read_dataset method to read the data\")\n    return self._values[key, :, :]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Setitem.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __setitem__(self, key, value: np.ndarray):\n    \"\"\"Setitem.\"\"\"\n    if not hasattr(self, \"values\"):\n        raise AttributeError(\"Please use the read_dataset method to read the data\")\n    self._values[key, :, :] = value\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__len__","title":"<code>__len__()</code>","text":"<p>Length of the Datacube.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __len__(self):\n    \"\"\"Length of the Datacube.\"\"\"\n    return self._values.shape[0]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the Datacube.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over the Datacube.\"\"\"\n    return iter(self._values[:])\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.head","title":"<code>head(n=5)</code>","text":"<p>First 5 Datasets.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def head(self, n: int = 5):\n    \"\"\"First 5 Datasets.\"\"\"\n    return self._values[:n, :, :]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.tail","title":"<code>tail(n=-5)</code>","text":"<p>Last 5 Datasets.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def tail(self, n: int = -5):\n    \"\"\"Last 5 Datasets.\"\"\"\n    return self._values[n:, :, :]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.first","title":"<code>first()</code>","text":"<p>First Dataset.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def first(self):\n    \"\"\"First Dataset.\"\"\"\n    return self._values[0, :, :]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.last","title":"<code>last()</code>","text":"<p>Last Dataset.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def last(self):\n    \"\"\"Last Dataset.\"\"\"\n    return self._values[-1, :, :]\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.iloc","title":"<code>iloc(i)</code>","text":"<p>iloc.</p> <pre><code>- Access dataset array using index.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the dataset to access.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset object.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def iloc(self, i) -&gt; Dataset:\n    \"\"\"iloc.\n\n        - Access dataset array using index.\n\n    Args:\n        i (int):\n            Index of the dataset to access.\n\n    Returns:\n        Dataset: Dataset object.\n    \"\"\"\n    if not hasattr(self, \"values\"):\n        raise DatasetNoFoundError(\"please read the dataset first\")\n    arr = self._values[i, :, :]\n    dst = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self.base.raster, 0)\n    dst.GetRasterBand(1).WriteArray(arr)\n    return Dataset(dst)\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.plot","title":"<code>plot(band=0, exclude_value=None, **kwargs)</code>","text":"<p>Read Array.</p> <pre><code>- read the values stored in a given band.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>The band you want to get its data. Default is 0.</p> <code>0</code> <code>exclude_value</code> <code>Any</code> <p>Value to exclude from the plot. Default is None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> Parameter Type Description points array 3-column array: col 1 = value to display, col 2 = row index, col 3 = column index. Columns 2 and 3 indicate the location of the point. point_color str Color of the points. point_size Any Size of the points. pid_color str Color of the annotation of the point. Default is blue. pid_size Any Size of the point annotation. figsize tuple, optional Figure size. Default is <code>(8, 8)</code>. title str, optional Title of the plot. Default is <code>'Total Discharge'</code>. title_size int, optional Title size. Default is <code>15</code>. orientation str, optional Orientation of the color bar (<code>horizontal</code> or <code>vertical</code>). Default is <code>'vertical'</code>. rotation number, optional Rotation of the color bar label. Default is <code>-90</code>. cbar_length float, optional Ratio to control the height of the color bar. Default is <code>0.75</code>. ticks_spacing int, optional Spacing in the color bar ticks. Default is <code>2</code>. cbar_label_size int, optional Size of the color bar label. Default is <code>12</code>. cbar_label str, optional Label of the color bar. Default is <code>'Discharge m\u00b3/s'</code>. color_scale int, optional Color scaling mode (default = <code>1</code>): 1 = normal scale, 2 = power scale, 3 = SymLogNorm scale, 4 = PowerNorm scale, 5 = BoundaryNorm scale. gamma float, optional Value needed for <code>color_scale=2</code>. Default is <code>1/2</code>. line_threshold float, optional Value needed for <code>color_scale=3</code>. Default is <code>0.0001</code>. line_scale float, optional Value needed for <code>color_scale=3</code>. Default is <code>0.001</code>. bounds list Discrete bounds for <code>color_scale=4</code>. Default is <code>None</code>. midpoint float, optional Value needed for <code>color_scale=5</code>. Default is <code>0</code>. cmap str, optional Color map style. Default is <code>'coolwarm_r'</code>. display_cell_value bool Whether to display the values of the cells as text. num_size int, optional Size of the numbers plotted on top of each cell. Default is <code>8</code>. background_color_threshold float | int, optional Threshold for deciding number color: if value &gt; threshold \u2192 black; else white. If <code>None</code>, uses <code>max_value/2</code>. Default is <code>None</code>. <code>{}</code> <p>Returns:</p> Name Type Description <code>ArrayGlyph</code> <code>ArrayGlyph</code> <p>A plotting/animation handle (from cleopatra.ArrayGlyph).</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def plot(self, band: int = 0, exclude_value: Any = None, **kwargs: Any) -&gt; \"ArrayGlyph\":\n    r\"\"\"Read Array.\n\n        - read the values stored in a given band.\n\n    Args:\n        band (int):\n            The band you want to get its data. Default is 0.\n        exclude_value (Any):\n            Value to exclude from the plot. Default is None.\n        **kwargs:\n            | Parameter                  | Type                  | Description |\n            |----------------------------|-----------------------|-------------|\n            | points                     | array                 | 3-column array: col 1 = value to display, col 2 = row index, col 3 = column index. Columns 2 and 3 indicate the location of the point. |\n            | point_color                | str                   | Color of the points. |\n            | point_size                 | Any                   | Size of the points. |\n            | pid_color                  | str                   | Color of the annotation of the point. Default is blue. |\n            | pid_size                   | Any                   | Size of the point annotation. |\n            | figsize                    | tuple, optional       | Figure size. Default is `(8, 8)`. |\n            | title                      | str, optional         | Title of the plot. Default is `'Total Discharge'`. |\n            | title_size                 | int, optional         | Title size. Default is `15`. |\n            | orientation                | str, optional         | Orientation of the color bar (`horizontal` or `vertical`). Default is `'vertical'`. |\n            | rotation                   | number, optional      | Rotation of the color bar label. Default is `-90`. |\n            | cbar_length                | float, optional       | Ratio to control the height of the color bar. Default is `0.75`. |\n            | ticks_spacing              | int, optional         | Spacing in the color bar ticks. Default is `2`. |\n            | cbar_label_size            | int, optional         | Size of the color bar label. Default is `12`. |\n            | cbar_label                 | str, optional         | Label of the color bar. Default is `'Discharge m\u00b3/s'`. |\n            | color_scale                | int, optional         | Color scaling mode (default = `1`): 1 = normal scale, 2 = power scale, 3 = SymLogNorm scale, 4 = PowerNorm scale, 5 = BoundaryNorm scale. |\n            | gamma                      | float, optional       | Value needed for `color_scale=2`. Default is `1/2`. |\n            | line_threshold             | float, optional       | Value needed for `color_scale=3`. Default is `0.0001`. |\n            | line_scale                 | float, optional       | Value needed for `color_scale=3`. Default is `0.001`. |\n            | bounds                     | list                  | Discrete bounds for `color_scale=4`. Default is `None`. |\n            | midpoint                   | float, optional       | Value needed for `color_scale=5`. Default is `0`. |\n            | cmap                       | str, optional         | Color map style. Default is `'coolwarm_r'`. |\n            | display_cell_value         | bool                  | Whether to display the values of the cells as text. |\n            | num_size                   | int, optional         | Size of the numbers plotted on top of each cell. Default is `8`. |\n            | background_color_threshold | float \\| int, optional| Threshold for deciding number color: if value &gt; threshold \u2192 black; else white. If `None`, uses `max_value/2`. Default is `None`. |\n\n\n    Returns:\n        ArrayGlyph: A plotting/animation handle (from cleopatra.ArrayGlyph).\n    \"\"\"\n    import_cleopatra(\n        \"The current funcrion uses cleopatra package to for plotting, please install it manually, for more info \"\n        \"check https://github.com/Serapieum-of-alex/cleopatra\"\n    )\n    from cleopatra.array_glyph import ArrayGlyph\n\n    data = self.values\n\n    exclude_value = (\n        [self.base.no_data_value[band], exclude_value]\n        if exclude_value is not None\n        else [self.base.no_data_value[band]]\n    )\n\n    cleo = ArrayGlyph(data, exclude_value=exclude_value)\n    time = list(range(self.time_length))\n    cleo.animate(time, **kwargs)\n    return cleo\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.to_file","title":"<code>to_file(path, driver='geotiff', band=0)</code>","text":"<p>Save to geotiff format.</p> <pre><code>saveRaster saves a raster to a path\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, List[str]]</code> <p>a path includng the name of the raster and extention.</p> required <code>driver</code> <code>str</code> <p>driver = \"geotiff\".</p> <code>'geotiff'</code> <code>band</code> <code>int</code> <p>band index, needed only in case of ascii drivers. Default is 1.</p> <code>0</code> <p>Examples:</p> <ul> <li>Save to a file:</li> </ul> <pre><code>&gt;&gt;&gt; raster_obj = Dataset.read_file(\"path/to/file/***.tif\")\n&gt;&gt;&gt; output_path = \"examples/GIS/data/save_raster_test.tif\"\n&gt;&gt;&gt; raster_obj.to_file(output_path)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>def to_file(\n    self, path: Union[str, List[str]], driver: str = \"geotiff\", band: int = 0\n):\n    \"\"\"Save to geotiff format.\n\n        saveRaster saves a raster to a path\n\n    Args:\n        path (Union[str, List[str]]):\n            a path includng the name of the raster and extention.\n        driver (str):\n            driver = \"geotiff\".\n        band (int):\n            band index, needed only in case of ascii drivers. Default is 1.\n\n    Examples:\n        - Save to a file:\n\n          ```python\n          &gt;&gt;&gt; raster_obj = Dataset.read_file(\"path/to/file/***.tif\")\n          &gt;&gt;&gt; output_path = \"examples/GIS/data/save_raster_test.tif\"\n          &gt;&gt;&gt; raster_obj.to_file(output_path)\n\n          ```\n    \"\"\"\n    ext = CATALOG.get_extension(driver)\n\n    if isinstance(path, str):\n        if not Path(path).exists():\n            Path(path).mkdir(parents=True, exist_ok=True)\n        path = [f\"{path}/{i}.{ext}\" for i in range(self.time_length)]\n    else:\n        if not len(path) == self.time_length:\n            raise ValueError(\n                f\"Length of the given paths: {len(path)} does not equal number of rasters in the data cube: {self.time_length}\"\n            )\n        if not Path(path[0]).parent.exists():\n            Path(path[0]).parent.mkdir(parents=True, exist_ok=True)\n\n    for i in range(self.time_length):\n        src = self.iloc(i)\n        src.to_file(path[i], band=band)\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.to_crs","title":"<code>to_crs(to_epsg=3857, method='nearest neighbor', maintain_alignment=False)</code>","text":"<p>to_epsg.</p> <pre><code>- to_epsg reprojects a raster to any projection (default the WGS84 web mercator projection,\nwithout resampling) The function returns a GDAL in-memory file object, where you can ReadAsArray etc.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>to_epsg</code> <code>int</code> <p>Reference number to the new projection (https://epsg.io/) (default 3857 the reference no of WGS84 web mercator).</p> <code>3857</code> <code>method</code> <code>str</code> <p>Resampling technique. Default is \"Nearest\". See https://gisgeography.com/raster-resampling/. \"Nearest\" for nearest neighbor, \"cubic\" for cubic convolution, \"bilinear\" for bilinear.</p> <code>'nearest neighbor'</code> <code>maintain_alignment</code> <code>bool</code> <p>True to maintain the number of rows and columns of the raster the same after reprojection. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Updates the datacube values and base in place after reprojection.</p> <p>Examples:</p> <ul> <li>Reproject dataset to EPSG:3857:</li> </ul> <pre><code>&gt;&gt;&gt; from pyramids.dataset import Dataset\n&gt;&gt;&gt; src = Dataset.read_file(\"path/raster_name.tif\")\n&gt;&gt;&gt; projected_raster = src.to_crs(to_epsg=3857)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>def to_crs(\n    self,\n    to_epsg: int = 3857,\n    method: str = \"nearest neighbor\",\n    maintain_alignment: int = False,\n) -&gt; None:\n    \"\"\"to_epsg.\n\n        - to_epsg reprojects a raster to any projection (default the WGS84 web mercator projection,\n        without resampling) The function returns a GDAL in-memory file object, where you can ReadAsArray etc.\n\n    Args:\n        to_epsg (int):\n            Reference number to the new projection (https://epsg.io/)\n            (default 3857 the reference no of WGS84 web mercator).\n        method (str):\n            Resampling technique. Default is \"Nearest\". See https://gisgeography.com/raster-resampling/.\n            \"Nearest\" for nearest neighbor, \"cubic\" for cubic convolution, \"bilinear\" for bilinear.\n        maintain_alignment (bool):\n            True to maintain the number of rows and columns of the raster the same after reprojection.\n            Default is False.\n\n    Returns:\n        None: Updates the datacube values and base in place after reprojection.\n\n    Examples:\n        - Reproject dataset to EPSG:3857:\n\n          ```python\n          &gt;&gt;&gt; from pyramids.dataset import Dataset\n          &gt;&gt;&gt; src = Dataset.read_file(\"path/raster_name.tif\")\n          &gt;&gt;&gt; projected_raster = src.to_crs(to_epsg=3857)\n\n          ```\n    \"\"\"\n    for i in range(self.time_length):\n        src = self.iloc(i)\n        dst = src.to_crs(\n            to_epsg, method=method, maintain_alignment=maintain_alignment\n        )\n        arr = dst.read_array()\n        if i == 0:\n            # create the array\n            array = (\n                np.ones(\n                    (\n                        self.time_length,\n                        arr.shape[0],\n                        arr.shape[1],\n                    )\n                )\n                * np.nan\n            )\n        array[i, :, :] = arr\n\n    self._values = array\n    # use the last src as\n    self._base = dst\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.crop","title":"<code>crop(mask, inplace=False, touch=True)</code>","text":"<p>crop.</p> <pre><code>crop matches the location of nodata value from src raster to dst raster. Mask is where the NoDatavalue will\nbe taken and the location of this value. src_dir is path to the folder where rasters exist where we need to\nput the NoDataValue of the mask in RasterB at the same locations.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Dataset</code> <p>Dataset object of the mask raster to crop the rasters (to get the NoData value and its location in the array). Mask should include the name of the raster and the extension like \"data/dem.tif\", or you can read the mask raster using gdal and use it as the first parameter to the function.</p> required <code>inplace</code> <code>bool</code> <p>True to make the changes in place.</p> <code>False</code> <code>touch</code> <code>bool</code> <p>Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[None, Dataset]</code> <p>Union[None, \"Datacube\"]: New rasters have the values from rasters in B_input_path with the NoDataValue in</p> <code>Union[None, Dataset]</code> <p>the same locations as raster A.</p> <p>Examples:</p> <ul> <li>Crop aligned rasters using a DEM mask:</li> </ul> <pre><code>&gt;&gt;&gt; dem_path = \"examples/GIS/data/acc4000.tif\"\n&gt;&gt;&gt; src_path = \"examples/GIS/data/aligned_rasters/\"\n&gt;&gt;&gt; out_path = \"examples/GIS/data/crop_aligned_folder/\"\n&gt;&gt;&gt; Datacube.crop(dem_path, src_path, out_path)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>def crop(\n    self, mask: Union[Dataset, str], inplace: bool = False, touch: bool = True\n) -&gt; Union[None, Dataset]:\n    \"\"\"crop.\n\n        crop matches the location of nodata value from src raster to dst raster. Mask is where the NoDatavalue will\n        be taken and the location of this value. src_dir is path to the folder where rasters exist where we need to\n        put the NoDataValue of the mask in RasterB at the same locations.\n\n    Args:\n        mask (Dataset):\n            Dataset object of the mask raster to crop the rasters (to get the NoData value and its location in the\n            array). Mask should include the name of the raster and the extension like \"data/dem.tif\", or you can\n            read the mask raster using gdal and use it as the first parameter to the function.\n        inplace (bool):\n            True to make the changes in place.\n        touch (bool):\n            Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask.\n            Default is True.\n\n    Returns:\n        Union[None, \"Datacube\"]: New rasters have the values from rasters in B_input_path with the NoDataValue in\n        the same locations as raster A.\n\n    Examples:\n        - Crop aligned rasters using a DEM mask:\n\n          ```python\n          &gt;&gt;&gt; dem_path = \"examples/GIS/data/acc4000.tif\"\n          &gt;&gt;&gt; src_path = \"examples/GIS/data/aligned_rasters/\"\n          &gt;&gt;&gt; out_path = \"examples/GIS/data/crop_aligned_folder/\"\n          &gt;&gt;&gt; Datacube.crop(dem_path, src_path, out_path)\n\n          ```\n    \"\"\"\n    for i in range(self.time_length):\n        src = self.iloc(i)\n        dst = src.crop(mask, touch=touch)\n        arr = dst.read_array()\n        if i == 0:\n            # create the array\n            array = (\n                np.ones(\n                    (self.time_length, arr.shape[0], arr.shape[1]),\n                )\n                * np.nan\n            )\n\n        array[i, :, :] = arr\n\n    if inplace:\n        self._values = array\n        # use the last src as\n        self._base = dst\n    else:\n        dataset = Datacube(dst, time_length=self.time_length)\n        dataset._values = array\n        return dataset\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.align","title":"<code>align(alignment_src)</code>","text":"<p>matchDataAlignment.</p> <p>This function matches the coordinate system and the number of rows and columns between two rasters. Raster A is the source of the coordinate system, number of rows, number of columns, and cell size. The result will be a raster with the same structure as Raster A but with values from Raster B using nearest neighbor interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_src</code> <code>Dataset</code> <p>Dataset to use as the spatial template (CRS, rows, columns).</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Updates the datacube values in place to match the alignment of alignment_src.</p> <p>Examples:</p> <ul> <li>Align all rasters in the datacube to a DEM raster:</li> </ul> <pre><code>&gt;&gt;&gt; dem_path = \"01GIS/inputs/4000/acc4000.tif\"\n&gt;&gt;&gt; prec_in_path = \"02Precipitation/CHIRPS/Daily/\"\n&gt;&gt;&gt; prec_out_path = \"02Precipitation/4km/\"\n&gt;&gt;&gt; Dataset.align(dem_path, prec_in_path, prec_out_path)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>def align(self, alignment_src: Dataset) -&gt; None:\n    \"\"\"matchDataAlignment.\n\n    This function matches the coordinate system and the number of rows and columns between two rasters. Raster A\n    is the source of the coordinate system, number of rows, number of columns, and cell size. The result will be\n    a raster with the same structure as Raster A but with values from Raster B using nearest neighbor interpolation.\n\n    Args:\n        alignment_src (Dataset):\n            Dataset to use as the spatial template (CRS, rows, columns).\n\n    Returns:\n        None:\n            Updates the datacube values in place to match the alignment of alignment_src.\n\n    Examples:\n        - Align all rasters in the datacube to a DEM raster:\n\n          ```python\n          &gt;&gt;&gt; dem_path = \"01GIS/inputs/4000/acc4000.tif\"\n          &gt;&gt;&gt; prec_in_path = \"02Precipitation/CHIRPS/Daily/\"\n          &gt;&gt;&gt; prec_out_path = \"02Precipitation/4km/\"\n          &gt;&gt;&gt; Dataset.align(dem_path, prec_in_path, prec_out_path)\n\n          ```\n    \"\"\"\n    if not isinstance(alignment_src, Dataset):\n        raise TypeError(\"alignment_src input should be a Dataset object\")\n\n    for i in range(self.time_length):\n        src = self.iloc(i)\n        dst = src.align(alignment_src)\n        arr = dst.read_array()\n        if i == 0:\n            # create the array\n            array = (\n                np.ones(\n                    (self.time_length, arr.shape[0], arr.shape[1]),\n                )\n                * np.nan\n            )\n\n        array[i, :, :] = arr\n\n    self._values = array\n    # use the last src as\n    self._base = dst\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.merge","title":"<code>merge(src, dst, no_data_value='0', init='nan', n='nan')</code>  <code>staticmethod</code>","text":"<p>merge.</p> <pre><code>Merges a group of rasters into one raster.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>List[str]</code> <p>List of paths to all input rasters.</p> required <code>dst</code> <code>str</code> <p>Path to the output raster.</p> required <code>no_data_value</code> <code>float | int | str</code> <p>Assign a specified nodata value to output bands.</p> <code>'0'</code> <code>init</code> <code>float | int | str</code> <p>Pre-initialize the output image bands with these values. However, it is not marked as the nodata value in the output file. If only one value is given, the same value is used in all the bands.</p> <code>'nan'</code> <code>n</code> <code>float | int | str</code> <p>Ignore pixels from files being merged in with this pixel value.</p> <code>'nan'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyramids/datacube.py</code> <pre><code>@staticmethod\ndef merge(\n    src: List[str],\n    dst: str,\n    no_data_value: Union[float, int, str] = \"0\",\n    init: Union[float, int, str] = \"nan\",\n    n: Union[float, int, str] = \"nan\",\n) -&gt; None:\n    \"\"\"merge.\n\n        Merges a group of rasters into one raster.\n\n    Args:\n        src (List[str]):\n            List of paths to all input rasters.\n        dst (str):\n            Path to the output raster.\n        no_data_value (float | int | str):\n            Assign a specified nodata value to output bands.\n        init (float | int | str):\n            Pre-initialize the output image bands with these values. However, it is not\n            marked as the nodata value in the output file. If only one value is given, the same value is used\n            in all the bands.\n        n (float | int | str):\n            Ignore pixels from files being merged in with this pixel value.\n\n    Returns:\n        None\n    \"\"\"\n    # run the command\n    # cmd = \"gdal_merge.py -o merged_image_1.tif\"\n    # subprocess.call(cmd.split() + file_list)\n    # vrt = gdal.BuildVRT(\"merged.vrt\", file_list)\n    # src = gdal.Translate(\"merged_image.tif\", vrt)\n\n    parameters = (\n        [\"\", \"-o\", dst]\n        + src\n        + [\n            \"-co\",\n            \"COMPRESS=LZW\",\n            \"-init\",\n            str(init),\n            \"-a_nodata\",\n            str(no_data_value),\n            \"-n\",\n            str(n),\n        ]\n    )  # '-separate'\n    gdal_merge.main(parameters)\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.apply","title":"<code>apply(ufunc)</code>","text":"<p>apply.</p> <p>Apply a function on each raster in the datacube.</p> <p>Parameters:</p> Name Type Description Default <code>ufunc</code> <code>Callable</code> <p>Callable universal function (builtin or user defined). See https://numpy.org/doc/stable/reference/ufuncs.html To create a ufunc from a normal function: https://numpy.org/doc/stable/reference/generated/numpy.frompyfunc.html</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <ul> <li>Apply a simple modulo operation to each value:</li> </ul> <pre><code>&gt;&gt;&gt; def func(val):\n&gt;&gt;&gt;    return val%2\n&gt;&gt;&gt; ufunc = np.frompyfunc(func, 1, 1)\n&gt;&gt;&gt; dataset.apply(ufunc)\n</code></pre> Source code in <code>pyramids/datacube.py</code> <pre><code>def apply(self, ufunc: Callable) -&gt; None:\n    \"\"\"apply.\n\n    Apply a function on each raster in the datacube.\n\n    Args:\n        ufunc (Callable):\n            Callable universal function (builtin or user defined). See\n            https://numpy.org/doc/stable/reference/ufuncs.html\n            To create a ufunc from a normal function: https://numpy.org/doc/stable/reference/generated/numpy.frompyfunc.html\n\n    Returns:\n        None\n\n    Examples:\n        - Apply a simple modulo operation to each value:\n\n          ```python\n          &gt;&gt;&gt; def func(val):\n          &gt;&gt;&gt;    return val%2\n          &gt;&gt;&gt; ufunc = np.frompyfunc(func, 1, 1)\n          &gt;&gt;&gt; dataset.apply(ufunc)\n\n          ```\n    \"\"\"\n    if not callable(ufunc):\n        raise TypeError(\"The Second argument should be a function\")\n    arr = self.values\n    no_data_value = self.base.no_data_value[0]\n    # execute the function on each raster\n    arr[~np.isclose(arr, no_data_value, rtol=0.001)] = ufunc(\n        arr[~np.isclose(arr, no_data_value, rtol=0.001)]\n    )\n</code></pre>"},{"location":"reference/datacube/#pyramids.datacube.Datacube.overlay","title":"<code>overlay(classes_map, exclude_value=None)</code>","text":"<p>Overlay.</p> <p>Parameters:</p> Name Type Description Default <code>classes_map</code> <code>Dataset</code> <p>Dataset object for the raster that has classes to overlay with.</p> required <code>exclude_value</code> <code>float | int</code> <p>Values to exclude from extracted values. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[List[float], List[float]]</code> <p>Dict[List[float], List[float]]: Dictionary with a list of values in the basemap as keys and for each key a list of all the intersected values in the maps from the path.</p> Source code in <code>pyramids/datacube.py</code> <pre><code>def overlay(\n    self,\n    classes_map,\n    exclude_value: Union[float, int] = None,\n) -&gt; Dict[List[float], List[float]]:\n    \"\"\"Overlay.\n\n    Args:\n        classes_map (Dataset):\n            Dataset object for the raster that has classes to overlay with.\n        exclude_value (float | int, optional):\n            Values to exclude from extracted values. Defaults to None.\n\n    Returns:\n        Dict[List[float], List[float]]:\n            Dictionary with a list of values in the basemap as keys and for each key a list of all the\n            intersected values in the maps from the path.\n    \"\"\"\n    values = {}\n    for i in range(self.time_length):\n        src = self.iloc(i)\n        dict_i = src.overlay(classes_map, exclude_value)\n\n        # these are the distinct values from the BaseMap which are keys in the\n        # values dict with each one having a list of values\n        classes = list(dict_i.keys())\n\n        for class_i in classes:\n            if class_i not in values.keys():\n                values[class_i] = list()\n\n            values[class_i] = values[class_i] + dict_i[class_i]\n\n    return values\n</code></pre>"},{"location":"reference/dataset/","title":"Dataset Class","text":"<ul> <li>Detailed class diagram for the <code>Dataset</code> class and related components:</li> </ul> <pre><code>classDiagram\n    %% configuration class\n    class Config {\n    }\n\n    %% abstract base class for rasters\n    class AbstractDataset {\n        +__init__(src, access)\n        +__str__()\n        +__repr__()\n        +access()\n        +raster()\n        +raster(value)\n        +values()\n        +rows()\n        +columns()\n        +shape()\n        +geotransform()\n        +top_left_corner()\n        +epsg()\n        +epsg(value)\n        +crs()\n        +crs(value)\n        +cell_size()\n        +no_data_value()\n        +no_data_value(value)\n        +meta_data()\n        +meta_data(value)\n        +block_size()\n        +block_size(value)\n        +file_name()\n        +driver_type()\n        +read_file(path, read_only)\n        +read_array(band, window)\n        +_read_block(band, window)\n        +plot(band, exclude_value, rgb, surface_reflectance, cutoff, overview, overview_index, **kwargs)\n    }\n\n    %% concrete raster class\n    class Dataset {\n        +__init__(src, access)\n        +__str__()\n        +__repr__()\n        +access()\n        +raster()\n        +raster(value)\n        +values()\n        +rows()\n        +columns()\n        +shape()\n        +geotransform()\n        +epsg()\n        +epsg(value)\n        +crs()\n        +crs(value)\n        +cell_size()\n        +band_count()\n        +band_names()\n        +band_names(name_list)\n        +band_units()\n        +band_units(value)\n        +no_data_value()\n        +no_data_value(value)\n        +meta_data()\n        +meta_data(value)\n        +block_size()\n        +block_size(value)\n        +file_name()\n        +driver_type()\n        +scale()\n        +scale(value)\n        +offset()\n        +offset(value)\n        +read_file(path, read_only)\n        +create_from_array(arr, top_left_corner, cell_size, epsg)\n        +read_array(band, window)\n        +_read_block(band, window)\n        +plot(band, exclude_value, rgb, surface_reflectance, cutoff, overview, overview_index, **kwargs)\n        +to_file(path, driver, band)\n        +to_crs(to_epsg, method, maintain_alignment)\n        +resample(cell_size, method)\n        +align(alignment_src)\n        +crop(mask, touch)\n        +merge(src, dst, no_data_value, init, n)\n        +apply(ufunc)\n        +overlay(classes_map, exclude_value)\n    }\n\n\n\n    %% Driver catalog\n    class _utils_Catalog {\n    }\n\n    %% NetCDF\n    class NetCDF {\n    }\n\n    %% error classes\n    class _errors_ReadOnlyError\n    class _errors_DatasetNoFoundError\n    class _errors_NoDataValueError\n    class _errors_AlignmentError\n    class _errors_DriverNotExistError\n    class _errors_FileFormatNotSupported\n    class _errors_OptionalPackageDoesNotExist\n    class _errors_FailedToSaveError\n    class _errors_OutOfBoundsError\n\n    %% inheritance relations\n    AbstractDataset &lt;|-- Dataset\n    Dataset &lt;|-- NetCDF\n\n    %% composition/usage relations\n    AbstractDataset ..&gt; _utils_Catalog : \"uses Catalog constant\"\n    AbstractDataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    Dataset ..&gt; featurecollection_FeatureCollection : \"vector ops\"\n    Dataset ..&gt; _errors_ReadOnlyError : \"raises\"\n    Dataset ..&gt; _errors_AlignmentError : \"raises\"\n    Dataset ..&gt; _errors_NoDataValueError : \"raises\"\n    Dataset ..&gt; _errors_FailedToSaveError : \"raises\"\n    Dataset ..&gt; _errors_OutOfBoundsError : \"raises\"\n    NetCDF ..&gt; _errors_OptionalPackageDoesNotExist : \"raises\"\n    Config ..&gt; Dataset : \"initialises raster settings\"\n</code></pre> <pre><code>classDiagram\n\n    %% Central dataset class with its main attributes\n    class Dataset {\n        +raster\n        +cell_size\n        +values\n        +shape\n        +rows\n        +columns\n        +pivot_point\n        +geotransform\n        +bounds\n        +bbox\n        +epsg\n        +crs\n        +lon\n        +lat\n        +x\n        +y\n        +band_count\n        +band_names\n        +variables\n        +no_data_value\n        +meta_data\n        +dtype\n        +gdal_dtype\n        +numpy_dtype\n        +file_name\n        +time_stamp\n        +driver_type\n    }\n\n    %% Group: visualisation functionality\n    class Visualization {\n        +plot()\n        +overview_count()\n        +read_overview_array()\n        +create_overviews()\n        +recreate_overviews()\n        +get_overview()\n    }\n    Dataset --&gt; Visualization : \u00abvisualisation\u00bb\n\n    %% Group: data access methods\n    class AccessData {\n        +read_array()\n        +get_variables()\n        +count_domain_cells()\n        +get_band_names()\n        +extract()\n        +stats()\n    }\n    Dataset --&gt; AccessData : \u00abdata access\u00bb\n\n    %% Group: mathematical operations on raster values\n    class MathOperations {\n        +apply()\n        +fill()\n        +normalize()\n        +cluster()\n        +cluster2()\n        +get_tile()\n        +groupNeighbours()\n    }\n    Dataset --&gt; MathOperations : \u00abmath ops\u00bb\n\n    %% Group: spatial operations and reprojection\n    class SpatialOperations {\n        +to_crs()\n        +resample()\n        +align()\n        +crop()\n        +locate_points()\n        +overlay()\n        +extract()\n        +footprint()\n    }\n    Dataset --&gt; SpatialOperations : \u00abspatial ops\u00bb\n\n    %% Group: conversion to other data types\n    class Conversion {\n        +to_feature_collection()\n    }\n    Dataset --&gt; Conversion : \u00abconversion\u00bb\n\n    %% Group: coordinate system handling\n    class OSR {\n        +create_sr_from_epsg()\n    }\n    Dataset --&gt; OSR : \u00abosr\u00bb\n\n    %% Group: bounding\u2010box and bounds calculations\n    class BBoxBounds {\n        +calculate_bbox()\n        +calculate_bounds()\n    }\n    Dataset --&gt; BBoxBounds : \u00abbbox/bounds\u00bb\n\n    %% Group: CRS/EPSG getters\n    class CrsEpsg {\n        +get_crs()\n        +get_epsg()\n    }\n    Dataset --&gt; CrsEpsg : \u00abcrs/epsg\u00bb\n\n    %% Group: latitude/longitude getters\n    class LatLon {\n        +get_lat_lon()\n    }\n    Dataset --&gt; LatLon : \u00ablat/lon\u00bb\n\n    %% Group: band names management\n    class BandNames {\n        +get_band_names_internal()\n        +set_band_names()\n    }\n    Dataset --&gt; BandNames : \u00abband names\u00bb\n\n    %% Group: timestamp handling\n    class TimeStamp {\n        +get_time_variable()\n        +read_variable()\n    }\n    Dataset --&gt; TimeStamp : \u00abtime\u00bb\n\n    %% Group: handling of no\u2010data values\n    class NoDataValue {\n        +set_no_data_value()\n        +set_no_data_value_backend()\n        +change_no_data_value_attr()\n    }\n    Dataset --&gt; NoDataValue : \u00abno data value\u00bb\n\n    %% Group: helpers for creating GDAL datasets\n    class GdalDataset {\n        +create_empty_driver()\n        +create_driver_from_scratch()\n        +create_mem_gtiff_dataset()\n    }\n    Dataset --&gt; GdalDataset : \u00abgdal creation\u00bb\n\n    %% Group: factory methods for creating Dataset objects\n    class CreateObject {\n        +from_gdal_dataset()\n        +read_file()\n        +create_from_array()\n        +dataset_like()\n    }\n    Dataset --&gt; CreateObject : \u00abobject factory\u00bb\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset","title":"<code>pyramids.dataset.Dataset</code>","text":"<p>               Bases: <code>AbstractDataset</code></p> <p>Dataset.</p> <p>The Dataset class contains methods to deal with raster and netcdf files, change projection and coordinate systems.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>class Dataset(AbstractDataset):\n    \"\"\"Dataset.\n\n    The Dataset class contains methods to deal with raster and netcdf files, change projection and coordinate\n    systems.\n    \"\"\"\n\n    default_no_data_value = DEFAULT_NO_DATA_VALUE\n\n    def __init__(self, src: gdal.Dataset, access: str = \"read_only\"):\n        \"\"\"__init__.\"\"\"\n        self.logger = logging.getLogger(__name__)\n        super().__init__(src, access=access)\n\n        self._no_data_value = [\n            src.GetRasterBand(i).GetNoDataValue() for i in range(1, self.band_count + 1)\n        ]\n        self._band_names = self._get_band_names()\n        self._band_units = [\n            src.GetRasterBand(i).GetUnitType() for i in range(1, self.band_count + 1)\n        ]\n\n    def __str__(self):\n        \"\"\"__str__.\"\"\"\n        message = f\"\"\"\n            Cell size: {self.cell_size}\n            Dimension: {self.rows} * {self.columns}\n            EPSG: {self.epsg}\n            Number of Bands: {self.band_count}\n            Band names: {self.band_names}\n            Mask: {self._no_data_value[0]}\n            Data type: {self.dtype[0]}\n            File: {self.file_name}\n        \"\"\"\n        return message\n\n    def __repr__(self):\n        \"\"\"__repr__.\"\"\"\n        message = \"\"\"\n            Cell size: {0}\n            Dimension: {1} * {2}\n            EPSG: {3}\n            Number of Bands: {4}\n            Band names: {5}\n            Mask: {6}\n            Data type: {7}\n            projection: {8}\n            Metadata: {9}\n            File: {10}\n        \"\"\".format(\n            self.cell_size,\n            self.rows,\n            self.columns,\n            self.epsg,\n            self.band_count,\n            self.band_names,\n            (\n                self._no_data_value\n                if self._no_data_value == []\n                else self._no_data_value[0]\n            ),\n            self.dtype if self.dtype == [] else self.dtype[0],\n            self.crs,\n            self.meta_data,\n            self.file_name,\n        )\n        return message\n\n    @property\n    def access(self) -&gt; str:\n        \"\"\"\n        Access mode.\n\n        Returns:\n            str:\n                The access mode of the dataset (read_only/write).\n        \"\"\"\n        return super().access\n\n    @property\n    def raster(self) -&gt; gdal.Dataset:\n        \"\"\"Base GDAL Dataset.\"\"\"\n        return super().raster\n\n    @raster.setter\n    def raster(self, value: gdal.Dataset):\n        \"\"\"Contains GDAL Dataset.\"\"\"\n        self._raster = value\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        \"\"\"Values of all the bands.\n\n        Returns:\n            np.ndarray:\n                the values of all the bands in the raster as a 3D numpy array (bands, rows, columns).\n        \"\"\"\n        return self.read_array()\n\n    @property\n    def rows(self) -&gt; int:\n        \"\"\"Number of rows in the raster array.\"\"\"\n        return self._rows\n\n    @property\n    def columns(self) -&gt; int:\n        \"\"\"Number of columns in the raster array.\"\"\"\n        return self._columns\n\n    @property\n    def shape(self) -&gt; Tuple[int, int, int]:\n        \"\"\"Shape (bands, rows, columns).\"\"\"\n        return self.band_count, self.rows, self.columns\n\n    @property\n    def geotransform(self) -&gt; Tuple[float, float, float]:\n        \"\"\"WKT projection.\n\n        (top left corner X/lon coordinate, cell_size, 0, top left corner y/lat coordinate, 0, -cell_size).\n\n        Examples:\n            - Create a dataset:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - To check the geotransform of the dataset, call the `geotransform` property:\n\n              ```python\n              &gt;&gt;&gt; print(dataset.geotransform)\n              (0.0, 0.05, 0.0, 0.0, 0.0, -0.05)\n\n              ```\n\n        See Also:\n            - Dataset.top_left_corner: Coordinate of the top left corner of the dataset.\n            - Dataset.epsg: EPSG number of the dataset coordinate reference system.\n        \"\"\"\n        return super().geotransform\n\n    @property\n    def epsg(self) -&gt; int:\n        \"\"\"EPSG number.\"\"\"\n        crs = self.raster.GetProjection()\n        return FeatureCollection.get_epsg_from_prj(crs)\n\n    @epsg.setter\n    def epsg(self, value: int):\n        \"\"\"EPSG number.\"\"\"\n        sr = Dataset._create_sr_from_epsg(value)\n        self.raster.SetProjection(sr.ExportToWkt())\n        self.__init__(self._raster)\n\n    @property\n    def crs(self) -&gt; str:\n        \"\"\"Coordinate reference system.\n\n        Returns:\n            str:\n                the coordinate reference system of the dataset.\n\n        Examples:\n            - Create a dataset:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Now, to check the coordinate reference system, call the `crs` property:\n\n              ```python\n              &gt;&gt;&gt; print(dataset.crs)\n              GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n              ```\n        \"\"\"\n        return self._get_crs()\n\n    @crs.setter\n    def crs(self, value: str):\n        \"\"\"Coordinate reference system.\n\n        Args:\n            value (str):\n                WellKnownText (WKT) string. i.e.\n                    ```python\n                    'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,\n                    298.257223563,AUTHORITY[\"EPSG\",\"7030\"], AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,\n                    AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\", 0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],\n                    AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'\n                    ```\n\n        See Also:\n            - Dataset.set_crs: Set the Coordinate Reference System (CRS).\n            - Dataset.to_crs: Reproject the dataset to any projection.\n            - Dataset.epsg: EPSG number of the dataset coordinate reference system.\n        \"\"\"\n        self.set_crs(value)\n\n    @property\n    def cell_size(self) -&gt; int:\n        \"\"\"Cell size.\"\"\"\n        return self._cell_size\n\n    @property\n    def band_count(self) -&gt; int:\n        \"\"\"Number of bands in the raster.\"\"\"\n        return self._band_count\n\n    @property\n    def band_names(self) -&gt; List[str]:\n        \"\"\"Band names.\"\"\"\n        return self._get_band_names()\n\n    @band_names.setter\n    def band_names(self, name_list: List):\n        \"\"\"Band names.\"\"\"\n        self._set_band_names(name_list)\n\n    @property\n    def band_units(self) -&gt; List[str]:\n        \"\"\"Band units.\"\"\"\n        return self._band_units\n\n    @band_units.setter\n    def band_units(self, value: List[str]):\n        \"\"\"Band units setter.\"\"\"\n        self._band_units = value\n        for i, val in enumerate(value):\n            self._iloc(i).SetUnitType(val)\n\n    @property\n    def no_data_value(self):\n        \"\"\"No data value that marks the cells out of the domain.\"\"\"\n        return self._no_data_value\n\n    @no_data_value.setter\n    def no_data_value(self, value: Union[List, Number]):\n        \"\"\"no_data_value.\n\n        No data value that marks the cells out of the domain\n\n        Notes:\n            - The setter does not change the values of the cells to the new no_data_value, it only changes the\n            `no_data_value` attribute.\n            - Use this method to change the `no_data_value` attribute to match the value that is stored in the cells.\n            - To change the values of the cells, to the new no_data_value, use the `change_no_data_value` method.\n\n        See Also:\n            - Dataset.change_no_data_value: Change the No Data Value.\n        \"\"\"\n        if isinstance(value, list):\n            for i, val in enumerate(value):\n                self._change_no_data_value_attr(i, val)\n        else:\n            self._change_no_data_value_attr(0, value)\n\n    @property\n    def meta_data(self):\n        \"\"\"Meta-data.\n\n        Hint:\n            - This property does not need the Dataset to be opened in a write mode to be set.\n            - The value of the offset will be stored in an xml file by the name of the raster file with the extension of\n                .aux.xml,\n\n        the content of the file will be like the following:\n\n        ```xml\n            &lt;PAMDataset&gt;\n              &lt;Metadata&gt;\n                &lt;MDI key=\"key\"&gt;value&lt;/MDI&gt;\n              &lt;/Metadata&gt;\n            &lt;/PAMDataset&gt;\n        ```\n        \"\"\"\n        return super().meta_data\n\n    @meta_data.setter\n    def meta_data(self, value: Dict[str, str]):\n        \"\"\"Meta-data.\"\"\"\n        for key, value in value.items():\n            self._raster.SetMetadataItem(key, value)\n\n    @property\n    def block_size(self) -&gt; List[Tuple[int, int]]:\n        \"\"\"Block Size.\n\n        The block size is the size of the block that the raster is divided into, the block size is used to read and\n        write the raster data in blocks.\n\n        Examples:\n            - Create a dataset and print block size:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset.block_size)\n              [[5, 1]]\n\n              ```\n\n        See Also:\n            - Dataset.get_block_arrangement: Get block arrangement to read the dataset in chunks.\n            - Dataset.get_tile: Get tiles.\n            - Dataset.read_array: Read the data stored in the dataset bands.\n        \"\"\"\n        return self._block_size\n\n    @block_size.setter\n    def block_size(self, value: List[Tuple[int, int]]):\n        \"\"\"Block Size.\n\n        Args:\n            value (List[Tuple[int, int]]):\n                block size for each band in the raster(512, 512).\n\n        \"\"\"\n        if len(value[0]) != 2:\n            raise ValueError(\"block size should be a tuple of 2 integers\")\n\n        self._block_size = value\n\n    @property\n    def file_name(self):\n        \"\"\"File name.\"\"\"\n        return super().file_name\n\n    @property\n    def driver_type(self):\n        \"\"\"Driver Type.\"\"\"\n        return super().driver_type\n\n    @property\n    def scale(self) -&gt; List[float]:\n        \"\"\"Scale.\n\n        The value of the scale is used to convert the pixel values to the real-world values.\n\n        Hint:\n            - This property does not need the Dataset to be opened in a write mode to be set.\n            - The value of the offset will be stored in an xml file by the name of the raster file with the extension of\n                .aux.xml.\n\n        the content of the file will be like the following:\n\n        ```xml\n\n            &lt;PAMDataset&gt;\n              &lt;PAMRasterBand band=\"1\"&gt;\n                &lt;Description&gt;Band_1&lt;/Description&gt;\n                &lt;UnitType&gt;m&lt;/UnitType&gt;\n                &lt;Offset&gt;100&lt;/Offset&gt;\n                &lt;Scale&gt;2&lt;/Scale&gt;\n              &lt;/PAMRasterBand&gt;\n            &lt;/PAMDataset&gt;\n        ```\n        \"\"\"\n        scale_list = []\n        for i in range(self.band_count):\n            band_scale = self._iloc(i).GetScale()\n            scale_list.append(band_scale if band_scale is not None else 1.0)\n        return scale_list\n\n    @scale.setter\n    def scale(self, value: List[float]):\n        \"\"\"Scale.\"\"\"\n        for i, val in enumerate(value):\n            self._iloc(i).SetScale(val)\n\n    @property\n    def offset(self):\n        \"\"\"Offset.\n\n        The value of the offset is used to convert the pixel values to the real-world values.\n\n        Hint:\n            - This property does not need the Dataset to be opened in a write mode to be set.\n            - The value of the offset will be stored in xml file by the name of the raster file with the extension of\n                .aux.xml.\n\n        the content of the file will be like the following:\n\n        ```xml\n\n            &lt;PAMDataset&gt;\n              &lt;PAMRasterBand band=\"1\"&gt;\n                &lt;Description&gt;Band_1&lt;/Description&gt;\n                &lt;UnitType&gt;m&lt;/UnitType&gt;\n                &lt;Offset&gt;100&lt;/Offset&gt;\n                &lt;Scale&gt;2&lt;/Scale&gt;\n              &lt;/PAMRasterBand&gt;\n            &lt;/PAMDataset&gt;\n\n        ```\n        \"\"\"\n        offset_list = []\n        for i in range(self.band_count):\n            band_offset = self._iloc(i).GetOffset()\n            offset_list.append(band_offset if band_offset is not None else 0)\n        return offset_list\n\n    @offset.setter\n    def offset(self, value: List[float]):\n        \"\"\"Offset.\"\"\"\n        for i, val in enumerate(value):\n            self._iloc(i).SetOffset(val)\n\n    @classmethod\n    def read_file(\n        cls,\n        path: str,\n        read_only=True,\n        file_i: int = 0,\n    ) -&gt; \"Dataset\":\n        \"\"\"read_file.\n\n        Args:\n            path (str):\n                Path of file to open.\n            read_only (bool):\n                File mode, set to False, to open in \"update\" mode.\n            file_i (int):\n                Index to the file inside the compressed file you want to read, if the compressed file has only one file. Default is 0.\n\n        Returns:\n            Dataset:\n                Opened dataset instance.\n\n        Examples:\n            Zip files:\n            - Internal Zip file path (one/multiple files inside the compressed file):\n                if the path contains a zip but does not end with zip (compressed-file-name.zip/1.asc), so the path contains\n                    the internal path inside the zip file, so just ad\n\n\n                ```python\n                &gt;&gt;&gt; rdir = \"tests/data/virtual-file-system\"\n                &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip/1.asc\")\n                &gt;&gt;&gt; print(dataset)\n                &lt;BLANKLINE&gt;\n                            Cell size: 4000.0\n                            Dimension: 13 * 14\n                            EPSG: 4326\n                            Number of Bands: 1\n                            Band names: ['Band_1']\n                            Mask: -3.4028230607370965e+38\n                            Data type: float32\n                            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n                &lt;BLANKLINE&gt;\n\n                ```\n\n            - Only the Zip file path (one/multiple files inside the compressed file):\n                If you provide the name of the zip file with multiple files inside it, it will return the path to the first\n                file.\n\n\n                ```python\n                &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\")\n                &gt;&gt;&gt; print(dataset)\n                &lt;BLANKLINE&gt;\n                            Cell size: 4000.0\n                            Dimension: 13 * 14\n                            EPSG: 4326\n                            Number of Bands: 1\n                            Band names: ['Band_1']\n                            Mask: -3.4028230607370965e+38\n                            Data type: float32\n                            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n                &lt;BLANKLINE&gt;\n\n                ```\n\n            - Zip file path and an index (one/multiple files inside the compressed file):\n                if you provide the path to the zip file and an index to the file inside the compressed file you want to\n                read.\n\n\n                ```python\n                &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\", file_i=1)\n                &gt;&gt;&gt; print(dataset)\n                &lt;BLANKLINE&gt;\n                            Cell size: 4000.0\n                            Dimension: 13 * 14\n                            EPSG: 4326\n                            Number of Bands: 1\n                            Band names: ['Band_1']\n                            Mask: -3.4028230607370965e+38\n                            Data type: float32\n                            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/2.asc\n                &lt;BLANKLINE&gt;\n\n                ```\n\n        See Also:\n            - Dataset.read_array: Read the values stored in a dataset band.\n        \"\"\"\n        src = _io.read_file(path, read_only=read_only, file_i=file_i)\n        return cls(src, access=\"read_only\" if read_only else \"write\")\n\n    def read_array(\n        self, band: int = None, window: Union[GeoDataFrame, List[int]] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Read the values stored in a given band.\n\n        Data Chuncks/blocks\n            When a raster dataset is stored on disk, it might not be stored as one continuous chunk of data. Instead,\n            it can be divided into smaller rectangular blocks or tiles. These blocks can be individually accessed,\n            which is particularly useful for large datasets:\n\n                - Efficiency: Reading or writing small blocks requires less memory than dealing with the entire dataset\n                      at once. This is especially beneficial when only a small portion of the data needs to be processed.\n                - Performance: For certain file formats and operations, working with optimal block sizes can significantly\n                      improve performance. For example, if the block size matches the reading or processing window,\n                      Pyramids can minimize disk access and data transfer.\n\n        Args:\n            band (int, optional):\n                The band you want to get its data. If None, data of all bands will be read. Default is None.\n            window (List[int] | GeoDataFrame, optional):\n                Specify a block of data to read from the dataset. The window can be specified in two ways:\n\n                - List:\n                    Window specified as a list of 4 integers [offset_x, offset_y, window_columns, window_rows].\n\n                    - offset_x/column index: x offset of the block.\n                    - offset_y/row index: y offset of the block.\n                    - window_columns: number of columns in the block.\n                    - window_rows: number of rows in the block.\n\n                - GeoDataFrame:\n                    GeoDataFrame with a geometry column filled with polygon geometries; the function will get the\n                    total_bounds of the GeoDataFrame and use it as a window to read the raster.\n\n        Returns:\n            np.ndarray:\n                array with all the values in the raster.\n\n        Examples:\n            - Create `Dataset` consisting of 4 bands, 5 rows, and 5 columns at the point lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Read all the values stored in a given band:\n\n              ```python\n              &gt;&gt;&gt; arr = dataset.read_array(band=0) # doctest: +SKIP\n              array([[0.50482225, 0.45678043, 0.53294294, 0.28862223, 0.66753579],\n                     [0.38471912, 0.14617829, 0.05045189, 0.00761358, 0.25501918],\n                     [0.32689036, 0.37358843, 0.32233918, 0.75450564, 0.45197608],\n                     [0.22944676, 0.2780928 , 0.71605189, 0.71859309, 0.61896933],\n                     [0.47740168, 0.76490779, 0.07679277, 0.16142599, 0.73630836]])\n\n              ```\n\n            - Read a 2x2 block from the first band. The block starts at the 2nd column (index 1) and 2nd row (index 1)\n                (the first index is the column index):\n\n              ```python\n              &gt;&gt;&gt; arr = dataset.read_array(band=0, window=[1, 1, 2, 2])\n              &gt;&gt;&gt; print(arr) # doctest: +SKIP\n              array([[0.14617829, 0.05045189],\n                     [0.37358843, 0.32233918]])\n\n              ```\n\n            - If you check the values of the 2x2 block, you will find them the same as the values in the entire array\n                of band 0, starting at the 2nd row and 2nd column.\n\n            - Read a block using a GeoDataFrame polygon that covers the same area as the window above:\n\n              ```python\n              &gt;&gt;&gt; import geopandas as gpd\n              &gt;&gt;&gt; from shapely.geometry import Polygon\n              &gt;&gt;&gt; poly = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n              &gt;&gt;&gt; arr = dataset.read_array(band=0, window=poly)\n              &gt;&gt;&gt; print(arr) # doctest: +SKIP\n              array([[0.14617829, 0.05045189],\n                     [0.37358843, 0.32233918]])\n\n              ```\n\n        See Also:\n            - Dataset.get_tile: Read the dataset in chunks.\n            - Dataset.get_block_arrangement: Get block arrangement to read the dataset in chunks.\n        \"\"\"\n        if band is None and self.band_count &gt; 1:\n            rows = self.rows if window is None else window[3]\n            columns = self.columns if window is None else window[2]\n            arr = np.ones(\n                (\n                    self.band_count,\n                    rows,\n                    columns,\n                ),\n                dtype=self.numpy_dtype[0],\n            )\n\n            for i in range(self.band_count):\n                if window is None:\n                    # this line could be replaced with the following line\n                    # arr[i, :, :] = self._iloc(i).ReadAsArray()\n                    arr[i, :, :] = self._raster.GetRasterBand(i + 1).ReadAsArray()\n                else:\n                    arr[i, :, :] = self._read_block(i, window)\n        else:\n            # given band number or the raster has only one band\n            if band is None:\n                band = 0\n            else:\n                if band &gt; self.band_count - 1:\n                    raise ValueError(\n                        f\"band index should be between 0 and {self.band_count - 1}\"\n                    )\n            if window is None:\n                arr = self._iloc(band).ReadAsArray()\n            else:\n                arr = self._read_block(band, window)\n\n        return arr\n\n    def _read_block(\n        self, band: int, window: Union[GeoDataFrame, List[int]]\n    ) -&gt; np.ndarray:\n        \"\"\"Read block of data from the dataset.\n\n        Args:\n            band (int):\n                Band index.\n            window (List[int] | GeoDataFrame):\n                - List[int]: Window to specify a block of data to read from the dataset.\n                    The window should be a list of 4 integers [offset_x, offset_y, window_columns, window_rows].\n                    - offset_x: x offset of the block.\n                    - offset_y: y offset of the block.\n                    - window_columns: number of columns in the block.\n                    - window_rows: number of rows in the block.\n                - GeoDataFrame:\n                    A GeoDataFrame with a polygon geometry. The function will get the total_bounds of the\n                    GeoDataFrame and use it as a window to read the raster.\n\n        Returns:\n            np.ndarray:\n                Array with the values of the block. The shape of the array is (window[2], window[3]), and the\n                location of the block in the raster is (window[0], window[1]).\n        \"\"\"\n        if isinstance(window, GeoDataFrame):\n            window = self._convert_polygon_to_window(window)\n        try:\n            block = self._iloc(band).ReadAsArray(\n                window[0], window[1], window[2], window[3]\n            )\n        except Exception as e:\n            if e.args[0].__contains__(\"Access window out of range in RasterIO()\"):\n                raise OutOfBoundsError(\n                    f\"The window you entered ({window})is out of the raster bounds: {self.rows, self.columns}\"\n                )\n            else:\n                raise e\n        return block\n\n    def _convert_polygon_to_window(\n        self, poly: Union[GeoDataFrame, \"FeatureCollection\"]\n    ) -&gt; List[Any]:\n        poly = FeatureCollection(poly)\n        bounds = poly.total_bounds\n        df = pd.DataFrame(columns=[\"id\", \"x\", \"y\"])\n        df.loc[\"top_left\", [\"x\", \"y\"]] = bounds[0], bounds[3]\n        df.loc[\"bottom_right\", [\"x\", \"y\"]] = bounds[2], bounds[1]\n        arr_indeces = self.map_to_array_coordinates(df)\n        xoff = arr_indeces[0, 1]\n        yoff = arr_indeces[0, 0]\n        x_size = arr_indeces[1, 0] - arr_indeces[0, 0]\n        y_size = arr_indeces[1, 1] - arr_indeces[0, 1]\n        return [xoff, yoff, x_size, y_size]\n\n    @property\n    def top_left_corner(self):\n        \"\"\"Top left corner coordinates.\n\n        See Also:\n            - Dataset.geotransform: Dataset geotransform.\n        \"\"\"\n        return super().top_left_corner\n\n    @property\n    def bounds(self) -&gt; GeoDataFrame:\n        \"\"\"\n        Bounds - the bbox as a geodataframe with a polygon geometry.\n\n        Examples:\n            - Create a Dataset (1 band, 10 rows, 10 columns) at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the bounds of the dataset:\n\n              ```python\n              &gt;&gt;&gt; bounds = dataset.bounds\n              &gt;&gt;&gt; print(bounds) # doctest: +SKIP\n                                                      geometry\n              0  POLYGON ((0 0, 0 -0.5, 0.5 -0.5, 0.5 0, 0 0))\n\n              ```\n\n        See Also:\n            - Dataset.bbox:\n                Dataset bounding box.\n        \"\"\"\n        return self._calculate_bounds()\n\n    @property\n    def bbox(self) -&gt; List:\n        \"\"\"\n        Bound box [xmin, ymin, xmax, ymax].\n\n        Examples:\n            - Create a Dataset (1 band, 10 rows, 10 columns) at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the bounding box of the dataset:\n\n              ```python\n              &gt;&gt;&gt; bbox = dataset.bbox\n              &gt;&gt;&gt; print(bbox) # doctest: +SKIP\n              [0.0, -0.5, 0.5, 0.0]\n\n              ```\n\n        See Also:\n            - Dataset.bounds: Dataset bounding polygon.\n        \"\"\"\n        return self._calculate_bbox()\n\n    @property\n    def lon(self):\n        \"\"\"Longitude coordinates.\n\n        Examples:\n            - Create a Dataset (1 band, 5 rows, 5 columns) at lon/lat (0, 0):\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the longitude/x coordinates of the center of all cells in the dataset:\n\n              ```python\n              &gt;&gt;&gt; print(dataset.lon)\n              [0.025 0.075 0.125 0.175 0.225]\n              &gt;&gt;&gt; print(dataset.x)\n              [0.025 0.075 0.125 0.175 0.225]\n\n              ```\n\n        See Also:\n            - Dataset.x: Dataset x coordinates.\n            - Dataset.lat: Dataset latitude.\n            - Dataset.lon: Dataset longitude.\n        \"\"\"\n        if not hasattr(self, \"_lon\"):\n            pivot_x = self.top_left_corner[0]\n            cell_size = self.cell_size\n            x_coords = [\n                pivot_x + i * cell_size + cell_size / 2 for i in range(self.columns)\n            ]\n        else:\n            # in case the lat and lon are read from the netcdf file just read the values from the file\n            x_coords = self._lon\n        return np.array(x_coords)\n\n    @property\n    def lat(self):\n        \"\"\"Latitude-coordinate.\n\n        Examples:\n            - Create a Dataset (1 band, 5 rows, 5 columns) at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the latitude/y coordinates of the center of all cells in the dataset:\n\n              ```python\n              &gt;&gt;&gt; print(dataset.lat)\n              [-0.025 -0.075 -0.125 -0.175 -0.225]\n              &gt;&gt;&gt; print(dataset.y)\n              [-0.025 -0.075 -0.125 -0.175 -0.225]\n\n              ```\n\n        See Also:\n            - Dataset.x: Dataset x coordinates.\n            - Dataset.y: Dataset y coordinates.\n            - Dataset.lon: Dataset longitude.\n        \"\"\"\n        if not hasattr(self, \"_lat\"):\n            pivot_y = self.top_left_corner[1]\n            cell_size = self.cell_size\n            y_coords = [\n                pivot_y - i * cell_size - cell_size / 2 for i in range(self.rows)\n            ]\n        else:\n            # in case the lat and lon are read from the netcdf file just read the values from the file\n            y_coords = self._lat\n        return np.array(y_coords)\n\n    @property\n    def x(self):\n        \"\"\"X-coordinate/Longitude.\n\n        Examples:\n            - Create `Dataset` consists of 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - To get the longitude/x coordinates of the center of all cells in the dataset.\n\n              ```python\n              &gt;&gt;&gt; print(dataset.lon)\n              [0.025 0.075 0.125 0.175 0.225]\n              &gt;&gt;&gt; print(dataset.x)\n              [0.025 0.075 0.125 0.175 0.225]\n\n              ```\n\n        See Also:\n            - Dataset.lat: Dataset latitude.\n            - Dataset.y: Dataset y coordinates.\n            - Dataset.lon: Dataset longitude.\n        \"\"\"\n        # X_coordinate = upper-left corner x + index * cell size + cell-size/2\n        if not hasattr(self, \"_lon\"):\n            pivot_x = self.top_left_corner[0]\n            cell_size = self.cell_size\n            x_coords = Dataset.get_x_lon_dimension_array(\n                pivot_x, cell_size, self.columns\n            )\n        else:\n            # in case the lat and lon are read from the netcdf file just read the values from the file\n            x_coords = self._lon\n        return np.array(x_coords)\n\n    @staticmethod\n    def get_x_lon_dimension_array(pivot_x, cell_size, columns) -&gt; List[float]:\n        \"\"\"Get X/Lon coordinates.\"\"\"\n        x_coords = [pivot_x + i * cell_size + cell_size / 2 for i in range(columns)]\n        return x_coords\n\n    @property\n    def y(self):\n        \"\"\"Y-coordinate/Latitude.\n\n        Examples:\n            - Create `Dataset` consists of 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - to get the longitude/x coordinates of the center of all cells in the dataset.\n\n              ```python\n              &gt;&gt;&gt; print(dataset.lat)\n              [-0.025 -0.075 -0.125 -0.175 -0.225]\n              &gt;&gt;&gt; print(dataset.y)\n              [-0.025 -0.075 -0.125 -0.175 -0.225]\n\n              ```\n\n        See Also:\n            - Dataset.x: Dataset y coordinates.\n            - Dataset.lat: Dataset latitude.\n            - Dataset.lon: Dataset longitude.\n        \"\"\"\n        # X_coordinate = upper-left corner x + index * cell size + cell-size/2\n        if not hasattr(self, \"_lat\"):\n            pivot_y = self.top_left_corner[1]\n            cell_size = self.cell_size\n            y_coords = Dataset.get_y_lat_dimension_array(pivot_y, cell_size, self.rows)\n        else:\n            # in case the lat and lon are read from the netcdf file, just read the values from the file\n            y_coords = self._lat\n        return np.array(y_coords)\n\n    @staticmethod\n    def get_y_lat_dimension_array(pivot_y, cell_size, rows) -&gt; List[float]:\n        \"\"\"Get Y/Lat coordinates.\"\"\"\n        y_coords = [pivot_y - i * cell_size - cell_size / 2 for i in range(rows)]\n        return y_coords\n\n    @property\n    def gdal_dtype(self):\n        \"\"\"Data Type.\"\"\"\n        return [\n            self.raster.GetRasterBand(i).DataType for i in range(1, self.band_count + 1)\n        ]\n\n    @property\n    def numpy_dtype(self) -&gt; List[type]:\n        \"\"\"List of the numpy data Type of each band, the data type is a numpy function.\"\"\"\n        return [\n            DTYPE_CONVERSION_DF.loc[DTYPE_CONVERSION_DF[\"gdal\"] == i, \"numpy\"].values[0]\n            for i in self.gdal_dtype\n        ]\n\n    @property\n    def dtype(self) -&gt; List[str]:\n        \"\"\"List of the data Type of each band as strings.\"\"\"\n        return [\n            DTYPE_CONVERSION_DF.loc[DTYPE_CONVERSION_DF[\"gdal\"] == i, \"name\"].values[0]\n            for i in self.gdal_dtype\n        ]\n\n    def get_block_arrangement(\n        self, band: int = 0, x_block_size: int = None, y_block_size: int = None\n    ) -&gt; DataFrame:\n        \"\"\"Get Block Arrangement.\n\n        Args:\n            band (int, optional):\n                band index, by default 0\n            x_block_size (int, optional):\n                x block size/number of columns, by default None\n            y_block_size (int, optional):\n                y block size/number of rows, by default None\n\n        Returns:\n            DataFrame:\n                with the following columns: [x_offset, y_offset, window_xsize, window_ysize]\n\n        Examples:\n            - Example of getting block arrangement:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(13, 14)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; df = dataset.get_block_arrangement(x_block_size=5, y_block_size=5)\n              &gt;&gt;&gt; print(df)\n                 x_offset  y_offset  window_xsize  window_ysize\n              0         0         0             5             5\n              1         5         0             5             5\n              2        10         0             4             5\n              3         0         5             5             5\n              4         5         5             5             5\n              5        10         5             4             5\n              6         0        10             5             3\n              7         5        10             5             3\n              8        10        10             4             3\n\n              ```\n        \"\"\"\n        block_sizes = self.block_size[band]\n        x_block_size = block_sizes[0] if x_block_size is None else x_block_size\n        y_block_size = block_sizes[1] if y_block_size is None else y_block_size\n\n        df = pd.DataFrame(\n            [\n                {\n                    \"x_offset\": x,\n                    \"y_offset\": y,\n                    \"window_xsize\": min(x_block_size, self.columns - x),\n                    \"window_ysize\": min(y_block_size, self.rows - y),\n                }\n                for y in range(0, self.rows, y_block_size)\n                for x in range(0, self.columns, x_block_size)\n            ],\n            columns=[\"x_offset\", \"y_offset\", \"window_xsize\", \"window_ysize\"],\n        )\n        return df\n\n    def copy(self, path: str = None) -&gt; \"Dataset\":\n        \"\"\"Deep copy.\n\n        Args:\n            path (str, optional):\n                Destination path to save the copied dataset. If None is passed, the copied dataset will be created in memory.\n\n        Examples:\n            - First, we will create a dataset with 1 band, 3 rows and 5 columns.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(3, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 3 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - Now, we will create a copy of the dataset.\n\n              ```python\n              &gt;&gt;&gt; copied_dataset = dataset.copy(path=\"copy-dataset.tif\")\n              &gt;&gt;&gt; print(copied_dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 3 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File: copy-dataset.tif\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - Now close the dataset.\n\n              ```python\n              &gt;&gt;&gt; copied_dataset.close()\n\n              ```\n\n        \"\"\"\n        if path is None:\n            path = \"\"\n            driver = \"MEM\"\n        else:\n            driver = \"GTiff\"\n\n        src = gdal.GetDriverByName(driver).CreateCopy(path, self._raster)\n\n        return Dataset(src, access=\"write\")\n\n    def close(self):\n        \"\"\"Close the dataset.\"\"\"\n        self._raster.FlushCache()\n        self._raster = None\n\n    def _iloc(self, i: int) -&gt; gdal.Band:\n        \"\"\"_iloc.\n\n            - Access dataset bands using index.\n\n        Args:\n            i (int):\n                index, the index starts from 1.\n\n        Returns:\n            gdal.Band:\n                Gdal Band.\n        \"\"\"\n        if i &lt; 0:\n            raise IndexError(\"negative index not supported\")\n\n        if i &gt; self.band_count - 1:\n            raise IndexError(\n                f\"index {i} is out of bounds for axis 0 with size {self.band_count}\"\n            )\n        band = self.raster.GetRasterBand(i + 1)\n        return band\n\n    def get_attribute_table(self, band: int = 0) -&gt; DataFrame:\n        \"\"\"Get the attribute table for a given band.\n\n            - Get the attribute table of a band.\n\n        Args:\n            band (int):\n                Band index, the index starts from 1.\n\n        Returns:\n            DataFrame:\n                DataFrame with the attribute table.\n\n        Examples:\n            - Read a dataset and fetch its attribute table:\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/south-america-mswep_1979010100.tif\")\n              &gt;&gt;&gt; df = dataset.get_attribute_table()\n              &gt;&gt;&gt; print(df)\n                Precipitation Range (mm)   Category              Description\n              0                     0-50        Low   Very low precipitation\n              1                   51-100   Moderate   Moderate precipitation\n              2                  101-200       High       High precipitation\n              3                  201-500  Very High  Very high precipitation\n              4                     &gt;500    Extreme    Extreme precipitation\n\n              ```\n        \"\"\"\n        band = self._iloc(band)\n        rat = band.GetDefaultRAT()\n        if rat is None:\n            df = None\n        else:\n            df = self._attribute_table_to_df(rat)\n\n        return df\n\n    def set_attribute_table(self, df: DataFrame, band: int = None) -&gt; None:\n        \"\"\"Set the attribute table for a band.\n\n        The attribute table can be used to associate tabular data with the values of a raster band.\n        This is particularly useful for categorical raster data, such as land cover classifications, where each pixel\n        value corresponds to a category that has additional attributes (e.g., class name, color description).\n\n        Notes:\n            - The attribute table is stored in an xml file by the name of the raster file with the extension of .aux.xml.\n            - Setting an attribute table to a band will overwrite the existing attribute table if it exists.\n            - Setting an attribute table to a band does not need the dataset to be opened in a write mode.\n\n        Args:\n            df (DataFrame):\n                DataFrame with the attribute table.\n            band (int):\n                Band index.\n\n        Examples:\n            - First create a dataset:\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.create(\n              ... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n              ... epsg=4326, no_data_value=-9999\n              ... )\n\n              ```\n\n            - Create a DataFrame with the attribute table:\n\n              ```python\n              &gt;&gt;&gt; data = {\n              ...     \"Value\": [1, 2, 3],\n              ...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n              ...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n              ... }\n              &gt;&gt;&gt; df = pd.DataFrame(data)\n\n              ```\n\n            - Set the attribute table to the dataset:\n\n              ```python\n              &gt;&gt;&gt; dataset.set_attribute_table(df, band=0)\n\n              ```\n\n            - Then the attribute table can be retrieved using the `get_attribute_table` method.\n            - The content of the attribute table will be stored in an xml file by the name of the raster file with\n              the extension of .aux.xml. The content of the file will be like the following:\n\n              ```xml\n\n                  &lt;PAMDataset&gt;\n                    &lt;PAMRasterBand band=\"1\"&gt;\n                      &lt;GDALRasterAttributeTable tableType=\"thematic\"&gt;\n                        &lt;FieldDefn index=\"0\"&gt;\n                          &lt;Name&gt;Precipitation Range (mm)&lt;/Name&gt;\n                          &lt;Type&gt;2&lt;/Type&gt;\n                          &lt;Usage&gt;0&lt;/Usage&gt;\n                        &lt;/FieldDefn&gt;\n                        &lt;FieldDefn index=\"1\"&gt;\n                          &lt;Name&gt;Category&lt;/Name&gt;\n                          &lt;Type&gt;2&lt;/Type&gt;\n                          &lt;Usage&gt;0&lt;/Usage&gt;\n                        &lt;/FieldDefn&gt;\n                        &lt;FieldDefn index=\"2\"&gt;\n                          &lt;Name&gt;Description&lt;/Name&gt;\n                          &lt;Type&gt;2&lt;/Type&gt;\n                          &lt;Usage&gt;0&lt;/Usage&gt;\n                        &lt;/FieldDefn&gt;\n                        &lt;Row index=\"0\"&gt;\n                          &lt;F&gt;0-50&lt;/F&gt;\n                          &lt;F&gt;Low&lt;/F&gt;\n                          &lt;F&gt;Very low precipitation&lt;/F&gt;\n                        &lt;/Row&gt;\n                        &lt;Row index=\"1\"&gt;\n                          &lt;F&gt;51-100&lt;/F&gt;\n                          &lt;F&gt;Moderate&lt;/F&gt;\n                          &lt;F&gt;Moderate precipitation&lt;/F&gt;\n                        &lt;/Row&gt;\n                        &lt;Row index=\"2\"&gt;\n                          &lt;F&gt;101-200&lt;/F&gt;\n                          &lt;F&gt;High&lt;/F&gt;\n                          &lt;F&gt;High precipitation&lt;/F&gt;\n                        &lt;/Row&gt;\n                        &lt;Row index=\"3\"&gt;\n                          &lt;F&gt;201-500&lt;/F&gt;\n                          &lt;F&gt;Very High&lt;/F&gt;\n                          &lt;F&gt;Very high precipitation&lt;/F&gt;\n                        &lt;/Row&gt;\n                        &lt;Row index=\"4\"&gt;\n                          &lt;F&gt;&amp;gt;500&lt;/F&gt;\n                          &lt;F&gt;Extreme&lt;/F&gt;\n                          &lt;F&gt;Extreme precipitation&lt;/F&gt;\n                        &lt;/Row&gt;\n                      &lt;/GDALRasterAttributeTable&gt;\n                    &lt;/PAMRasterBand&gt;\n                  &lt;/PAMDataset&gt;\n\n              ```\n        \"\"\"\n        rat = self._df_to_attribute_table(df)\n        band = self._iloc(band)\n        band.SetDefaultRAT(rat)\n\n    @staticmethod\n    def _df_to_attribute_table(df: DataFrame) -&gt; gdal.RasterAttributeTable:\n        \"\"\"df_to_attribute_table.\n\n            Convert a DataFrame to a GDAL RasterAttributeTable.\n\n        Args:\n            df (DataFrame):\n                DataFrame with columns to be converted to RAT columns.\n\n        Returns:\n            gdal.RasterAttributeTable:\n                The resulting RasterAttributeTable.\n        \"\"\"\n        # Create a new RasterAttributeTable\n        rat = gdal.RasterAttributeTable()\n\n        # Create columns in the RAT based on the DataFrame columns\n        for column in df.columns:\n            dtype = df[column].dtype\n            if pd.api.types.is_integer_dtype(dtype):\n                rat.CreateColumn(column, gdal.GFT_Integer, gdal.GFU_Generic)\n            elif pd.api.types.is_float_dtype(dtype):\n                rat.CreateColumn(column, gdal.GFT_Real, gdal.GFU_Generic)\n            else:  # Assume string for any other type\n                rat.CreateColumn(column, gdal.GFT_String, gdal.GFU_Generic)\n\n        # Populate the RAT with the DataFrame data\n        for row_index in range(len(df)):\n            for col_index, column in enumerate(df.columns):\n                dtype = df[column].dtype\n                value = df.iloc[row_index, col_index]\n                if pd.api.types.is_integer_dtype(dtype):\n                    rat.SetValueAsInt(row_index, col_index, int(value))\n                elif pd.api.types.is_float_dtype(dtype):\n                    rat.SetValueAsDouble(row_index, col_index, float(value))\n                else:  # Assume string for any other type\n                    rat.SetValueAsString(row_index, col_index, str(value))\n\n        return rat\n\n    @staticmethod\n    def _attribute_table_to_df(rat: gdal.RasterAttributeTable) -&gt; DataFrame:\n        \"\"\"attribute_table_to_df.\n\n        Convert a GDAL RasterAttributeTable to a pandas DataFrame.\n\n        Args:\n            rat (gdal.RasterAttributeTable):\n                The RasterAttributeTable to convert.\n\n        Returns:\n            pd.DataFrame: The resulting DataFrame.\n        \"\"\"\n        columns = []\n        data = {}\n\n        # Get the column names and create empty lists for data\n        for col_index in range(rat.GetColumnCount()):\n            col_name = rat.GetNameOfCol(col_index)\n            col_type = rat.GetTypeOfCol(col_index)\n            columns.append((col_name, col_type))\n            data[col_name] = []\n\n        # Get the row count\n        row_count = rat.GetRowCount()\n\n        # Populate the data dictionary with RAT values\n        for row_index in range(row_count):\n            for col_index, (col_name, col_type) in enumerate(columns):\n                if col_type == gdal.GFT_Integer:\n                    value = rat.GetValueAsInt(row_index, col_index)\n                elif col_type == gdal.GFT_Real:\n                    value = rat.GetValueAsDouble(row_index, col_index)\n                else:  # gdal.GFT_String\n                    value = rat.GetValueAsString(row_index, col_index)\n                data[col_name].append(value)\n\n        # Create the DataFrame\n        df = pd.DataFrame(data)\n        return df\n\n    def add_band(\n        self,\n        array: np.ndarray,\n        unit: Any = None,\n        attribute_table: DataFrame = None,\n        inplace: bool = False,\n    ) -&gt; Union[None, \"Dataset\"]:\n        \"\"\"Add a new band to the dataset.\n\n        Args:\n            array (np.ndarray):\n                2D array to add as a new band.\n            unit (Any, optional):\n                Unit of the values in the new band.\n            attribute_table (DataFrame, optional):\n                Attribute table provides a way to associate tabular data with the values of a raster band. This is\n                particularly useful for categorical raster data, such as land cover classifications, where each pixel\n                value corresponds to a category that has additional attributes (e.g., class name, color, description).\n                Default is None.\n            inplace (bool, optional):\n                If True the new band will be added to the current dataset, if False the new band will be added to a\n                new dataset. Default is False.\n\n        Returns:\n            None\n\n        Examples:\n            - First create a dataset:\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.create(\n              ... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n              ... epsg=4326, no_data_value=-9999\n              ... )\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float32\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - Create a 2D array to add as a new band:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; array = np.random.rand(10, 10)\n\n              ```\n\n            - Add the new band to the dataset inplace:\n\n              ```python\n              &gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=None, inplace=True)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 2\n                          Band names: ['Band_1', 'Band_2']\n                          Mask: -9999.0\n                          Data type: float32\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - The new band will be added to the dataset inplace.\n            - You can also add an attribute table to the band when you add a new band to the dataset.\n\n              ```python\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; data = {\n              ...     \"Value\": [1, 2, 3],\n              ...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n              ...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n              ... }\n              &gt;&gt;&gt; df = pd.DataFrame(data)\n              &gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=df, inplace=True)\n\n              ```\n\n        See Also:\n            Dataset.create_from_array: create a new dataset from an array.\n            Dataset.create: create a new dataset with an empty band.\n            Dataset.dataset_like: create a new dataset from another dataset.\n            Dataset.get_attribute_table: get the attribute table for a specific band.\n            Dataset.set_attribute_table: Set the attribute table for a specific band.\n        \"\"\"\n        # check the dimensions of the new array\n        if array.ndim != 2:\n            raise ValueError(\"The array must be 2D.\")\n        if array.shape[0] != self.rows or array.shape[1] != self.columns:\n            raise ValueError(\n                f\"The array must have the same dimensions as the raster.{self.rows} {self.columns}\"\n            )\n        # check if the dataset is opened in a write mode\n        if inplace:\n            if self.access == \"read_only\":\n                raise ValueError(\"The dataset is not opened in a write mode.\")\n            else:\n                src = self._raster\n        else:\n            src = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self._raster)\n\n        dtype = numpy_to_gdal_dtype(array.dtype)\n        num_bands = src.RasterCount\n        src.AddBand(dtype, [])\n        band = src.GetRasterBand(num_bands + 1)\n\n        if unit is not None:\n            band.SetUnitType(unit)\n\n        if attribute_table is not None:\n            # Attach the RAT to the raster band\n            rat = Dataset._df_to_attribute_table(attribute_table)\n            band.SetDefaultRAT(rat)\n\n        band.WriteArray(array)\n\n        if inplace:\n            self.__init__(src, self.access)\n        else:\n            return Dataset(src, self.access)\n\n    def stats(self, band: int = None, mask: GeoDataFrame = None) -&gt; DataFrame:\n        \"\"\"Get statistics of a band [Min, max, mean, std].\n\n        Args:\n            band (int, optional):\n                Band index. If None, the statistics of all bands will be returned.\n            mask (Polygon GeoDataFrame or Dataset, optional):\n                GeodataFrame with a geometry of polygon type.\n\n        Returns:\n            DataFrame:\n                DataFrame wit the stats of each band, the dataframe has the following columns\n                [min, max, mean, std], the index of the dataframe is the band names.\n\n                ```text\n\n                                   Min         max        mean       std\n                    Band_1  270.369720  270.762299  270.551361  0.154270\n                    Band_2  269.611938  269.744751  269.673645  0.043788\n                    Band_3  273.641479  274.168823  273.953979  0.198447\n                    Band_4  273.991516  274.540344  274.310669  0.205754\n                ```\n\n        Notes:\n            - The value of the stats will be stored in an xml file by the name of the raster file with the extension of\n              .aux.xml.\n            - The content of the file will be like the following:\n\n              ```xml\n\n                  &lt;PAMDataset&gt;\n                    &lt;PAMRasterBand band=\"1\"&gt;\n                      &lt;Description&gt;Band_1&lt;/Description&gt;\n                      &lt;Metadata&gt;\n                        &lt;MDI key=\"RepresentationType\"&gt;ATHEMATIC&lt;/MDI&gt;\n                        &lt;MDI key=\"STATISTICS_MAXIMUM\"&gt;88&lt;/MDI&gt;\n                        &lt;MDI key=\"STATISTICS_MEAN\"&gt;7.9662921348315&lt;/MDI&gt;\n                        &lt;MDI key=\"STATISTICS_MINIMUM\"&gt;0&lt;/MDI&gt;\n                        &lt;MDI key=\"STATISTICS_STDDEV\"&gt;18.294377743948&lt;/MDI&gt;\n                        &lt;MDI key=\"STATISTICS_VALID_PERCENT\"&gt;48.9&lt;/MDI&gt;\n                      &lt;/Metadata&gt;\n                    &lt;/PAMRasterBand&gt;\n                  &lt;/PAMDataset&gt;\n\n              ```\n\n        Examples:\n            - Get the statistics of all bands in the dataset:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n              &gt;&gt;&gt; print(dataset.stats()) # doctest: +SKIP\n                           min       max      mean       std\n              Band_1  0.006443  0.942943  0.468935  0.266634\n              Band_2  0.020377  0.978130  0.477189  0.306864\n              Band_3  0.019652  0.992184  0.537215  0.286502\n              Band_4  0.011955  0.984313  0.503616  0.295852\n              &gt;&gt;&gt; print(dataset.stats(band=1))  # doctest: +SKIP\n                           min      max      mean       std\n              Band_2  0.020377  0.97813  0.477189  0.306864\n\n              ```\n\n            - Get the statistics of all the bands using a mask polygon.\n\n              - Create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2,\n                0.2 -0.1] to cover the 4 cells.\n              ```python\n              &gt;&gt;&gt; from shapely.geometry import Polygon\n              &gt;&gt;&gt; import geopandas as gpd\n              &gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])],crs=4326)\n              &gt;&gt;&gt; print(dataset.stats(mask=mask))  # doctest: +SKIP\n                           min       max      mean       std\n              Band_1  0.193441  0.702108  0.541478  0.202932\n              Band_2  0.281281  0.932573  0.665602  0.239410\n              Band_3  0.031395  0.982235  0.493086  0.377608\n              Band_4  0.079562  0.930965  0.591025  0.341578\n\n              ```\n\n        \"\"\"\n        if mask is not None:\n            dst = self.crop(mask, touch=True)\n\n        if band is None:\n            df = pd.DataFrame(\n                index=self.band_names,\n                columns=[\"min\", \"max\", \"mean\", \"std\"],\n                dtype=np.float32,\n            )\n            for i in range(self.band_count):\n                if mask is not None:\n                    df.iloc[i, :] = dst._get_stats(i)\n                else:\n                    df.iloc[i, :] = self._get_stats(i)\n        else:\n            df = pd.DataFrame(\n                index=[self.band_names[band]],\n                columns=[\"min\", \"max\", \"mean\", \"std\"],\n                dtype=np.float32,\n            )\n            if mask is not None:\n                df.iloc[0, :] = dst._get_stats(band)\n            else:\n                df.iloc[0, :] = self._get_stats(band)\n\n        return df\n\n    def _get_stats(self, band: int = None) -&gt; List[float]:\n        \"\"\"_get_stats.\"\"\"\n        band_i = self._iloc(band)\n        try:\n            vals = band_i.GetStatistics(True, True)\n        except RuntimeError:\n            # when the GetStatistics gives an error \"RuntimeError: Failed to compute statistics, no valid pixels\n            # found in sampling.\"\n            vals = [0]\n\n        if sum(vals) == 0:\n            warnings.warn(\n                f\"Band {band} has no statistics, and the statistics are going to be calculate\"\n            )\n            vals = band_i.ComputeStatistics(False)\n\n        return vals\n\n    def plot(\n        self,\n        band: int = None,\n        exclude_value: Any = None,\n        rgb: List[int] = None,\n        surface_reflectance: int = 10000,\n        cutoff: List = None,\n        overview: bool = False,\n        overview_index: int = 0,\n        **kwargs: Any,\n    ) -&gt; Tuple[Any, Any]:\n        \"\"\"Plot the values/overviews of a given band.\n\n        Args:\n            band (int, optional):\n                The band you want to get its data. Default is 0.\n            exclude_value (Any, optional):\n                Value to exclude from the plot. Default is None.\n            rgb (List[int], optional):\n                The `plot` method will check if the RGB bands are defined in the raster file; if all three bands\n                (red, green, blue) are defined, the method will use them to plot the real image; otherwise, the\n                RGB bands will be considered as [2, 1, 0].\n            surface_reflectance (int, optional):\n                Default is 10,000.\n            cutoff (List, optional):\n                Clip the range of pixel values for each band (take only the pixel values from 0 to the value of the cutoff\n                and scale them back to between 0 and 1). Default is None.\n            overview (bool, optional):\n                True if you want to plot the overview. Default is False.\n            overview_index (int, optional):\n                Index of the overview. Default is 0.\n            kwargs:\n                | Parameter                   | Type                | Description |\n                |-----------------------------|---------------------|-------------|\n                | `points`                    | array               | 3 column array with the first column as the value to display for the point, the second as the row index, and the third as the column index in the array. The second and third columns tell the location of the point. |\n                | `point_color`               | str                 | Color of the point. |\n                | `point_size`                | Any                 | Size of the point. |\n                | `pid_color`                 | str                 | Color of the annotation of the point. Default is blue. |\n                | `pid_size`                  | Any                 | Size of the point annotation. |\n                | `figsize`                   | tuple, optional     | Figure size. Default is `(8, 8)`. |\n                | `title`                     | str, optional       | Title of the plot. Default is `'Total Discharge'`. |\n                | `title_size`                | int, optional       | Title size. Default is `15`. |\n                | `orientation`               | str, optional       | Orientation of the color bar (`horizontal` or `vertical`). Default is `'vertical'`. |\n                | `rotation`                  | number, optional    | Rotation of the color bar label. Default is `-90`. |\n                | `cbar_length`               | float, optional     | Ratio to control the height of the color bar. Default is `0.75`. |\n                | `ticks_spacing`             | int, optional       | Spacing between color bar ticks. Default is `2`. |\n                | `cbar_label_size`           | int, optional       | Size of the color bar label. Default is `12`. |\n                | `cbar_label`                | str, optional       | Label of the color bar. Default is `'Discharge m\u00b3/s'`. |\n                | `color_scale`               | int, optional       | Scale mode for colors. Options: 1 = normal, 2 = power, 3 = SymLogNorm, 4 = PowerNorm, 5 = BoundaryNorm. Default is `1`. |\n                | `gamma`                     | float, optional     | Value needed for color scale option 2. Default is `1/2`. |\n                | `line_threshold`            | float, optional     | Value needed for color scale option 3. Default is `0.0001`. |\n                | `line_scale`                | float, optional     | Value needed for color scale option 3. Default is `0.001`. |\n                | `bounds`                    | list, optional      | Discrete bounds for color scale option 4. Default is `None`. |\n                | `midpoint`                  | float, optional     | Value needed for color scale option 5. Default is `0`. |\n                | `cmap`                      | str, optional       | Color map style. Default is `'coolwarm_r'`. |\n                | `display_cell_value`        | bool, optional      | Whether to display cell values as text. |\n                | `num_size`                  | int, optional       | Size of numbers plotted on top of each cell. Default is `8`. |\n                | `background_color_threshold`| float or int, optional | Threshold for deciding text color over cells: if value &gt; threshold \u2192 black text; else white text. If `None`, max value / 2 is used. Default is `None`. |\n\n\n        Returns:\n            Tuple[Any, Any]:\n                The axes of the matplotlib figure and the figure object.\n\n        Examples:\n            - Plot a certain band:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n              &gt;&gt;&gt; dataset.plot(band=0)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n\n            - plot using power scale.\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, color_scale=2)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n\n            - plot using SymLogNorm scale.\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, color_scale=3)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n\n            - plot using PowerNorm scale.\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, color_scale=4, bounds=[0, 0.2, 0.4, 0.6, 0.8, 1])\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n\n            - plot using BoundaryNorm scale.\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, color_scale=5)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n        \"\"\"\n        import_cleopatra(\n            \"The current function uses cleopatra package to for plotting, please install it manually, for more info \"\n            \"check https://github.com/Serapieum-of-alex/cleopatra\"\n        )\n        from cleopatra.array_glyph import ArrayGlyph\n\n        no_data_value = [np.nan if i is None else i for i in self.no_data_value]\n        if overview:\n            arr = self.read_overview_array(band=band, overview_index=overview_index)\n        else:\n            arr = self.read_array(band=band)\n        # if the raster has three bands or more.\n        if self.band_count &gt;= 3:\n            if band is None:\n                if rgb is None:\n                    rgb = [\n                        self.get_band_by_color(\"red\"),\n                        self.get_band_by_color(\"green\"),\n                        self.get_band_by_color(\"blue\"),\n                    ]\n                    if None in rgb:\n                        rgb = [2, 1, 0]\n                # first make the band index the first band in the rgb list (red band)\n                band = rgb[0]\n        # elif self.band_count == 1:\n        #     band = 0\n        else:\n            if band is None:\n                band = 0\n\n        exclude_value = (\n            [no_data_value[band], exclude_value]\n            if exclude_value is not None\n            else [no_data_value[band]]\n        )\n\n        cleo = ArrayGlyph(\n            arr,\n            exclude_value=exclude_value,\n            extent=self.bbox,\n            rgb=rgb,\n            surface_reflectance=surface_reflectance,\n            cutoff=cutoff,\n            **kwargs,\n        )\n        fig, ax = cleo.plot(**kwargs)\n        return fig, ax\n\n    @staticmethod\n    def _create_dataset(\n        cols: int,\n        rows: int,\n        bands: int,\n        dtype: int,\n        driver: str = \"MEM\",\n        path: str = None,\n    ) -&gt; gdal.Dataset:\n        \"\"\"Create a GDAL driver.\n\n            creates a driver and save it to disk and in memory if the path is not given.\n\n        Args:\n            cols (int):\n                Number of columns.\n            rows (int):\n                Number of rows.\n            bands (int):\n                Number of bands.\n            dtype:\n                GDAL data type; use functions in the utils module to map data types from numpy or ogr to gdal.\n                gdal data type, the data type should be one of the following code:\n                    ```python\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], which refers to the following data types:.\n                    GDT_Unknown\t0\tGDT_UInt32\t4\tGDT_CInt16\t8\tGDT_UInt64\t12\n                    GDT_Byte\t1\tGDT_Int32\t5\tGDT_CInt32\t9\tGDT_Int64\t13\n                    GDT_UInt16\t2\tGDT_Float32 6\tGDT_CFloat32 10\tGDT_Int8\t14\n                    GDT_Int16\t3\tGDT_Float64 7\tGDT_CFloat64 11\tGDT_TypeCount 15\n                    ```\n            driver (str):\n                Driver type [\"GTiff\", \"MEM\"].\n            path (str):\n                Path to save the GTiff driver.\n\n        Returns:\n            gdal driver\n        \"\"\"\n        if path:\n            driver = \"GTiff\" if driver == \"MEM\" else driver\n            if not isinstance(path, str):\n                raise TypeError(\"The path input should be string\")\n            if driver == \"GTiff\":\n                if not path.endswith(\".tif\"):\n                    raise TypeError(\n                        \"The path to save the created raster should end with .tif\"\n                    )\n            # LZW is a lossless compression method achieve the highest compression but with a lot of computations.\n            src = gdal.GetDriverByName(driver).Create(\n                path, cols, rows, bands, dtype, [\"COMPRESS=LZW\"]\n            )\n        else:\n            # for memory drivers\n            driver = \"MEM\"\n            src = gdal.GetDriverByName(driver).Create(\"\", cols, rows, bands, dtype)\n        return src\n\n    @classmethod\n    def create(\n        cls,\n        cell_size: Union[int, float],\n        rows: int,\n        columns: int,\n        dtype: str,\n        bands: int,\n        top_left_corner: Tuple,\n        epsg: int,\n        no_data_value: Any = None,\n        path: str = None,\n    ) -&gt; \"Dataset\":\n        \"\"\"Create a new dataset and fill it with the no_data_value.\n\n        The new dataset will have an array filled with the no_data_value.\n\n        Args:\n            cell_size (int|float):\n                Cell size.\n            rows (int):\n                Number of rows.\n            columns (int):\n                Number of columns.\n            dtype (str):\n                Data type. One of: None, \"byte\", \"uint16\", \"int16\", \"uint32\", \"int32\", \"float32\", \"float64\",\n                \"complex-int16\", \"complex-int32\", \"complex-float32\", \"complex-float64\", \"uint64\", \"int64\", \"int8\",\n                \"count\".\n            bands (int|None):\n                Number of bands to create in the output raster.\n            top_left_corner (Tuple):\n                Coordinates of the top left corner point.\n            epsg (int):\n                EPSG number to identify the projection of the coordinates in the created raster.\n            no_data_value (float|None):\n                No data value.\n            path (str, optional):\n                Path on disk; if None, the dataset is created in memory. Default is None.\n\n        Returns:\n            Dataset: A new dataset\n\n        Hint:\n            - The no_data_value will be filled in the array of the output dataset.\n            - The coordinates of the top left corner point should be in the same projection as the epsg.\n            - The cell size should be in the same unit as the coordinates.\n            - The number of rows and columns should be positive integers.\n\n        Examples:\n            - To create a dataset using the `create` method you need to provide all the information needed to locate the\n             dataset in space `top_left_corner` and `epsg`, then the information needed to specify the data to be stored\n             in the dataset like `dtype`, `rows`, `columns`, `cell_size`, `bands` and `no_data_value`.\n\n              ```python\n              &gt;&gt;&gt; cell_size = 10\n              &gt;&gt;&gt; rows = 5\n              &gt;&gt;&gt; columns = 5\n              &gt;&gt;&gt; dtype = \"float32\"\n              &gt;&gt;&gt; bands = 1\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; epsg = 32618\n              &gt;&gt;&gt; no_data_value = -9999\n              &gt;&gt;&gt; path = \"create-new-dataset.tif\"\n              &gt;&gt;&gt; dataset = Dataset.create(cell_size, rows, columns, dtype, bands, top_left_corner, epsg, no_data_value, path)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 10.0\n                          Dimension: 5 * 5\n                          EPSG: 32618\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float32\n                          File: create-new-dataset.tif\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - If you check the value stored in the band using the `read_array` method, you will find that the band is\n                full of the `no_data_value` value which we used here as -9999.\n\n              ```python\n              &gt;&gt;&gt; print(dataset.read_array(band=0))\n              [[-9999. -9999. -9999. -9999. -9999.]\n               [-9999. -9999. -9999. -9999. -9999.]\n               [-9999. -9999. -9999. -9999. -9999.]\n               [-9999. -9999. -9999. -9999. -9999.]\n               [-9999. -9999. -9999. -9999. -9999.]]\n\n              ```\n        \"\"\"\n        # Create the driver.\n        dtype = numpy_to_gdal_dtype(dtype)\n        dst = Dataset._create_dataset(columns, rows, bands, dtype, path=path)\n        sr = Dataset._create_sr_from_epsg(epsg)\n        geotransform = (\n            top_left_corner[0],\n            cell_size,\n            0,\n            top_left_corner[1],\n            0,\n            -1 * cell_size,\n        )\n        dst.SetGeoTransform(geotransform)\n        # Set the projection.\n        dst.SetProjection(sr.ExportToWkt())\n\n        dst = cls(dst, access=\"write\")\n        if no_data_value is not None:\n            dst._set_no_data_value(no_data_value=no_data_value)\n\n        return dst\n\n    @classmethod\n    def create_from_array(\n        cls,\n        arr: np.ndarray,\n        top_left_corner: Tuple[float, float] = None,\n        cell_size: Union[int, float] = None,\n        geo: Tuple[float, float, float, float, float, float] = None,\n        epsg: Union[str, int] = 4326,\n        no_data_value: Union[Any, list] = DEFAULT_NO_DATA_VALUE,\n        driver_type: str = \"MEM\",\n        path: str = None,\n    ) -&gt; \"Dataset\":\n        \"\"\"Create a new dataset from an array.\n\n        Args:\n            arr (np.ndarray):\n                Numpy array.\n            top_left_corner (Tuple[float, float], optional):\n                The coordinates of the top left corner of the dataset.\n            cell_size (int|float, optional):\n                Cell size in the same units of the coordinate reference system defined by the `epsg` parameter.\n            geo (Tuple[float, float, float, float, float, float], optional):\n                Geotransform tuple (minimum lon/x, pixel-size, rotation, maximum lat/y, rotation, pixel-size).\n            epsg (int):\n                Integer reference number to the projection (https://epsg.io/).\n            no_data_value (Any, optional):\n                No data value to mask the cells out of the domain. The default is -9999.\n            driver_type (str, optional):\n                Driver type [\"GTiff\", \"MEM\", \"netcdf\"]. Default is \"MEM\".\n            path (str, optional):\n                Path to save the driver.\n\n        Returns:\n            Dataset:\n                Dataset object will be returned.\n\n        Hint:\n            - The `geo` parameter can replace both the `cell_size` and the `top_left_corner` parameters.\n            - The function checks first if the `geo` parameter is defined; it will ignore the `cell_size` and the `top_left_corner` parameters if given.\n\n        Examples:\n            - Create dataset using the `cell_size` and `top_left_corner` parameters.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File: ...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - Create dataset using the `geo` parameter.\n\n              - To create the same dataset using the `geotransform` parameter, we will use the dataset `top_left_corner`\n                coordinates and the `cell_size` to create it.\n\n              ```python\n              &gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File: ...\n              &lt;BLANKLINE&gt;\n\n              ```\n        \"\"\"\n        if geo is None:\n            if top_left_corner is None or cell_size is None:\n                raise ValueError(\n                    \"Either top_left_corner and cell_size or geo should be provided.\"\n                )\n            geo = (\n                top_left_corner[0],\n                cell_size,\n                0,\n                top_left_corner[1],\n                0,\n                -1 * cell_size,\n            )\n\n        if arr.ndim == 2:\n            bands = 1\n            rows = int(arr.shape[0])\n            cols = int(arr.shape[1])\n        else:\n            bands = arr.shape[0]\n            rows = int(arr.shape[1])\n            cols = int(arr.shape[2])\n\n        dst_obj = cls._create_gtiff_from_array(\n            arr,\n            cols,\n            rows,\n            bands,\n            geo,\n            epsg,\n            no_data_value,\n            driver_type=driver_type,\n            path=path,\n        )\n\n        return dst_obj\n\n    @staticmethod\n    def _create_gtiff_from_array(\n        arr: np.ndarray,\n        cols: int,\n        rows: int,\n        bands: int = None,\n        geo: Tuple[float, float, float, float, float, float] = None,\n        epsg: Union[str, int] = None,\n        no_data_value: Union[Any, list] = DEFAULT_NO_DATA_VALUE,\n        driver_type: str = \"MEM\",\n        path: str = None,\n    ) -&gt; \"Dataset\":\n        dtype = numpy_to_gdal_dtype(arr)\n        dst_ds = Dataset._create_dataset(\n            cols, rows, bands, dtype, driver=driver_type, path=path\n        )\n\n        srse = Dataset._create_sr_from_epsg(epsg=epsg)\n        dst_ds.SetProjection(srse.ExportToWkt())\n        dst_ds.SetGeoTransform(geo)\n\n        dst_obj = Dataset(dst_ds, access=\"write\")\n        dst_obj._set_no_data_value(no_data_value=no_data_value)\n\n        if bands == 1:\n            dst_obj.raster.GetRasterBand(1).WriteArray(arr)\n        else:\n            for i in range(bands):\n                dst_obj.raster.GetRasterBand(i + 1).WriteArray(arr[i, :, :])\n\n        # flush data to disk\n        if path is not None:\n            dst_obj._raster.FlushCache()\n        return dst_obj\n\n    @classmethod\n    def dataset_like(\n        cls,\n        src: \"Dataset\",\n        array: np.ndarray,\n        path: str = None,\n    ) -&gt; \"Dataset\":\n        \"\"\"Create a new dataset like another dataset.\n\n        dataset_like method creates a Dataset from an array like another source dataset. The new dataset\n        will have the same `projection`, `coordinates` or the `top left corner` of the original dataset,\n        `cell size`, `no_data_velue`, and number of `rows` and `columns`.\n        the array and the source dataset should have the same number of columns and rows\n\n        Args:\n            src (Dataset):\n                source raster to get the spatial information\n            array (ndarray):\n                data to store in the new dataset.\n            path (str, optional):\n                path to save the new dataset, if not given, the method will return in-memory dataset.\n\n        Returns:\n            Dataset:\n                if the `path` is given, the method will save the new raster to the given path, else the method will\n                return an in-memory dataset.\n\n        Hint:\n            - If the given array is 3D, the bands have to be the first dimension, the x/lon has to be the second\n              dimension, and the y/lon has to be the third dimension of the array.\n\n        Examples:\n            - Create a source dataset and then create another dataset like it:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Now let's create another `dataset` from the previous dataset using the `dataset_like`:\n\n              ```python\n              &gt;&gt;&gt; new_arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; dataset_new = Dataset.dataset_like(dataset, new_arr)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 5 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n        \"\"\"\n        if not isinstance(array, np.ndarray):\n            raise TypeError(\"array should be of type numpy array\")\n\n        if array.ndim == 2:\n            bands = 1\n        else:\n            bands = array.shape[0]\n\n        dtype = numpy_to_gdal_dtype(array)\n\n        dst = Dataset._create_dataset(src.columns, src.rows, bands, dtype, path=path)\n\n        dst.SetGeoTransform(src.geotransform)\n        dst.SetProjection(src.crs)\n        # setting the NoDataValue does not accept double precision numbers\n        dst_obj = cls(dst, access=\"write\")\n        dst_obj._set_no_data_value(no_data_value=src.no_data_value[0])\n\n        if bands == 1:\n            dst_obj.raster.GetRasterBand(1).WriteArray(array)\n        else:\n            for band_i in range(bands):\n                dst_obj.raster.GetRasterBand(band_i + 1).WriteArray(array[band_i, :, :])\n\n        if path is not None:\n            dst_obj.raster.FlushCache()\n\n        return dst_obj\n\n    def write_array(self, array: np.array, top_left_corner: List[Any] = None):\n        \"\"\"Write an array to the dataset at the given xoff, yoff position.\n\n        Args:\n            array (np.ndarray):\n                The array to write\n            top_left_corner (List[float, float]):\n                indices [row, column]/[y_offset, x_offset] of the cell to write the array to. If None, the array will\n                be written to the top left corner of the dataset.\n\n        Raises:\n            Exception: If the array is not written successfully.\n\n        Hint:\n            - The `Dataset` has to be opened in a write mode `read_only=False`.\n\n        Returns:\n        None\n\n        Examples:\n            - First, create a dataset on disk:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; path = 'write_array.tif'\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(\n              ...     arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326, path=path\n              ... )\n              &gt;&gt;&gt; dataset = None\n\n              ```\n\n            - In a later session you can read the dataset in a `write` mode and update it:\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.read_file(path, read_only=False)\n              &gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n              &gt;&gt;&gt; dataset.write_array(arr, top_left_corner=[1, 1])\n              &gt;&gt;&gt; dataset.read_array()    # doctest: +SKIP\n              array([[0.77359738, 0.64789596, 0.37912658, 0.03673771, 0.69571106],\n                     [0.60804387, 1.        , 2.        , 0.501909  , 0.99597122],\n                     [0.83879291, 3.        , 4.        , 0.33058081, 0.59824467],\n                     [0.774213  , 0.94338147, 0.16443719, 0.28041457, 0.61914179],\n                     [0.97201104, 0.81364799, 0.35157525, 0.65554998, 0.8589739 ]])\n\n              ```\n        \"\"\"\n        yoff, xoff = top_left_corner\n        try:\n            self._raster.WriteArray(array, xoff=xoff, yoff=yoff)\n            self._raster.FlushCache()\n        except Exception as e:\n            raise e\n\n    def _get_crs(self) -&gt; str:\n        \"\"\"Get coordinate reference system.\"\"\"\n        return self.raster.GetProjection()\n\n    def set_crs(self, crs: Optional = None, epsg: int = None):\n        \"\"\"Set the Coordinate Reference System (CRS).\n\n            Set the Coordinate Reference System (CRS) of a\n\n        Args:\n            crs (str):\n                Optional if epsg is specified. WKT string. i.e.\n                    ```\n                    'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\", 6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"],\n                    AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",\n                    0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],\n                    AUTHORITY[\"EPSG\",\"4326\"]]'\n                    ```\n            epsg (int):\n                Optional if crs is specified. EPSG code specifying the projection.\n        \"\"\"\n        # first change the projection of the gdal dataset object\n        # second change the epsg attribute of the Dataset object\n        if self.driver_type == \"ascii\":\n            raise TypeError(\n                \"Setting CRS for ASCII file is not possible, you can save the files to a geotiff and then reset the crs\"\n            )\n        else:\n            if crs is not None:\n                self.raster.SetProjection(crs)\n                self._epsg = FeatureCollection.get_epsg_from_prj(crs)\n            else:\n                sr = Dataset._create_sr_from_epsg(epsg)\n                self.raster.SetProjection(sr.ExportToWkt())\n                self._epsg = epsg\n\n    def to_crs(\n        self,\n        to_epsg: int,\n        method: str = \"nearest neighbor\",\n        maintain_alignment: int = False,\n        inplace: bool = False,\n    ) -&gt; Union[\"Dataset\", None]:\n        \"\"\"Reproject the dataset to any projection.\n\n            (default the WGS84 web mercator projection, without resampling)\n\n        Args:\n            to_epsg (int):\n                reference number to the new projection (https://epsg.io/). Default 3857 is the reference number of WGS84\n                web mercator.\n            method (str):\n                resampling method. Default is \"nearest neighbor\". See https://gisgeography.com/raster-resampling/.\n                Allowed values: \"nearest neighbor\", \"cubic\", \"bilinear\".\n            maintain_alignment (bool):\n                True to maintain the number of rows and columns of the raster the same after reprojection.\n                Default is False.\n            inplace (bool):\n                True to make changes inplace. Default is False.\n\n        Returns:\n            Dataset:\n                Dataset object, if inplace is True, the method returns None.\n\n        Examples:\n            - Create a dataset and reproject it:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 5 * 5\n                          EPSG: 4326\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n              &gt;&gt;&gt; print(dataset.crs)\n              GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n              &gt;&gt;&gt; print(dataset.epsg)\n              4326\n              &gt;&gt;&gt; reprojected_dataset = dataset.to_crs(to_epsg=3857)\n              &gt;&gt;&gt; print(reprojected_dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 5565.983370404396\n                          Dimension: 5 * 5\n                          EPSG: 3857\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n              &gt;&gt;&gt; print(reprojected_dataset.crs)\n              PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]\n              &gt;&gt;&gt; print(reprojected_dataset.epsg)\n              3857\n\n              ```\n\n        \"\"\"\n        if not isinstance(to_epsg, int):\n            raise TypeError(\n                \"please enter correct integer number for to_epsg more information \"\n                f\"https://epsg.io/, given {type(to_epsg)}\"\n            )\n        if not isinstance(method, str):\n            raise TypeError(\n                \"Please enter a correct method, for more information, see documentation \"\n            )\n        if method not in INTERPOLATION_METHODS.keys():\n            raise ValueError(\n                f\"The given interpolation method: {method} does not exist, existing methods are {INTERPOLATION_METHODS.keys()}\"\n            )\n\n        method = INTERPOLATION_METHODS.get(method)\n\n        if maintain_alignment:\n            dst_obj = self._reproject_with_ReprojectImage(to_epsg, method)\n        else:\n            dst = gdal.Warp(\"\", self.raster, dstSRS=f\"EPSG:{to_epsg}\", format=\"VRT\")\n            dst_obj = Dataset(dst)\n\n        if inplace:\n            self.__init__(dst_obj.raster)\n        else:\n            return dst_obj\n\n    def _get_epsg(self) -&gt; int:\n        \"\"\"Get the EPSG number.\n\n            This function reads the projection of a GEOGCS file or tiff file.\n\n        Returns:\n            int: EPSG number.\n        \"\"\"\n        prj = self._get_crs()\n        epsg = FeatureCollection.get_epsg_from_prj(prj)\n\n        return epsg\n\n    def count_domain_cells(self, band: int = 0) -&gt; int:\n        \"\"\"Count cells inside the domain.\n\n        Args:\n            band (int):\n                Band index. Default is 0.\n\n        Returns:\n            int:\n                Number of cells.\n        \"\"\"\n        arr = self.read_array(band=band)\n        domain_count = np.size(arr[:, :]) - np.count_nonzero(\n            (arr[np.isclose(arr, self.no_data_value[band], rtol=0.001)])\n        )\n        return domain_count\n\n    @staticmethod\n    def _create_sr_from_epsg(epsg: int = None) -&gt; SpatialReference:\n        \"\"\"Create a spatial reference object from EPSG number.\n\n        https://gdal.org/tutorials/osr_api_tut.html\n\n        Args:\n            epsg (int):\n                EPSG number.\n\n        Returns:\n            SpatialReference:\n                SpatialReference object.\n        \"\"\"\n        sr = osr.SpatialReference()\n        sr.ImportFromEPSG(int(epsg))\n        return sr\n\n    def _get_band_names(self) -&gt; List[str]:\n        \"\"\"Get band names from band metadata if exists otherwise will return index [1,2, ...].\n\n        Returns:\n            list[str]:\n                List of band names.\n        \"\"\"\n        names = []\n        for i in range(1, self.band_count + 1):\n            band_i = self.raster.GetRasterBand(i)\n\n            if band_i.GetDescription():\n                # Use the band_i description.\n                names.append(band_i.GetDescription())\n            else:\n                # Check for metadata.\n                band_i_name = \"Band_{}\".format(band_i.GetBand())\n                metadata = band_i.GetDataset().GetMetadata_Dict()\n\n                # If in metadata, return the metadata entry, else Band_N.\n                if band_i_name in metadata and metadata[band_i_name]:\n                    names.append(metadata[band_i_name])\n                else:\n                    names.append(band_i_name)\n\n        return names\n\n    def _set_band_names(self, name_list: List):\n        \"\"\"Set band names from a given list of names.\n\n        Returns:\n            list[str]:\n                List of band names.\n        \"\"\"\n        for i in range(self.band_count):\n            # first set the band name in the gdal dataset object\n            band_i = self.raster.GetRasterBand(i + 1)\n            band_i.SetDescription(name_list[i])\n            # second, change the band names in the _band_names property.\n            self._band_names[i] = name_list[i]\n\n    def _check_no_data_value(self, no_data_value: List):\n        \"\"\"Validate the no_data_value with the dtype of the object.\n\n        Args:\n            no_data_value:\n                No-data value(s) to validate.\n\n        Returns:\n            Any:\n                Convert the no_data_value to comply with the dtype.\n        \"\"\"\n        # convert the no_data_value based on the dtype of each raster band.\n        for i, val in enumerate(self.gdal_dtype):\n            try:\n                val = no_data_value[i]\n                # if not None or np.nan\n                if val is not None and not np.isnan(val):\n                    # if val &lt; np.iinfo(self.dtype[i]).min or val &gt; np.iinfo(self.dtype[i]).max:\n                    # if the no_data_value is out of the range of the data type\n                    no_data_value[i] = self.numpy_dtype[i](val)\n                else:\n                    # None and np.nan\n                    if self.dtype[i].startswith(\"u\"):\n                        # only Unsigned integer data types.\n                        # if None or np.nan it will make a problem with the unsigned integer data type\n                        # use the max bound of the data type as a no_data_value\n                        no_data_value[i] = np.iinfo(self.dtype[i]).max\n                    else:\n                        # no_data_type is None/np,nan and all other data types that is not Unsigned integer\n                        no_data_value[i] = val\n\n            except OverflowError:\n                # no_data_value = -3.4028230607370965e+38, numpy_dtype = np.int64\n                warnings.warn(\n                    f\"The no_data_value:{no_data_value[i]} is out of range, Band data type is {self.numpy_dtype[i]}\"\n                )\n                no_data_value[i] = self.numpy_dtype[i](DEFAULT_NO_DATA_VALUE)\n        return no_data_value\n\n    def _set_no_data_value(\n        self, no_data_value: Union[Any, list] = DEFAULT_NO_DATA_VALUE\n    ):\n        \"\"\"setNoDataValue.\n\n            - Set the no data value in all raster bands.\n            - Fill the whole raster with the no_data_value.\n            - used only when creating an empty driver.\n\n            now the no_data_value is converted to the dtype of the raster bands and updated in the\n            dataset attribute, gdal no_data_value attribute, used to fill the raster band.\n            from here you have to use the no_data_value stored in the no_data_value attribute as it is updated.\n\n        Args:\n            no_data_value (numeric):\n                No data value to fill the masked part of the array.\n        \"\"\"\n        if not isinstance(no_data_value, list):\n            no_data_value = [no_data_value] * self.band_count\n\n        no_data_value = self._check_no_data_value(no_data_value)\n\n        for band in range(self.band_count):\n            try:\n                # now the no_data_value is converted to the dtype of the raster bands and updated in the\n                # dataset attribute, gdal no_data_value attribute, used to fill the raster band.\n                # from here you have to use the no_data_value stored in the no_data_value attribute as it is updated.\n                self._set_no_data_value_backend(band, no_data_value[band])\n            except Exception as e:\n                if str(e).__contains__(\n                    \"Attempt to write to read only dataset in GDALRasterBand::Fill().\"\n                ):\n                    raise ReadOnlyError(\n                        \"The Dataset is open with a read only, please read the raster using update access mode\"\n                    )\n                elif str(e).__contains__(\n                    \"in method 'Band_SetNoDataValue', argument 2 of type 'double'\"\n                ):\n                    self._set_no_data_value_backend(\n                        band, np.float64(no_data_value[band])\n                    )\n                else:\n                    self._set_no_data_value_backend(band, DEFAULT_NO_DATA_VALUE)\n                    logger.warning(\n                        \"the type of the given no_data_value differs from the dtype of the raster\"\n                        f\"no_data_value now is set to {DEFAULT_NO_DATA_VALUE} in the raster\"\n                    )\n\n    def _calculate_bbox(self) -&gt; List:\n        \"\"\"Calculate bounding box.\"\"\"\n        xmin, ymax = self.top_left_corner\n        ymin = ymax - self.rows * self.cell_size\n        xmax = xmin + self.columns * self.cell_size\n        return [xmin, ymin, xmax, ymax]\n\n    def _calculate_bounds(self) -&gt; GeoDataFrame:\n        \"\"\"Get the bbox as a geodataframe with a polygon geometry.\"\"\"\n        xmin, ymin, xmax, ymax = self._calculate_bbox()\n        coords = [(xmin, ymax), (xmin, ymin), (xmax, ymin), (xmax, ymax)]\n        poly = FeatureCollection.create_polygon(coords)\n        gdf = gpd.GeoDataFrame(geometry=[poly])\n        gdf.set_crs(epsg=self.epsg, inplace=True)\n        return gdf\n\n    def _set_no_data_value_backend(self, band_i: int, no_data_value: Any):\n        \"\"\"\n            - band_i starts from 0 to the number of bands-1.\n\n        Args:\n            band_i:\n                Band index, starts from 0.\n            no_data_value:\n                Numerical value.\n        \"\"\"\n        # check if the dtype of the no_data_value complies with the dtype of the raster itself.\n        self._change_no_data_value_attr(band_i, no_data_value)\n        # initialize the band with the nodata value instead of 0\n        # the no_data_value may have changed inside the _change_no_data_value_attr method to float64, so redefine it.\n        no_data_value = self.no_data_value[band_i]\n        try:\n            self.raster.GetRasterBand(band_i + 1).Fill(no_data_value)\n        except Exception as e:\n            if str(e).__contains__(\" argument 2 of type 'double'\"):\n                self.raster.GetRasterBand(band_i + 1).Fill(np.float64(no_data_value))\n            elif str(e).__contains__(\n                    \"Attempt to write to read only dataset in GDALRasterBand::Fill().\"\n            ) or str(e).__contains__(\"attempt to write to dataset opened in read-only mode.\"):\n                raise ReadOnlyError(\n                    \"The Dataset is open with a read only, please read the raster using update access mode\"\n                )\n            else:\n                raise ValueError(\n                    f\"Failed to fill the band {band_i} with value: {no_data_value}, because of {e}\"\n                )\n        # update the no_data_value in the Dataset object\n        self.no_data_value[band_i] = no_data_value\n\n    def _change_no_data_value_attr(self, band: int, no_data_value):\n        \"\"\"Change the no_data_value attribute.\n\n            - Change only the no_data_value attribute in the gdal Datacube object.\n            - Change the no_data_value in the Dataset object for the given band index.\n            - The corresponding value in the array will not be changed.\n\n        Args:\n            band (int):\n                Band index, starts from 0.\n            no_data_value (Any):\n                No data value.\n        \"\"\"\n        try:\n            self.raster.GetRasterBand(band + 1).SetNoDataValue(no_data_value)\n        except Exception as e:\n            if str(e).__contains__(\n                \"Attempt to write to read only dataset in GDALRasterBand::Fill().\"\n            ):\n                raise ReadOnlyError(\n                    \"The Dataset is open with a read only, please read the raster using update \"\n                    \"access mode\"\n                )\n            # TypeError\n            elif e.args == (\n                \"in method 'Band_SetNoDataValue', argument 2 of type 'double'\",\n            ):\n                no_data_value = np.float64(no_data_value)\n                self.raster.GetRasterBand(band + 1).SetNoDataValue(no_data_value)\n\n        self._no_data_value[band] = no_data_value\n\n    def change_no_data_value(self, new_value: Any, old_value: Any = None):\n        \"\"\"Change No Data Value.\n\n            - Set the no data value in all raster bands.\n            - Fill the whole raster with the no_data_value.\n            - Change the no_data_value in the array in all bands.\n\n        Args:\n            new_value (numeric):\n                No data value to set in the raster bands.\n            old_value (numeric):\n                Old no data value that is already in the raster bands.\n\n        Warning:\n            The `change_no_data_value` method creates a new dataset in memory in order to change the `no_data_value` in the raster bands.\n\n        Examples:\n            - Create a Dataset (4 bands, 10 rows, 10 columns) at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.create(\n              ...     cell_size=0.05, rows=3, columns=3, bands=1, top_left_corner=(0, 0),dtype=\"float32\",\n              ...     epsg=4326, no_data_value=-9\n              ... )\n              &gt;&gt;&gt; arr = dataset.read_array()\n              &gt;&gt;&gt; print(arr)\n              [[-9. -9. -9.]\n               [-9. -9. -9.]\n               [-9. -9. -9.]]\n              &gt;&gt;&gt; print(dataset.no_data_value) # doctest: +SKIP\n              [-9.0]\n\n              ```\n\n            - The dataset is full of the no_data_value. Now change it using `change_no_data_value`:\n\n              ```python\n              &gt;&gt;&gt; new_dataset = dataset.change_no_data_value(-10, -9)\n              &gt;&gt;&gt; arr = new_dataset.read_array()\n              &gt;&gt;&gt; print(arr)\n              [[-10. -10. -10.]\n               [-10. -10. -10.]\n               [-10. -10. -10.]]\n              &gt;&gt;&gt; print(new_dataset.no_data_value) # doctest: +SKIP\n              [-10.0]\n\n              ```\n        \"\"\"\n        if not isinstance(new_value, list):\n            new_value = [new_value] * self.band_count\n\n        if old_value is not None and not isinstance(old_value, list):\n            old_value = [old_value] * self.band_count\n\n        dst = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self.raster, 0)\n        # create a new dataset\n        new_dataset = Dataset(dst, \"write\")\n        # the new_value could change inside the _set_no_data_value method before it is used to set the no_data_value\n        # attribute in the gdal object/pyramids object and to fill the band.\n        new_dataset._set_no_data_value(new_value)\n        # now we have to use the no_data_value value in the no_data_value attribute in the Dataset object as it is\n        # updated.\n        new_value = new_dataset.no_data_value\n        for band in range(self.band_count):\n            arr = self.read_array(band)\n            try:\n                if old_value is not None:\n                    arr[np.isclose(arr, old_value, rtol=0.001)] = new_value[band]\n                else:\n                    arr[np.isnan(arr)] = new_value[band]\n            except TypeError:\n                raise NoDataValueError(\n                    f\"The dtype of the given no_data_value: {new_value[band]} differs from the dtype of the \"\n                    f\"band: {gdal_to_numpy_dtype(self.gdal_dtype[band])}\"\n                )\n            new_dataset.raster.GetRasterBand(band + 1).WriteArray(arr)\n        return new_dataset\n\n    def get_cell_coords(\n        self, location: str = \"center\", mask: bool = False\n    ) -&gt; np.ndarray:\n        \"\"\"Get coordinates for the center/corner of cells inside the dataset domain.\n\n        Returns the coordinates of the cell centers inside the domain (only the cells that\n        do not have nodata value)\n\n        Args:\n            location (str):\n                Location of the coordinates. Use `center` for the center of a cell, `corner` for the corner of the\n                cell (top-left corner).\n            mask (bool):\n                True to exclude the cells out of the domain. Default is False.\n\n        Returns:\n            np.ndarray:\n                Array with a list of the coordinates to be interpolated, without the NaN.\n            np.ndarray:\n                Array with all the centers of cells in the domain of the DEM.\n\n        Examples:\n            - Create `Dataset` consists of 1 bands, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the coordinates of the center of cells inside the domain.\n\n              ```python\n              &gt;&gt;&gt; coords = dataset.get_cell_coords()\n              &gt;&gt;&gt; print(coords)\n              [[ 0.025 -0.025]\n               [ 0.075 -0.025]\n               [ 0.125 -0.025]\n               [ 0.025 -0.075]\n               [ 0.075 -0.075]\n               [ 0.125 -0.075]\n               [ 0.025 -0.125]\n               [ 0.075 -0.125]\n               [ 0.125 -0.125]]\n\n              ```\n\n            - Get the coordinates of the top left corner of cells inside the domain.\n\n              ```python\n              &gt;&gt;&gt; coords = dataset.get_cell_coords(location=\"corner\")\n              &gt;&gt;&gt; print(coords)\n              [[ 0.    0.  ]\n               [ 0.05  0.  ]\n               [ 0.1   0.  ]\n               [ 0.   -0.05]\n               [ 0.05 -0.05]\n               [ 0.1  -0.05]\n               [ 0.   -0.1 ]\n               [ 0.05 -0.1 ]\n               [ 0.1  -0.1 ]]\n\n              ```\n        \"\"\"\n        # check the location parameter\n        location = location.lower()\n        if location not in [\"center\", \"corner\"]:\n            raise ValueError(\n                \"The location parameter can have one of these values: 'center', 'corner', \"\n                f\"but the value: {location} is given.\"\n            )\n\n        if location == \"center\":\n            # Adding 0.5*cell size to get the center\n            add_value = 0.5\n        else:\n            add_value = 0\n        # Getting data for the whole grid\n        (\n            x_init,\n            cell_size_x,\n            xy_span,\n            y_init,\n            yy_span,\n            cell_size_y,\n        ) = self.geotransform\n        if cell_size_x != cell_size_y:\n            if np.abs(cell_size_x) != np.abs(cell_size_y):\n                logger.warning(\n                    f\"The given raster does not have a square cells, the cell size is {cell_size_x}*{cell_size_y} \"\n                )\n\n        # data in the array\n        no_val = self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n        arr = self.read_array(band=0)\n        if mask is not None and no_val not in arr:\n            logger.warning(\n                \"The no data value does not exist in the band, so all the cells will be considered, and the \"\n                \"mask will not be considered.\"\n            )\n\n        if mask:\n            mask = [no_val]\n        else:\n            mask = None\n        indices = get_indices2(arr, mask=mask)\n\n        # exclude the no_data_values cells.\n        f1 = [i[0] for i in indices]\n        f2 = [i[1] for i in indices]\n        x = [x_init + cell_size_x * (i + add_value) for i in f2]\n        y = [y_init + cell_size_y * (i + add_value) for i in f1]\n        coords = np.array(list(zip(x, y)))\n\n        return coords\n\n    def get_cell_polygons(self, mask: bool = False) -&gt; GeoDataFrame:\n        \"\"\"Get a polygon shapely geometry for the raster cells.\n\n        Args:\n            mask (bool):\n                True to get the polygons of the cells inside the domain.\n\n        Returns:\n            GeoDataFrame:\n                With two columns, geometry, and id.\n\n        Examples:\n            - Create `Dataset` consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the coordinates of the center of cells inside the domain.\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.get_cell_polygons()\n              &gt;&gt;&gt; print(gdf)\n                                                     geometry  id\n              0  POLYGON ((0 0, 0.05 0, 0.05 -0.05, 0 -0.05, 0 0))   0\n              1  POLYGON ((0.05 0, 0.1 0, 0.1 -0.05, 0.05 -0.05...   1\n              2  POLYGON ((0.1 0, 0.15 0, 0.15 -0.05, 0.1 -0.05...   2\n              3  POLYGON ((0 -0.05, 0.05 -0.05, 0.05 -0.1, 0 -0...   3\n              4  POLYGON ((0.05 -0.05, 0.1 -0.05, 0.1 -0.1, 0.0...   4\n              5  POLYGON ((0.1 -0.05, 0.15 -0.05, 0.15 -0.1, 0....   5\n              6  POLYGON ((0 -0.1, 0.05 -0.1, 0.05 -0.15, 0 -0....   6\n              7  POLYGON ((0.05 -0.1, 0.1 -0.1, 0.1 -0.15, 0.05...   7\n              8  POLYGON ((0.1 -0.1, 0.15 -0.1, 0.15 -0.15, 0.1...   8\n              &gt;&gt;&gt; fig, ax = dataset.plot()\n              &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='none', edgecolor=\"gray\", linewidth=2)\n              &lt;Axes: &gt;\n\n              ```\n\n        ![get_cell_polygons](./../_images/dataset/get_cell_polygons.png)\n        \"\"\"\n        coords = self.get_cell_coords(location=\"corner\", mask=mask)\n        cell_size = self.geotransform[1]\n        epsg = self._get_epsg()\n        x = np.zeros((coords.shape[0], 4))\n        y = np.zeros((coords.shape[0], 4))\n        # fill the top left corner point\n        x[:, 0] = coords[:, 0]\n        y[:, 0] = coords[:, 1]\n        # fill the top right\n        x[:, 1] = x[:, 0] + cell_size\n        y[:, 1] = y[:, 0]\n        # fill the bottom right\n        x[:, 2] = x[:, 0] + cell_size\n        y[:, 2] = y[:, 0] - cell_size\n\n        # fill the bottom left\n        x[:, 3] = x[:, 0]\n        y[:, 3] = y[:, 0] - cell_size\n\n        coords_tuples = [list(zip(x[:, i], y[:, i])) for i in range(4)]\n        polys_coords = [\n            (\n                coords_tuples[0][i],\n                coords_tuples[1][i],\n                coords_tuples[2][i],\n                coords_tuples[3][i],\n            )\n            for i in range(len(x))\n        ]\n        polygons = list(map(FeatureCollection.create_polygon, polys_coords))\n        gdf = gpd.GeoDataFrame(geometry=polygons)\n        gdf.set_crs(epsg=epsg, inplace=True)\n        gdf[\"id\"] = gdf.index\n        return gdf\n\n    def get_cell_points(self, location: str = \"center\", mask=False) -&gt; GeoDataFrame:\n        \"\"\"Get a point shapely geometry for the raster cells center point.\n\n        Args:\n            location (str):\n                Location of the point, [\"corner\", \"center\"]. Default is \"center\".\n            mask (bool):\n                True to get the polygons of the cells inside the domain.\n\n        Returns:\n            GeoDataFrame:\n                With two columns, geometry, and id.\n\n        Examples:\n            - Create `Dataset` consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Get the coordinates of the center of cells inside the domain.\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.get_cell_points()\n              &gt;&gt;&gt; print(gdf)\n                             geometry  id\n              0  POINT (0.025 -0.025)   0\n              1  POINT (0.075 -0.025)   1\n              2  POINT (0.125 -0.025)   2\n              3  POINT (0.025 -0.075)   3\n              4  POINT (0.075 -0.075)   4\n              5  POINT (0.125 -0.075)   5\n              6  POINT (0.025 -0.125)   6\n              7  POINT (0.075 -0.125)   7\n              8  POINT (0.125 -0.125)   8\n              &gt;&gt;&gt; fig, ax = dataset.plot()\n              &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=2)\n              &lt;Axes: &gt;\n\n              ```\n\n            ![get_cell_points](./../_images/dataset/get_cell_points.png)\n\n            - Get the coordinates of the top left corner of cells inside the domain.\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.get_cell_points(location=\"corner\")\n              &gt;&gt;&gt; print(gdf)\n                          geometry  id\n              0         POINT (0 0)   0\n              1      POINT (0.05 0)   1\n              2       POINT (0.1 0)   2\n              3     POINT (0 -0.05)   3\n              4  POINT (0.05 -0.05)   4\n              5   POINT (0.1 -0.05)   5\n              6      POINT (0 -0.1)   6\n              7   POINT (0.05 -0.1)   7\n              8    POINT (0.1 -0.1)   8\n              &gt;&gt;&gt; fig, ax = dataset.plot()\n              &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=4)\n              &lt;Axes: &gt;\n\n              ```\n\n            ![get_cell_points-corner](./../_images/dataset/get_cell_points-corner.png)\n        \"\"\"\n        coords = self.get_cell_coords(location=location, mask=mask)\n        epsg = self._get_epsg()\n\n        coords_tuples = list(zip(coords[:, 0], coords[:, 1]))\n        points = FeatureCollection.create_point(coords_tuples)\n        gdf = gpd.GeoDataFrame(geometry=points)\n        gdf.set_crs(epsg=epsg, inplace=True)\n        gdf[\"id\"] = gdf.index\n        return gdf\n\n    def to_file(self, path: str, band: int = 0, tile_length: int = None) -&gt; None:\n        \"\"\"Save dataset to tiff file.\n\n            `to_file` saves a raster to disk, the type of the driver (georiff/netcdf/ascii) will be implied from the\n            extension at the end of the given path.\n\n        Args:\n            path (str):\n                A path including the name of the dataset.\n            band (int):\n                Band index, needed only in case of ascii drivers. Default is 0.\n            tile_length (int, optional):\n                Length of the tiles in the driver. Default is 256.\n\n        Examples:\n            - Create a Dataset with 4 bands, 5 rows, 5 columns, at the point lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset.file_name)\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - Now save the dataset as a geotiff file:\n\n              ```python\n              &gt;&gt;&gt; dataset.to_file(\"my-dataset.tif\")\n              &gt;&gt;&gt; print(dataset.file_name)\n              my-dataset.tif\n\n              ```\n        \"\"\"\n        if not isinstance(path, str):\n            raise TypeError(\"path input should be string type\")\n\n        extension = path.split(\".\")[-1]\n        driver = CATALOG.get_driver_name_by_extension(extension)\n        driver_name = CATALOG.get_gdal_name(driver)\n\n        if driver == \"ascii\":\n            arr = self.read_array(band=band)\n            no_data_value = self.no_data_value[band]\n            xmin, ymin, _, _ = self.bbox\n            _io.to_ascii(arr, self.cell_size, xmin, ymin, no_data_value, path)\n        else:\n            # saving rasters with color table fails with a runtime error\n            options = [\"COMPRESS=DEFLATE\"]\n            if tile_length is not None:\n                options += [\n                    \"TILED=YES\",\n                    f\"TILE_LENGTH={tile_length}\",\n                ]\n            if self._block_size is not None and self._block_size != []:\n                options += [\n                    \"BLOCKXSIZE={}\".format(self._block_size[0][0]),\n                    \"BLOCKYSIZE={}\".format(self._block_size[0][1]),\n                ]\n\n            try:\n                dst = gdal.GetDriverByName(driver_name).CreateCopy(\n                    path, self.raster, 0, options=options\n                )\n                self.__init__(dst, \"write\")\n                # flush the data to the dataset on disk.\n                dst.FlushCache()\n            except RuntimeError:\n                if not os.path.exists(path):\n                    raise FailedToSaveError(\n                        f\"Failed to save the {driver_name} raster to the path: {path}\"\n                    )\n\n    def convert_longitude(self, inplace: bool = False) -&gt; Optional[\"Dataset\"]:\n        \"\"\"Convert Longitude.\n\n        - convert the longitude from 0-360 to -180 - 180.\n        - currently the function works correctly if the raster covers the whole world, it means that the columns\n            in the rasters covers from longitude 0 to 360.\n\n        Args:\n            inplace (bool):\n                True to make the changes in place.\n\n        Returns:\n            Dataset:\n                The converted dataset if inplace is False; otherwise None.\n        \"\"\"\n        # dst = gdal.Warp(\n        #     \"\",\n        #     self.raster,\n        #     dstSRS=\"+proj=longlat +ellps=WGS84 +datum=WGS84 +lon_0=0 +over\",\n        #     format=\"VRT\",\n        # )\n        lon = self.lon\n        src = self.raster\n        # create a copy\n        drv = gdal.GetDriverByName(\"MEM\")\n        dst = drv.CreateCopy(\"\", src, 0)\n        # convert the 0 to 360 to -180 to 180\n        if lon[-1] &lt;= 180:\n            raise ValueError(\"The raster should cover the whole globe\")\n\n        first_to_translated = np.where(lon &gt; 180)[0][0]\n\n        ind = list(range(first_to_translated, len(lon)))\n        ind_2 = list(range(0, first_to_translated))\n\n        for band in range(self.band_count):\n            arr = self.read_array(band=band)\n            arr_rearranged = arr[:, ind + ind_2]\n            dst.GetRasterBand(band + 1).WriteArray(arr_rearranged)\n\n        # correct the geotransform\n        top_left_corner = self.top_left_corner\n        gt = list(self.geotransform)\n        if lon[-1] &gt; 180:\n            new_gt = top_left_corner[0] - 180\n            gt[0] = new_gt\n\n        dst.SetGeoTransform(gt)\n        if not inplace:\n            return Dataset(dst)\n        else:\n            self.__init__(dst)\n\n    def _band_to_polygon(self, band: int, col_name: str):\n        band = self.raster.GetRasterBand(band + 1)\n        srs = osr.SpatialReference(wkt=self.crs)\n\n        dst_ds = FeatureCollection.create_ds(\"memory\")\n        dst_layer = dst_ds.CreateLayer(col_name, srs=srs)\n        dtype = gdal_to_ogr_dtype(self.raster)\n        new_field = ogr.FieldDefn(col_name, dtype)\n        dst_layer.CreateField(new_field)\n        gdal.Polygonize(band, band, dst_layer, 0, [], callback=None)\n\n        vector = FeatureCollection(dst_ds)\n        gdf = vector._ds_to_gdf()\n        return gdf\n\n    def to_feature_collection(\n        self,\n        vector_mask: GeoDataFrame = None,\n        add_geometry: str = None,\n        tile: bool = False,\n        tile_size: int = 256,\n        touch: bool = True,\n    ) -&gt; Union[DataFrame, GeoDataFrame]:\n        \"\"\"Convert a dataset to a vector.\n\n        The function does the following:\n            - Flatten the array in each band in the raster then mask the values if a vector_mask file is given\n                otherwise it will flatten all values.\n            - Put the values for each band in a column in a dataframe under the name of the raster band,\n                but if no meta-data in the raster band exists, an index number will be used [1, 2, 3, ...]\n            - The function has an add_geometry parameter with two possible values [\"point\", \"polygon\"], which you can\n                specify the type of shapely geometry you want to create from each cell,\n\n                - If point is chosen, the created point will be at the center of each cell\n                - If a polygon is chosen, a square polygon will be created that covers the entire cell.\n\n        Args:\n            vector_mask (GeoDataFrame, optional):\n                GeoDataFrame for the vector_mask. If given, it will be used to clip the raster.\n            add_geometry (str):\n                \"Polygon\" or \"Point\" if you want to add a polygon geometry of the cells as column in dataframe.\n                Default is None.\n            tile (bool):\n                True to use tiles in extracting the values from the raster. Default is False.\n            tile_size (int):\n                Tile size. Default is 1500.\n            touch (bool):\n                Include the cells that touch the polygon not only those that lie entirely inside the polygon mask.\n                Default is True.\n\n        Returns:\n            DataFrame | GeoDataFrame:\n                The resulting frame will have the band value under the name of the band (if the raster file has\n                metadata; if not, the bands will be indexed from 1 to the number of bands).\n\n        Examples:\n            - Create a dataset from array with 2 bands and 3*3 array each:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(2, 3, 3)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset.read_array(band=0)) # doctest: +SKIP\n              [[0.88625832 0.81804328 0.99372706]\n               [0.85333054 0.35448201 0.78079262]\n               [0.43887136 0.68166208 0.53170966]]\n              &gt;&gt;&gt; print(dataset.read_array(band=1)) # doctest: +SKIP\n              [[0.07051872 0.67650833 0.17625027]\n               [0.41258071 0.38327938 0.18783139]\n               [0.83741314 0.70446373 0.64913575]]\n\n              ```\n\n            - Convert the dataset to dataframe by calling the `to_feature_collection` method:\n\n              ```python\n              &gt;&gt;&gt; df = dataset.to_feature_collection()\n              &gt;&gt;&gt; print(df) # doctest: +SKIP\n                   Band_1    Band_2\n              0  0.886258  0.070519\n              1  0.818043  0.676508\n              2  0.993727  0.176250\n              3  0.853331  0.412581\n              4  0.354482  0.383279\n              5  0.780793  0.187831\n              6  0.438871  0.837413\n              7  0.681662  0.704464\n              8  0.531710  0.649136\n\n              ```\n\n            - Convert the dataset into geodataframe with either a polygon or a point geometry that represents each cell.\n                To specify the geometry type use the parameter `add_geometry`:\n\n                  ```python\n                  &gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"point\")\n                  &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                       Band_1    Band_2                  geometry\n                  0  0.886258  0.070519  POINT (0.02500 -0.02500)\n                  1  0.818043  0.676508  POINT (0.07500 -0.02500)\n                  2  0.993727  0.176250  POINT (0.12500 -0.02500)\n                  3  0.853331  0.412581  POINT (0.02500 -0.07500)\n                  4  0.354482  0.383279  POINT (0.07500 -0.07500)\n                  5  0.780793  0.187831  POINT (0.12500 -0.07500)\n                  6  0.438871  0.837413  POINT (0.02500 -0.12500)\n                  7  0.681662  0.704464  POINT (0.07500 -0.12500)\n                  8  0.531710  0.649136  POINT (0.12500 -0.12500)\n                  &gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"polygon\")\n                  &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                       Band_1    Band_2                                           geometry\n                  0  0.886258  0.070519  POLYGON ((0.00000 0.00000, 0.05000 0.00000, 0....\n                  1  0.818043  0.676508  POLYGON ((0.05000 0.00000, 0.10000 0.00000, 0....\n                  2  0.993727  0.176250  POLYGON ((0.10000 0.00000, 0.15000 0.00000, 0....\n                  3  0.853331  0.412581  POLYGON ((0.00000 -0.05000, 0.05000 -0.05000, ...\n                  4  0.354482  0.383279  POLYGON ((0.05000 -0.05000, 0.10000 -0.05000, ...\n                  5  0.780793  0.187831  POLYGON ((0.10000 -0.05000, 0.15000 -0.05000, ...\n                  6  0.438871  0.837413  POLYGON ((0.00000 -0.10000, 0.05000 -0.10000, ...\n                  7  0.681662  0.704464  POLYGON ((0.05000 -0.10000, 0.10000 -0.10000, ...\n                  8  0.531710  0.649136  POLYGON ((0.10000 -0.10000, 0.15000 -0.10000, ...\n\n                  ```\n\n            - Use a mask to crop part of the dataset, and then convert the cropped part to a dataframe/geodataframe:\n\n              - Create a mask that covers only the cell in the middle of the dataset.\n\n                  ```python\n                  &gt;&gt;&gt; import geopandas as gpd\n                  &gt;&gt;&gt; from shapely.geometry import Polygon\n                  &gt;&gt;&gt; poly = gpd.GeoDataFrame(\n                  ...             geometry=[Polygon([(0.05, -0.05), (0.05, -0.1), (0.1, -0.1), (0.1, -0.05)])], crs=4326\n                  ... )\n                  &gt;&gt;&gt; df = dataset.to_feature_collection(vector_mask=poly)\n                  &gt;&gt;&gt; print(df) # doctest: +SKIP\n                       Band_1    Band_2\n                  0  0.354482  0.383279\n\n                  ```\n\n            - If you have a big dataset, and you want to convert it to dataframe in tiles (do not read the whole dataset\n                at once but in tiles), you can use the `tile` and the `tile_size` parameters. The values will be the\n                same as above; the difference is reading in chunks:\n\n                  ```python\n                  &gt;&gt;&gt; gdf = dataset.to_feature_collection(tile=True, tile_size=1)\n                  &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                       Band_1    Band_2\n                  0  0.886258  0.070519\n                  1  0.818043  0.676508\n                  2  0.993727  0.176250\n                  3  0.853331  0.412581\n                  4  0.354482  0.383279\n                  5  0.780793  0.187831\n                  6  0.438871  0.837413\n                  7  0.681662  0.704464\n                  8  0.531710  0.649136\n\n                  ```\n\n        \"\"\"\n        # Get raster band names. open the dataset using gdal.Open\n        band_names = self.band_names\n\n        # Create a mask from the pixels touched by the vector_mask.\n        if vector_mask is not None:\n            src = self.crop(mask=vector_mask, touch=touch)\n        else:\n            src = self\n\n        if tile:\n            df_list = []  # DataFrames of each tile.\n            for arr in self.get_tile(tile_size):\n                # Assume multi-band\n                idx = (1, 2)\n                if arr.ndim == 2:\n                    # Handle single band rasters\n                    idx = (0, 1)\n\n                mask_arr = np.ones((arr.shape[idx[0]], arr.shape[idx[1]]))\n                pixels = get_pixels(arr, mask_arr).transpose()\n                df_list.append(pd.DataFrame(pixels, columns=band_names))\n\n            # Merge all the tiles.\n            df = pd.concat(df_list)\n        else:\n            arr = src.read_array()\n\n            if self.band_count == 1:\n                pixels = arr.flatten()\n            else:\n                pixels = (\n                    arr.flatten()\n                    .reshape(src.band_count, src.columns * src.rows)\n                    .transpose()\n                )\n            df = pd.DataFrame(pixels, columns=band_names)\n            # mask no data values.\n            if src.no_data_value[0] is not None:\n                df.replace(src.no_data_value[0], np.nan, inplace=True)\n            df.dropna(axis=0, inplace=True, ignore_index=True)\n\n        if add_geometry:\n            if add_geometry.lower() == \"point\":\n                coords = src.get_cell_points(mask=True)\n            else:\n                coords = src.get_cell_polygons(mask=True)\n\n        df.drop(columns=[\"burn_value\", \"geometry\"], errors=\"ignore\", inplace=True)\n        if add_geometry:\n            df = gpd.GeoDataFrame(df.loc[:], geometry=coords[\"geometry\"].to_list())\n            df.set_crs(coords.crs.to_epsg())\n\n        return df\n\n    def apply(self, func, band: int = 0) -&gt; \"Dataset\":\n        \"\"\"Apply a function to all domain cells.\n\n        - apply method executes a mathematical operation on the raster array.\n        - The apply method executes the function only on one cell at a time.\n\n        Args:\n            func (function):\n                Defined function that takes one input (the cell value).\n            band (int):\n                Band number.\n\n        Returns:\n            Dataset:\n                Dataset object.\n\n        Examples:\n            - Create a dataset from an array filled with values between -1 and 1:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.uniform(-1, 1, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n              [[ 0.94997539 -0.80083622 -0.30948769 -0.77439961 -0.83836424]\n               [-0.36810158 -0.23979251  0.88051216 -0.46882913  0.64511056]\n               [ 0.50585374 -0.46905902  0.67856589  0.2779605   0.05589759]\n               [ 0.63382852 -0.49259597  0.18471423 -0.49308984 -0.52840286]\n               [-0.34076174 -0.53073014 -0.18485789 -0.40033474 -0.38962938]]\n\n              ```\n\n            - Apply the absolute function to the dataset:\n\n              ```python\n              &gt;&gt;&gt; abs_dataset = dataset.apply(np.abs)\n              &gt;&gt;&gt; print(abs_dataset.read_array()) # doctest: +SKIP\n              [[0.94997539 0.80083622 0.30948769 0.77439961 0.83836424]\n               [0.36810158 0.23979251 0.88051216 0.46882913 0.64511056]\n               [0.50585374 0.46905902 0.67856589 0.2779605  0.05589759]\n               [0.63382852 0.49259597 0.18471423 0.49308984 0.52840286]\n               [0.34076174 0.53073014 0.18485789 0.40033474 0.38962938]]\n\n              ```\n        \"\"\"\n        if not callable(func):\n            raise TypeError(\"The second argument should be a function\")\n\n        no_data_value = self.no_data_value[band]\n        src_array = self.read_array(band)\n        dtype = self.gdal_dtype[band]\n\n        # fill the new array with the nodata value\n        new_array = np.ones((self.rows, self.columns)) * no_data_value\n        # execute the function on each cell\n        # TODO: optimize executing a function over a whole array\n        for i in range(self.rows):\n            for j in range(self.columns):\n                if not np.isclose(src_array[i, j], no_data_value, rtol=0.001):\n                    new_array[i, j] = func(src_array[i, j])\n\n        # create the output raster\n        dst = Dataset._create_dataset(self.columns, self.rows, 1, dtype, driver=\"MEM\")\n        # set the geotransform\n        dst.SetGeoTransform(self.geotransform)\n        # set the projection\n        dst.SetProjection(self.crs)\n        dst_obj = Dataset(dst)\n        dst_obj._set_no_data_value(no_data_value=no_data_value)\n        dst_obj.raster.GetRasterBand(band + 1).WriteArray(new_array)\n\n        return dst_obj\n\n    def fill(\n        self, value: Union[float, int], inplace: bool = False, path: str = None\n    ) -&gt; Union[\"Dataset\", None]:\n        \"\"\"Fill the domain cells with a certain value.\n\n            Fill takes a raster and fills it with one value\n\n        Args:\n            value (float | int):\n                Numeric value to fill.\n            inplace (bool):\n                If True, the original dataset will be modified. If False, a new dataset will be created. Default is False.\n            path (str):\n                Path including the extension (.tif).\n\n        Returns:\n            Dataset:\n                The resulting dataset if inplace is False; otherwise None.\n\n        Examples:\n            - Create a Dataset with 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n              [[1 1 3 1 2]\n               [2 2 2 1 2]\n               [2 2 3 1 3]\n               [3 4 3 3 4]\n               [4 4 2 1 1]]\n              &gt;&gt;&gt; new_dataset = dataset.fill(10)\n              &gt;&gt;&gt; print(new_dataset.read_array())\n              [[10 10 10 10 10]\n               [10 10 10 10 10]\n               [10 10 10 10 10]\n               [10 10 10 10 10]\n               [10 10 10 10 10]]\n\n              ```\n        \"\"\"\n        no_data_value = self.no_data_value[0]\n        src_array = self.raster.ReadAsArray()\n\n        if no_data_value is None:\n            no_data_value = np.nan\n\n        if not np.isnan(no_data_value):\n            src_array[~np.isclose(src_array, no_data_value, rtol=0.000001)] = value\n        else:\n            src_array[~np.isnan(src_array)] = value\n\n        dst = Dataset.dataset_like(self, src_array, path=path)\n        if inplace:\n            self.__init__(dst.raster)\n        else:\n            return dst\n\n    def resample(\n        self, cell_size: Union[int, float], method: str = \"nearest neighbor\"\n    ) -&gt; \"Dataset\":\n        \"\"\"resample.\n\n        resample method reprojects a raster to any projection (default the WGS84 web mercator projection,\n        without resampling). The function returns a GDAL in-memory file object.\n\n        Args:\n            cell_size (int):\n                New cell size to resample the raster. If None, raster will not be resampled.\n            method (str):\n                Resampling method: \"nearest neighbor\", \"cubic\", or \"bilinear\". Default is \"nearest neighbor\".\n\n        Returns:\n            Dataset:\n                Dataset object.\n\n        Examples:\n            - Create a Dataset with 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File: ...\n              &lt;BLANKLINE&gt;\n              &gt;&gt;&gt; dataset.plot(band=0)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n              ![resample-source](./../_images/dataset/resample-source.png)\n\n            - Resample the raster to a new cell size of 0.1:\n\n              ```python\n              &gt;&gt;&gt; new_dataset = dataset.resample(cell_size=0.1)\n              &gt;&gt;&gt; print(new_dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.1\n                          Dimension: 5 * 5\n                          EPSG: 4326\n                          Number of Bands: 4\n                          Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n              &gt;&gt;&gt; new_dataset.plot(band=0)\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n              ![resample-new](./../_images/dataset/resample-new.png)\n\n            - Resampling the dataset from cell_size 0.05 to 0.1 degrees reduced the number of cells to 5 in each dimension instead of 10.\n        \"\"\"\n        if not isinstance(method, str):\n            raise TypeError(\n                \"Please enter a correct method, for more information, see documentation\"\n            )\n        if method not in INTERPOLATION_METHODS.keys():\n            raise ValueError(\n                f\"The given interpolation method does not exist, existing methods are {INTERPOLATION_METHODS.keys()}\"\n            )\n\n        method = INTERPOLATION_METHODS.get(method)\n\n        sr_src = osr.SpatialReference(wkt=self.crs)\n\n        ulx = self.geotransform[0]\n        uly = self.geotransform[3]\n        # transform the right lower corner point\n        lrx = self.geotransform[0] + self.geotransform[1] * self.columns\n        lry = self.geotransform[3] + self.geotransform[5] * self.rows\n\n        # new geotransform\n        new_geo = (\n            self.geotransform[0],\n            cell_size,\n            self.geotransform[2],\n            self.geotransform[3],\n            self.geotransform[4],\n            -1 * cell_size,\n        )\n        # create a new raster\n        cols = int(np.round(abs(lrx - ulx) / cell_size))\n        rows = int(np.round(abs(uly - lry) / cell_size))\n        dtype = self.gdal_dtype[0]\n        bands = self.band_count\n\n        dst = Dataset._create_dataset(cols, rows, bands, dtype)\n        # set the geotransform\n        dst.SetGeoTransform(new_geo)\n        # set the projection\n        dst.SetProjection(sr_src.ExportToWkt())\n        dst_obj = Dataset(dst, \"write\")\n        # set the no data value\n        dst_obj._set_no_data_value(self.no_data_value)\n        # perform the projection &amp; resampling\n        gdal.ReprojectImage(\n            self.raster,\n            dst_obj.raster,\n            sr_src.ExportToWkt(),\n            sr_src.ExportToWkt(),\n            method,\n        )\n\n        return dst_obj\n\n    def _reproject_with_ReprojectImage(\n        self, to_epsg: int, method: str = \"nearest neighbor\"\n    ) -&gt; \"Dataset\":\n        src_gt = self.geotransform\n        src_x = self.columns\n        src_y = self.rows\n\n        src_sr = osr.SpatialReference(wkt=self.crs)\n        src_epsg = self.epsg\n\n        dst_sr = self._create_sr_from_epsg(to_epsg)\n\n        # in case the source crs is GCS and longitude is in the west hemisphere, gdal\n        # reads longitude from 0 to 360 and a transformation factor wont work with values\n        # greater than 180\n        if src_epsg != to_epsg:\n            if src_epsg == \"4326\" and src_gt[0] &gt; 180:\n                lng_new = src_gt[0] - 360\n                # transformation factors\n                tx = osr.CoordinateTransformation(src_sr, dst_sr)\n                # transform the right upper corner point\n                (ulx, uly, ulz) = tx.TransformPoint(lng_new, src_gt[3])\n                # transform the right lower corner point\n                (lrx, lry, lrz) = tx.TransformPoint(\n                    lng_new + src_gt[1] * src_x, src_gt[3] + src_gt[5] * src_y\n                )\n            else:\n                xs = [src_gt[0], src_gt[0] + src_gt[1] * src_x]\n                ys = [src_gt[3], src_gt[3] + src_gt[5] * src_y]\n\n                [uly, lry], [ulx, lrx] = FeatureCollection.reproject_points(\n                    ys, xs, from_epsg=src_epsg, to_epsg=to_epsg\n                )\n                # old transform\n                # # transform the right upper corner point\n                # (ulx, uly, ulz) = tx.TransformPoint(src_gt[0], src_gt[3])\n                # # transform the right lower corner point\n                # (lrx, lry, lrz) = tx.TransformPoint(\n                #     src_gt[0] + src_gt[1] * src_x, src_gt[3] + src_gt[5] * src_y\n                # )\n\n        else:\n            ulx = src_gt[0]\n            uly = src_gt[3]\n            lrx = src_gt[0] + src_gt[1] * src_x\n            lry = src_gt[3] + src_gt[5] * src_y\n\n        # get the cell size in the source raster and convert it to the new crs\n        # x coordinates or longitudes\n        xs = [src_gt[0], src_gt[0] + src_gt[1]]\n        # y coordinates or latitudes\n        ys = [src_gt[3], src_gt[3]]\n\n        if src_epsg != to_epsg:\n            # transform the two-point coordinates to the new crs to calculate the new cell size\n            new_ys, new_xs = FeatureCollection.reproject_points(\n                ys, xs, from_epsg=src_epsg, to_epsg=to_epsg, precision=6\n            )\n        else:\n            new_xs = xs\n            # new_ys = ys\n\n        # TODO: the function does not always maintain alignment, based on the conversion of the cell_size and the\n        # pivot point\n        pixel_spacing = np.abs(new_xs[0] - new_xs[1])\n\n        # create a new raster\n        cols = int(np.round(abs(lrx - ulx) / pixel_spacing))\n        rows = int(np.round(abs(uly - lry) / pixel_spacing))\n\n        dtype = self.gdal_dtype[0]\n        dst = Dataset._create_dataset(cols, rows, self.band_count, dtype)\n\n        # new geotransform\n        new_geo = (\n            ulx,\n            pixel_spacing,\n            src_gt[2],\n            uly,\n            src_gt[4],\n            np.sign(src_gt[-1]) * pixel_spacing,\n        )\n        # set the geotransform\n        dst.SetGeoTransform(new_geo)\n        # set the projection\n        dst.SetProjection(dst_sr.ExportToWkt())\n        # set the no data value\n        dst_obj = Dataset(dst)\n        dst_obj._set_no_data_value(self.no_data_value)\n        # perform the projection &amp; resampling\n        gdal.ReprojectImage(\n            self.raster,\n            dst_obj.raster,\n            src_sr.ExportToWkt(),\n            dst_sr.ExportToWkt(),\n            method,\n        )\n        return dst_obj\n\n    def fill_gaps(self, mask, src_array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Fill gaps in src_array using nearest neighbors where mask indicates valid cells.\n\n        Args:\n            mask (Dataset | np.ndarray):\n                Mask dataset or array used to determine valid cells.\n            src_array (np.ndarray):\n                Source array whose gaps will be filled.\n\n        Returns:\n            np.ndarray: The source array with gaps filled where applicable.\n        \"\"\"\n        # align function only equate the no of rows and columns only\n        # match no_data_value inserts no_data_value in src raster to all places like mask\n        # still places that has no_data_value in the src raster, but it is not no_data_value in the mask\n        # and now has to be filled with values\n        # compare no of element that is not no_data_value in both rasters to make sure they are matched\n        # if both inputs are rasters\n        mask_array = mask.read_array()\n        row = mask.rows\n        col = mask.columns\n        mask_noval = mask.no_data_value[0]\n\n        if isinstance(mask, Dataset) and isinstance(self, Dataset):\n            # there might be cells that are out of domain in the src but not out of domain in the mask\n            # so change all the src_noval to mask_noval in the src_array\n            # src_array[np.isclose(src_array, self.no_data_value[0], rtol=0.001)] = mask_noval\n            # then count them (out of domain cells) in the src_array\n            elem_src = src_array.size - np.count_nonzero(\n                (src_array[np.isclose(src_array, self.no_data_value[0], rtol=0.001)])\n            )\n            # count the out of domain cells in the mask\n            elem_mask = mask_array.size - np.count_nonzero(\n                (mask_array[np.isclose(mask_array, mask_noval, rtol=0.001)])\n            )\n\n            # if not equal, then store indices of those cells that don't match\n            if elem_mask &gt; elem_src:\n                rows = [\n                    i\n                    for i in range(row)\n                    for j in range(col)\n                    if np.isclose(src_array[i, j], self.no_data_value[0], rtol=0.001)\n                    and not np.isclose(mask_array[i, j], mask_noval, rtol=0.001)\n                ]\n                cols = [\n                    j\n                    for i in range(row)\n                    for j in range(col)\n                    if np.isclose(src_array[i, j], self.no_data_value[0], rtol=0.001)\n                    and not np.isclose(mask_array[i, j], mask_noval, rtol=0.001)\n                ]\n            # interpolate those missing cells by the nearest neighbor\n            if elem_mask &gt; elem_src:\n                src_array = Dataset._nearest_neighbour(\n                    src_array, self.no_data_value[0], rows, cols\n                )\n            return src_array\n\n    def _crop_aligned(\n        self,\n        mask: Union[gdal.Dataset, np.ndarray],\n        mask_noval: Union[int, float] = None,\n        fill_gaps: bool = False,\n    ) -&gt; \"Dataset\":\n        \"\"\"Clip/crop by matching the nodata layout from mask to the source raster.\n\n        Both rasters must have the same dimensions (rows and columns). Use MatchRasterAlignment prior to this\n        method to align both rasters.\n\n        Args:\n            mask (Dataset | np.ndarray):\n                Mask raster to get the location of the NoDataValue and where it is in the array.\n            mask_noval (int | float, optional):\n                In case the mask is a numpy array, the mask_noval has to be given.\n            fill_gaps (bool):\n                Whether to fill gaps after cropping. Default is False.\n\n        Returns:\n            Dataset:\n                The raster with NoDataValue stored in its cells exactly the same as the source raster.\n        \"\"\"\n        if isinstance(mask, Dataset):\n            mask_gt = mask.geotransform\n            mask_epsg = mask.epsg\n            row = mask.rows\n            col = mask.columns\n            mask_noval = mask.no_data_value[0]\n            mask_array = mask.read_array(band=0)\n        elif isinstance(mask, np.ndarray):\n            if mask_noval is None:\n                raise ValueError(\n                    \"You have to enter the value of the no_val parameter when the mask is a numpy array\"\n                )\n            mask_array = mask.copy()\n            row, col = mask.shape\n        else:\n            raise TypeError(\n                \"The second parameter 'mask' has to be either gdal.Datacube or numpy array\"\n                f\"given - {type(mask)}\"\n            )\n\n        band_count = self.band_count\n        src_sref = osr.SpatialReference(wkt=self.crs)\n        src_array = self.read_array()\n\n        if not row == self.rows or not col == self.columns:\n            raise ValueError(\n                \"Two rasters have different number of columns or rows, please resample or match both rasters\"\n            )\n\n        if isinstance(mask, Dataset):\n            if (\n                not self.top_left_corner == mask.top_left_corner\n                or not self.cell_size == mask.cell_size\n            ):\n                raise ValueError(\n                    \"the location of the upper left corner of both rasters is not the same or cell size is \"\n                    \"different please match both rasters first \"\n                )\n\n            if not mask_epsg == self.epsg:\n                raise ValueError(\n                    \"Dataset A &amp; B are using different coordinate systems please reproject one of them to \"\n                    \"the other raster coordinate system\"\n                )\n\n        if band_count &gt; 1:\n            # check if the no data value for the src complies with the dtype of the src as sometimes the band is full\n            # of values and the no_data_value is not used at all in the band, and when we try to replace any value in\n            # the array with the no_data_value it will raise an error.\n            no_data_value = self._check_no_data_value(self.no_data_value)\n\n            for band in range(self.band_count):\n                if mask_noval is None:\n                    src_array[band, np.isnan(mask_array)] = self.no_data_value[band]\n                else:\n                    src_array[band, np.isclose(mask_array, mask_noval, rtol=0.001)] = (\n                        no_data_value[band]\n                    )\n        else:\n            if mask_noval is None:\n                src_array[np.isnan(mask_array)] = self.no_data_value[0]\n            else:\n                src_array[np.isclose(mask_array, mask_noval, rtol=0.001)] = (\n                    self.no_data_value[0]\n                )\n\n        if fill_gaps:\n            src_array = self.fill_gaps(mask, src_array)\n\n        dst = Dataset._create_dataset(\n            col, row, band_count, self.gdal_dtype[0], driver=\"MEM\"\n        )\n        # but with a lot of computations,\n        # if the mask is an array and the mask_gt is not defined, use the src_gt as both the mask and the src\n        # are aligned, so they have the sam gt\n        try:\n            # set the geotransform\n            dst.SetGeoTransform(mask_gt)\n            # set the projection\n            dst.SetProjection(mask.crs)\n        except UnboundLocalError:\n            dst.SetGeoTransform(self.geotransform)\n            dst.SetProjection(src_sref.ExportToWkt())\n\n        dst_obj = Dataset(dst)\n        # set the no data value\n        dst_obj._set_no_data_value(self.no_data_value)\n        if band_count &gt; 1:\n            for band in range(band_count):\n                dst_obj.raster.GetRasterBand(band + 1).WriteArray(src_array[band, :, :])\n        else:\n            dst_obj.raster.GetRasterBand(1).WriteArray(src_array)\n        return dst_obj\n\n    def _check_alignment(self, mask) -&gt; bool:\n        \"\"\"Check if raster is aligned with a given mask raster.\"\"\"\n        if not isinstance(mask, Dataset):\n            raise TypeError(\"The second parameter should be a Dataset\")\n\n        return self.rows == mask.rows and self.columns == mask.columns\n\n    def align(\n        self,\n        alignment_src: \"Dataset\",\n    ) -&gt; \"Dataset\":\n        \"\"\"Align the current dataset (rows and columns) to match a given dataset.\n\n        Copies spatial properties from alignment_src to the current raster:\n            - The coordinate system\n            - The number of rows and columns\n            - Cell size\n        Then resamples values from the current dataset using the nearest neighbor interpolation.\n\n        Args:\n            alignment_src (Dataset):\n                Spatial information source raster to get the spatial information (coordinate system, number of rows and\n                columns). The data values of the current dataset are resampled to this alignment.\n\n        Returns:\n            Dataset: The aligned dataset.\n\n        Examples:\n            - The source dataset has a `top_left_corner` at (0, 0) with a 5*5 alignment, and a 0.05 degree cell size.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 5 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            - The dataset to be aligned has a top_left_corner at (-0.1, 0.1) (i.e., it has two more rows on top of the\n              dataset, and two columns on the left of the dataset).\n\n              ```python\n              &gt;&gt;&gt; arr = np.random.rand(10, 10)\n              &gt;&gt;&gt; top_left_corner = (-0.1, 0.1)\n              &gt;&gt;&gt; cell_size = 0.07\n              &gt;&gt;&gt; dataset_target = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,\n              ... epsg=4326)\n              &gt;&gt;&gt; print(dataset_target)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.07\n                          Dimension: 10 * 10\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            ![align-source-target](./../_images/dataset/align-source-target.png)\n\n            - Now call the `align` method and use the dataset as the alignment source.\n\n              ```python\n              &gt;&gt;&gt; aligned_dataset = dataset_target.align(dataset)\n              &gt;&gt;&gt; print(aligned_dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 5 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              ```\n\n            ![align-result](./../_images/dataset/align-result.png)\n        \"\"\"\n        if isinstance(alignment_src, Dataset):\n            src = alignment_src\n        else:\n            raise TypeError(\n                \"First parameter should be a Dataset read using Dataset.openRaster or a path to the raster, \"\n                f\"given {type(alignment_src)}\"\n            )\n\n        # reproject the raster to match the projection of alignment_src\n        if not self.epsg == src.epsg:\n            reprojected_RasterB = self.to_crs(src.epsg)\n        else:\n            reprojected_RasterB = self\n        # create a new raster\n        dst = Dataset._create_dataset(\n            src.columns, src.rows, self.band_count, src.gdal_dtype[0], driver=\"MEM\"\n        )\n        # set the geotransform\n        dst.SetGeoTransform(src.geotransform)\n        # set the projection\n        dst.SetProjection(src.crs)\n        # set the no data value\n        dst_obj = Dataset(dst)\n        dst_obj._set_no_data_value(self.no_data_value)\n        # perform the projection &amp; resampling\n        method = gdal.GRA_NearestNeighbour\n        # resample the reprojected_RasterB\n        gdal.ReprojectImage(\n            reprojected_RasterB.raster,\n            dst_obj.raster,\n            src.crs,\n            src.crs,\n            method,\n        )\n\n        return dst_obj\n\n    def _crop_with_raster(\n        self,\n        mask: Union[gdal.Dataset, str],\n    ) -&gt; \"Dataset\":\n        \"\"\"Crop this raster using another raster as a mask.\n\n        Args:\n            mask (Dataset | str):\n                The raster you want to use as a mask to crop this raster; it can be a path or a GDAL Dataset.\n\n        Returns:\n            Dataset:\n                The cropped raster.\n        \"\"\"\n        # get information from the mask raster\n        if isinstance(mask, str):\n            mask = Dataset.read_file(mask)\n        elif isinstance(mask, Dataset):\n            mask = mask\n        else:\n            raise TypeError(\n                \"The second parameter has to be either path to the mask raster or a gdal.Datacube object\"\n            )\n        if not self._check_alignment(mask):\n            # first align the mask with the src raster\n            mask = mask.align(self)\n        # crop the src raster with the aligned mask\n        dst_obj = self._crop_aligned(mask)\n\n        dst_obj = Dataset.correct_wrap_cutline_error(dst_obj)\n        return dst_obj\n\n    def _crop_with_polygon_warp(\n        self, feature: Union[FeatureCollection, GeoDataFrame], touch: bool = True\n    ) -&gt; \"Dataset\":\n        \"\"\"Crop raster with polygon.\n\n            - Do not convert the polygon into a raster but rather use it directly to crop the raster using the\n            gdal.warp function.\n\n        Args:\n            feature (FeatureCollection | GeoDataFrame):\n                Vector mask.\n            touch (bool):\n                Include cells that touch the polygon, not only those entirely inside the polygon mask. Defaults to True.\n\n        Returns:\n            Dataset:\n                Cropped dataset.\n        \"\"\"\n        if isinstance(feature, GeoDataFrame):\n            feature = FeatureCollection(feature)\n        else:\n            if not isinstance(feature, FeatureCollection):\n                raise TypeError(\n                    f\"The function takes only a FeatureCollection or GeoDataFrame, given {type(feature)}\"\n                )\n\n        feature = feature._gdf_to_ds()\n        warp_options = gdal.WarpOptions(\n            format=\"VRT\",\n            # outputBounds=feature.total_bounds,\n            cropToCutline=not touch,\n            cutlineDSName=feature.file_name,\n            # cutlineLayer=feature.layer_names[0],\n            multithread=True,\n        )\n        dst = gdal.Warp(\"\", self.raster, options=warp_options)\n        dst_obj = Dataset(dst)\n\n        if touch:\n            dst_obj = Dataset.correct_wrap_cutline_error(dst_obj)\n\n        return dst_obj\n\n    @staticmethod\n    def correct_wrap_cutline_error(src: \"Dataset\"):\n        \"\"\"Correct wrap cutline error.\n\n        https://github.com/Serapieum-of-alex/pyramids/issues/74\n        \"\"\"\n        big_array = src.read_array()\n        value_to_remove = src.no_data_value[0]\n        \"\"\"Remove rows and columns that are all filled with a certain value from a 2D array.\"\"\"\n        # Find rows and columns to be removed\n        if big_array.ndim == 2:\n            rows_to_remove = np.all(big_array == value_to_remove, axis=1)\n            cols_to_remove = np.all(big_array == value_to_remove, axis=0)\n            # Use boolean indexing to remove rows and columns\n            small_array = big_array[~rows_to_remove][:, ~cols_to_remove]\n        elif big_array.ndim == 3:\n            rows_to_remove = np.all(big_array == value_to_remove, axis=(0, 2))\n            cols_to_remove = np.all(big_array == value_to_remove, axis=(0, 1))\n            # Use boolean indexing to remove rows and columns\n            # first remove the rows then the columns\n            small_array = big_array[:, ~rows_to_remove, :]\n            small_array = small_array[:, :, ~cols_to_remove]\n            n_rows = np.count_nonzero(~rows_to_remove)\n            n_cols = np.count_nonzero(~cols_to_remove)\n            small_array = small_array.reshape((src.band_count, n_rows, n_cols))\n        else:\n            raise ValueError(\"Array must be 2D or 3D\")\n\n        x_ind = np.where(~rows_to_remove)[0][0]\n        y_ind = np.where(~cols_to_remove)[0][0]\n        new_x = src.x[y_ind] - src.cell_size / 2\n        new_y = src.y[x_ind] + src.cell_size / 2\n        new_gt = (new_x, src.cell_size, 0, new_y, 0, -src.cell_size)\n        new_src = src.create_from_array(\n            small_array, geo=new_gt, epsg=src.epsg, no_data_value=src.no_data_value\n        )\n        return new_src\n\n    def crop(\n        self,\n        mask: Union[GeoDataFrame, FeatureCollection],\n        touch: bool = True,\n        inplace: bool = False,\n    ) -&gt; Union[\"Dataset\", None]:\n        \"\"\"Crop dataset using dataset/feature collection.\n\n            Crop/Clip the Dataset object using a polygon/raster.\n\n        Args:\n            mask (GeoDataFrame | Dataset):\n                GeoDataFrame with a polygon geometry, or a Dataset object.\n            touch (bool):\n                Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask.\n                Default is True.\n            inplace (bool):\n                If True, apply changes in place. Default is False.\n\n        Returns:\n            Dataset | None:\n                The cropped raster. If inplace is True, the method will change the raster in place and return None.\n\n        Hint:\n            - If the mask is a dataset with multi-bands, the `crop` method will use the first band as the mask.\n\n        Examples:\n            - Crop the raster using a polygon mask.\n\n              - The polygon covers 4 cells in the 3rd and 4th rows and 3rd and 4th column `arr[2:4, 2:4]`, so the result\n                dataset will have the same number of bands `4`, 2 rows and 2 columns.\n              - First, create the dataset to have 4 bands, 10 rows and 10 columns; the dataset has a cell size of 0.05\n                degree, the top left corner of the dataset is (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import geopandas as gpd\n              &gt;&gt;&gt; from shapely.geometry import Polygon\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(\n              ...         arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326\n              ... )\n\n              ```\n            - Second, create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2 -0.1]\n                to cover the 4 cells.\n\n                ```python\n                &gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n\n                ```\n            - Pass the `geodataframe` to the crop method using the `mask` parameter.\n\n              ```python\n              &gt;&gt;&gt; cropped_dataset = dataset.crop(mask=mask)\n\n              ```\n            - Check the cropped dataset:\n\n              ```python\n              &gt;&gt;&gt; print(cropped_dataset.shape)\n              (4, 2, 2)\n              &gt;&gt;&gt; print(cropped_dataset.geotransform)\n              (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n              &gt;&gt;&gt; print(cropped_dataset.read_array(band=0))# doctest: +SKIP\n              [[0.00921161 0.90841171]\n               [0.355636   0.18650262]]\n              &gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n              [[0.00921161 0.90841171]\n               [0.355636   0.18650262]]\n\n              ```\n            - Crop a raster using another raster mask:\n\n              - Create a mask dataset with the same extent of the polygon we used in the previous example.\n\n              ```python\n              &gt;&gt;&gt; geotransform = (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n              &gt;&gt;&gt; mask_dataset = Dataset.create_from_array(np.random.rand(2, 2), geo=geotransform, epsg=4326)\n\n              ```\n            - Then use the mask dataset to crop the dataset.\n\n              ```python\n              &gt;&gt;&gt; cropped_dataset_2 = dataset.crop(mask=mask_dataset)\n              &gt;&gt;&gt; print(cropped_dataset_2.shape)\n              (4, 2, 2)\n\n              ```\n            - Check the cropped dataset:\n\n              ```python\n              &gt;&gt;&gt; print(cropped_dataset_2.geotransform)\n              (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n              &gt;&gt;&gt; print(cropped_dataset_2.read_array(band=0))# doctest: +SKIP\n              [[0.00921161 0.90841171]\n               [0.355636   0.18650262]]\n              &gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n               [[0.00921161 0.90841171]\n               [0.355636   0.18650262]]\n\n              ```\n\n        \"\"\"\n        if isinstance(mask, GeoDataFrame):\n            dst = self._crop_with_polygon_warp(mask, touch=touch)\n        elif isinstance(mask, Dataset):\n            dst = self._crop_with_raster(mask)\n        else:\n            raise TypeError(\n                \"The second parameter: mask could be either GeoDataFrame or Dataset object\"\n            )\n\n        if inplace:\n            self.__init__(dst.raster)\n        else:\n            return dst\n\n    @staticmethod\n    def _nearest_neighbour(\n        array: np.ndarray, no_data_value: Union[float, int], rows: list, cols: list\n    ) -&gt; np.ndarray:\n        \"\"\"Fill specified cells with the value of the nearest neighbor.\n\n            - The _nearest_neighbour method fills the cells with the given indices in rows and cols with the value of the nearest neighbor.\n            - The raster grid is square, so the 4 perpendicular directions are of the same proximity; the function gives priority to the right, left, bottom, and then top, and similarly for 45-degree directions: right-bottom, left-bottom, left-top, right-top.\n\n        Args:\n            array (np.ndarray):\n                Array to fill some of its cells with the nearest value.\n            no_data_value (float | int):\n                Value stored in cells that are out of the domain.\n            rows (list[int]):\n                Row indices of the cells you want to fill with the nearest neighbor.\n            cols (list[int]):\n                Column indices of the cells you want to fill with the nearest neighbor.\n\n        Returns:\n            np.ndarray:\n                Cells of given indices filled with the value of the nearest neighbor.\n\n        Examples:\n            - Basic usage:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(5, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; req_rows = [1,3]\n              &gt;&gt;&gt; req_cols = [2,4]\n              &gt;&gt;&gt; no_data_value = dataset.no_data_value[0]\n              &gt;&gt;&gt; new_array = Dataset._nearest_neighbour(arr, no_data_value, req_rows, req_cols)\n\n              ```\n        \"\"\"\n        if not isinstance(array, np.ndarray):\n            raise TypeError(\n                \"src should be read using gdal (gdal dataset please read it using gdal library) \"\n            )\n        if not isinstance(rows, list):\n            raise TypeError(\"The `rows` input has to be of type list\")\n        if not isinstance(cols, list):\n            raise TypeError(\"The `cols` input has to be of type list\")\n\n        no_rows = np.shape(array)[0]\n        no_cols = np.shape(array)[1]\n\n        for i in range(len(rows)):\n            # give the cell the value of the cell that is at the right\n            if cols[i] + 1 &lt; no_cols:\n                if array[rows[i], cols[i] + 1] != no_data_value:\n                    array[rows[i], cols[i]] = array[rows[i], cols[i] + 1]\n\n            elif array[rows[i], cols[i] - 1] != no_data_value and cols[i] - 1 &gt; 0:\n                # give the cell the value of the cell that is at the left\n                array[rows[i], cols[i]] = array[rows[i], cols[i] - 1]\n\n            elif array[rows[i] - 1, cols[i]] != no_data_value and rows[i] - 1 &gt; 0:\n                # give the cell the value of the cell that is at the bottom\n                array[rows[i], cols[i]] = array[rows[i] - 1, cols[i]]\n\n            elif array[rows[i] + 1, cols[i]] != no_data_value and rows[i] + 1 &lt; no_rows:\n                # give the cell the value of the cell that is at the Top\n                array[rows[i], cols[i]] = array[rows[i] + 1, cols[i]]\n\n            elif (\n                array[rows[i] - 1, cols[i] + 1] != no_data_value\n                and rows[i] - 1 &gt; 0\n                and cols[i] + 1 &lt;= no_cols\n            ):\n                # give the cell the value of the cell that is at the right bottom\n                array[rows[i], cols[i]] = array[rows[i] - 1, cols[i] + 1]\n\n            elif (\n                array[rows[i] - 1, cols[i] - 1] != no_data_value\n                and rows[i] - 1 &gt; 0\n                and cols[i] - 1 &gt; 0\n            ):\n                # give the cell the value of the cell that is at the left bottom\n                array[rows[i], cols[i]] = array[rows[i] - 1, cols[i] - 1]\n\n            elif (\n                array[rows[i] + 1, cols[i] - 1] != no_data_value\n                and rows[i] + 1 &lt;= no_rows\n                and cols[i] - 1 &gt; 0\n            ):\n                # give the cell the value of the cell that is at the left Top\n                array[rows[i], cols[i]] = array[rows[i] + 1, cols[i] - 1]\n\n            elif (\n                array[rows[i] + 1, cols[i] + 1] != no_data_value\n                and rows[i] + 1 &lt;= no_rows\n                and cols[i] + 1 &lt;= no_cols\n            ):\n                # give the cell the value of the cell that is at the right Top\n                array[rows[i], cols[i]] = array[rows[i] + 1, cols[i] + 1]\n            else:\n                print(\"the cell is isolated (No surrounding cells exist)\")\n        return array\n\n    def map_to_array_coordinates(\n        self,\n        points: Union[GeoDataFrame, FeatureCollection, DataFrame],\n    ) -&gt; np.ndarray:\n        \"\"\"Convert coordinates of points to array indices.\n\n        - map_to_array_coordinates locates a point with real coordinates (x, y) or (lon, lat) on the array by finding\n            the cell indices (row, column) of the nearest cell in the raster.\n        - The point coordinate system of the raster has to be projected to be able to calculate the distance.\n\n        Args:\n            points (GeoDataFrame | pandas.DataFrame | FeatureCollection):\n                - GeoDataFrame: GeoDataFrame with POINT geometry.\n                - DataFrame: DataFrame with x, y columns.\n\n        Returns:\n            np.ndarray:\n                Array with shape (N, 2) containing the row and column indices in the array.\n\n        Examples:\n            - Create `Dataset` consisting of 2 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n            - DataFrame with x, y columns:\n\n              - We can give the function a DataFrame with x, y columns to array the coordinates of the points that are located within the dataset domain.\n\n              ```python\n              &gt;&gt;&gt; points = pd.DataFrame({\"x\": [0.025, 0.175, 0.375], \"y\": [0.025, 0.225, 0.125]})\n              &gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n              &gt;&gt;&gt; print(indices)\n              [[0 0]\n               [0 3]\n               [0 7]]\n\n              ```\n            - GeoDataFrame with POINT geometry:\n\n              - We can give the function a GeoDataFrame with POINT geometry to array the coordinates of the points that locate within the dataset domain.\n\n              ```python\n              &gt;&gt;&gt; from shapely.geometry import Point\n              &gt;&gt;&gt; from geopandas import GeoDataFrame\n              &gt;&gt;&gt; points = GeoDataFrame({\"geometry\": [Point(0.025, 0.025), Point(0.175, 0.225), Point(0.375, 0.125)]})\n              &gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n              &gt;&gt;&gt; print(indices)\n              [[0 0]\n               [0 3]\n               [0 7]]\n\n              ```\n        \"\"\"\n        if isinstance(points, GeoDataFrame):\n            points = FeatureCollection(points)\n        elif isinstance(points, DataFrame):\n            if all(elem not in points.columns for elem in [\"x\", \"y\"]):\n                raise ValueError(\n                    \"If the input is a DataFrame, it should have two columns x, and y\"\n                )\n        else:\n            if not isinstance(points, FeatureCollection):\n                raise TypeError(\n                    \"please check points input it should be GeoDataFrame/DataFrame/FeatureCollection - given\"\n                    f\" {type(points)}\"\n                )\n        if not isinstance(points, DataFrame):\n            # get the x, y coordinates.\n            points.xy()\n            points = points.feature.loc[:, [\"x\", \"y\"]].values\n        else:\n            points = points.loc[:, [\"x\", \"y\"]].values\n\n        # since the first row is x-coords so the first column in the indices is the column index\n        indices = locate_values(points, self.x, self.y)\n        # rearrange the columns to make the row index first\n        indices = indices[:, [1, 0]]\n        return indices\n\n    def array_to_map_coordinates(\n        self,\n        rows_index: Union[List[Number], np.ndarray],\n        column_index: Union[List[Number], np.ndarray],\n        center: bool = False,\n    ) -&gt; Tuple[List[Number], List[Number]]:\n        \"\"\"Convert array indices to map coordinates.\n\n        array_to_map_coordinates converts the array indices (rows, cols) to real coordinates (x, y) or (lon, lat).\n\n        Args:\n            rows_index (List[Number] | np.ndarray):\n                The row indices of the cells in the raster array.\n            column_index (List[Number] | np.ndarray):\n                The column indices of the cells in the raster array.\n            center (bool):\n                If True, the coordinates will be the center of the cell. Default is False.\n\n        Returns:\n            Tuple[List[Number], List[Number]]:\n                A tuple of two lists: the x coordinates and the y coordinates of the cells.\n\n        Examples:\n            - Create `Dataset` consisting of 1 band, 10 rows, 10 columns, at the point lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Now call the function with two lists of row and column indices:\n\n              ```python\n              &gt;&gt;&gt; rows_index = [1, 3, 5]\n              &gt;&gt;&gt; column_index = [2, 4, 6]\n              &gt;&gt;&gt; coords = dataset.array_to_map_coordinates(rows_index, column_index)\n              &gt;&gt;&gt; print(coords) # doctest: +SKIP\n              ([0.1, 0.2, 0.3], [-0.05, -0.15, -0.25])\n\n              ```\n        \"\"\"\n        top_left_x, top_left_y = self.top_left_corner\n        cell_size = self.cell_size\n        if center:\n            # for the top left corner of the cell\n            top_left_x += cell_size / 2\n            top_left_y -= cell_size / 2\n\n        x_coord_fn = lambda x: top_left_x + x * cell_size\n        y_coord_fn = lambda y: top_left_y - y * cell_size\n\n        x_coords = list(map(x_coord_fn, column_index))\n        y_coords = list(map(y_coord_fn, rows_index))\n\n        return x_coords, y_coords\n\n    def extract(\n        self,\n        band: int = None,\n        exclude_value: Any = None,\n        feature: Union[FeatureCollection, GeoDataFrame] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Extract.\n\n        - Extract method gets all the values in a raster, and excludes the values in the exclude_value parameter.\n        - If the feature parameter is given, the raster will be clipped to the extent of the given feature and the\n          values within the feature are extracted.\n\n        Args:\n            band (int, optional):\n                Band index. Default is None.\n            exclude_value (Numeric, optional):\n                Values to exclude from extracted values. If the dataset is multi-band, the values in `exclude_value`\n                will be filtered out from the first band only.\n            feature (FeatureCollection | GeoDataFrame, optional):\n                Vector data containing point geometries at which to extract the values. Default is None.\n\n        Returns:\n            np.ndarray:\n                The extracted values from each band in the dataset will be in one row in the returned array.\n\n        Examples:\n            - Extract all values from the dataset:\n\n              - First, create a dataset with 2 bands, 4 rows and 4 columns:\n\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(2, 4, 4))\n                &gt;&gt;&gt; top_left_corner = (0, 0)\n                &gt;&gt;&gt; cell_size = 0.05\n                &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n                &gt;&gt;&gt; print(dataset)\n                &lt;BLANKLINE&gt;\n                            Cell size: 0.05\n                            Dimension: 4 * 4\n                            EPSG: 4326\n                            Number of Bands: 2\n                            Band names: ['Band_1', 'Band_2']\n                            Mask: -9999.0\n                            Data type: int32\n                            File:...\n                &lt;BLANKLINE&gt;\n                &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n                [[[1 3 3 4]\n                  [1 4 2 4]\n                  [2 4 2 1]\n                  [1 3 2 3]]\n                 [[3 2 1 3]\n                  [4 3 2 2]\n                  [2 2 3 4]\n                  [1 4 1 4]]]\n\n                ```\n\n              - Now, extract the values in the dataset:\n\n                ```python\n                &gt;&gt;&gt; values = dataset.extract()\n                &gt;&gt;&gt; print(values) # doctest: +SKIP\n                [[1 3 3 4 1 4 2 4 2 4 2 1 1 3 2 3]\n                 [3 2 1 3 4 3 2 2 2 2 3 4 1 4 1 4]]\n\n                ```\n\n              - Extract all the values except 2:\n\n                ```python\n                &gt;&gt;&gt; values = dataset.extract(exclude_value=2)\n                &gt;&gt;&gt; print(values) # doctest: +SKIP\n\n                ```\n\n            - Extract values at the location of the given point geometries:\n\n              ```python\n              &gt;&gt;&gt; import geopandas as gpd\n              &gt;&gt;&gt; from shapely.geometry import Point\n              ```\n\n              - Create the points using shapely and GeoPandas to cover the 4 cells with xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2, -0.1]:\n\n                ```python\n                &gt;&gt;&gt; points = gpd.GeoDataFrame(geometry=[Point(0.1, -0.1), Point(0.1, -0.2), Point(0.2, -0.2), Point(0.2, -0.1)],crs=4326)\n                &gt;&gt;&gt; values = dataset.extract(feature=points)\n                &gt;&gt;&gt; print(values) # doctest: +SKIP\n                [[4 3 3 4]\n                 [3 4 4 2]]\n\n                ```\n        \"\"\"\n        # Optimize: make the read_array return only the array for inside the mask feature, and not to read the whole\n        #  raster\n        arr = self.read_array(band=band)\n        no_data_value = (\n            self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n        )\n        if feature is None:\n            mask = (\n                [no_data_value, exclude_value]\n                if exclude_value is not None\n                else [no_data_value]\n            )\n            values = get_pixels2(arr, mask)\n        else:\n            indices = self.map_to_array_coordinates(feature)\n            if arr.ndim &gt; 2:\n                values = arr[:, indices[:, 0], indices[:, 1]]\n            else:\n                values = arr[indices[:, 0], indices[:, 1]]\n\n        return values\n\n    def overlay(\n        self,\n        classes_map,\n        band: int = 0,\n        exclude_value: Union[float, int] = None,\n    ) -&gt; Dict[List[float], List[float]]:\n        \"\"\"Overlay.\n\n        Overlay method extracts all the values in the dataset for each class in the given class map.\n\n        Args:\n            classes_map (Dataset):\n                Dataset object for the raster that has classes you want to overlay with the raster.\n            band (int):\n                If the raster is multi-band, choose the band you want to overlay with the classes map. Default is 0.\n            exclude_value (Numeric, optional):\n                Values you want to exclude from extracted values. Default is None.\n\n        Returns:\n            Dict:\n                Dictionary with class values as keys (from the class map), and for each key a list of all the intersected\n                values in the base map.\n\n        Examples:\n            - Read the dataset:\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/raster-folder/MSWEP_1979.01.01.tif\")\n              &gt;&gt;&gt; dataset.plot(figsize=(6, 8)) # doctest: +SKIP\n\n              ```\n\n              ![rhine-rainfall](./../_images/dataset/rhine-rainfall.png)\n\n            - Read the classes dataset:\n\n              ```python\n              &gt;&gt;&gt; classes = Dataset.read_file(\"examples/data/geotiff/rhine-classes.tif\")\n              &gt;&gt;&gt; classes.plot(figsize=(6, 8), color_scale=4, bounds=[1,2,3,4,5,6]) # doctest: +SKIP\n\n              ```\n\n              ![rhine-classes](./../_images/dataset/rhine-classes.png)\n\n            - Overlay the dataset with the classes dataset:\n\n              ```python\n              &gt;&gt;&gt; classes_dict = dataset.overlay(classes)\n              &gt;&gt;&gt; print(classes_dict.keys()) # doctest: +SKIP\n              dict_keys([1, 2, 3, 4, 5])\n\n              ```\n\n            - You can use the key `1` to get the values that overlay class 1.\n        \"\"\"\n        if not self._check_alignment(classes_map):\n            raise AlignmentError(\n                \"The class Dataset is not aligned with the current raster, please use the method \"\n                \"'align' to align both rasters.\"\n            )\n        arr = self.read_array(band=band)\n        no_data_value = (\n            self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n        )\n        mask = (\n            [no_data_value, exclude_value]\n            if exclude_value is not None\n            else [no_data_value]\n        )\n        ind = get_indices2(arr, mask)\n        classes = classes_map.read_array()\n        values = dict()\n\n        # extract values\n        for i, ind_i in enumerate(ind):\n            # first check if the sub-basin has a list in the dict if not create a list\n            key = classes[ind_i[0], ind_i[1]]\n            if key not in list(values.keys()):\n                values[key] = list()\n\n            values[key].append(arr[ind_i[0], ind_i[1]])\n\n        return values\n\n    def get_mask(self, band: int = 0) -&gt; np.ndarray:\n        \"\"\"Get the mask array.\n\n        Args:\n            band (int):\n                Band index. Default is 0.\n\n        Returns:\n            np.ndarray:\n                Array of the mask. 0 value for cells out of the domain, and 255 for cells in the domain.\n        \"\"\"\n        # TODO: there is a CreateMaskBand method in the gdal.Dataset class, it creates a mask band for the dataset\n        #   either internally or externally.\n        arr = self._iloc(band).GetMaskBand().ReadAsArray()\n        return arr\n\n    def footprint(\n        self,\n        band: int = 0,\n        exclude_values: Optional[List[Any]] = None,\n    ) -&gt; Union[GeoDataFrame, None]:\n        \"\"\"Extract the real coverage of the values in a certain band.\n\n        Args:\n            band (int):\n                Band index. Default is 0.\n            exclude_values (Optional[List[Any]]):\n                If you want to exclude a certain value in the raster with another value inter the two values as a\n                list of tuples a [(value_to_be_exclude_valuesd, new_value)].\n\n                - Example of exclude_values usage:\n\n                  ```python\n                  &gt;&gt;&gt; exclude_values = [0]\n\n                  ```\n\n                - This parameter is introduced particularly in the case of rasters that has the no_data_value stored in\n                  the `no_data_value` property does not match the value stored in the band, so this option can correct\n                  this behavior.\n\n        Returns:\n            GeoDataFrame: \n                - geodataframe containing the polygon representing the extent of the raster. the extent column should\n                  contain a value of 2 only.\n                - if the dataset had separate polygons, each polygon will be in a separate row.\n\n        Examples:\n            - The following raster dataset has flood depth stored in its values, and the non-flooded cells are filled with\n              zero, so to extract the flood extent, we need to exclude the zero flood depth cells.\n\n              ```python\n              &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/rhine-flood.tif\")\n              &gt;&gt;&gt; dataset.plot()\n              (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n              ```\n\n            ![dataset-footprint-rhine-flood](./../_images/dataset/dataset-footprint-rhine-flood.png)\n\n            - Now, to extract the footprint of the dataset band, we need to specify the `exclude_values` parameter with the\n              value of the non-flooded cells.\n\n              ```python\n              &gt;&gt;&gt; extent = dataset.footprint(band=0, exclude_values=[0])\n              &gt;&gt;&gt; print(extent)\n                 Band_1                                           geometry\n              0     2.0  POLYGON ((4070974.182 3181069.473, 4070974.182...\n              1     2.0  POLYGON ((4077674.182 3181169.473, 4077674.182...\n              2     2.0  POLYGON ((4091174.182 3169169.473, 4091174.182...\n              3     2.0  POLYGON ((4088574.182 3176269.473, 4088574.182...\n              4     2.0  POLYGON ((4082974.182 3167869.473, 4082974.182...\n              5     2.0  POLYGON ((4092274.182 3168269.473, 4092274.182...\n              6     2.0  POLYGON ((4072474.182 3181169.473, 4072474.182...\n\n              &gt;&gt;&gt; extent.plot()\n              &lt;Axes: &gt;\n\n              ```\n\n            ![dataset-footprint-rhine-flood-extent](./../_images/dataset/dataset-footprint-rhine-flood-extent.png)\n\n        \"\"\"\n        arr = self.read_array(band=band)\n        no_data_val = self.no_data_value[band]\n\n        if no_data_val is None:\n            if not (np.isnan(arr)).any():\n                logger.warning(\n                    \"The nodata value stored in the raster does not exist in the raster \"\n                    \"so either the raster extent is all full of data, or the no_data_value stored in the raster is\"\n                    \" not correct\"\n                )\n        else:\n            if not (np.isclose(arr, no_data_val, rtol=0.00001)).any():\n                logger.warning(\n                    \"the nodata value stored in the raster does not exist in the raster \"\n                    \"so either the raster extent is all full of data, or the no_data_value stored in the raster is\"\n                    \" not correct\"\n                )\n        # if you want to exclude_values any value in the raster\n        if exclude_values:\n            for val in exclude_values:\n                try:\n                    # in case the val2 is None, and the array is int type, the following line will give error as None\n                    # is considered as float\n                    arr[np.isclose(arr, val)] = no_data_val\n                except TypeError:\n                    arr = arr.astype(np.float32)\n                    arr[np.isclose(arr, val)] = no_data_val\n\n        # replace all the values with 2\n        if no_data_val is None:\n            # check if the whole raster is full of no_data_value\n            if (np.isnan(arr)).all():\n                logger.warning(\"the raster is full of no_data_value\")\n                return None\n\n            arr[~np.isnan(arr)] = 2\n        else:\n            # check if the whole raster is full of no_data_value\n            if (np.isclose(arr, no_data_val, rtol=0.00001)).all():\n                logger.warning(\"the raster is full of no_data_value\")\n                return None\n\n            arr[~np.isclose(arr, no_data_val, rtol=0.00001)] = 2\n        new_dataset = self.create_from_array(\n            arr, geo=self.geotransform, epsg=self.epsg, no_data_value=self.no_data_value\n        )\n        # then convert the raster into polygon\n        gdf = new_dataset.cluster2(band=band)\n        gdf.rename(columns={\"Band_1\": self.band_names[band]}, inplace=True)\n\n        return gdf\n\n    @staticmethod\n    def normalize(array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize numpy arrays into scale 0.0\u20131.0.\n\n        Args:\n            array (np.ndarray): Numpy array to normalize.\n\n        Returns:\n            np.ndarray: Normalized array.\n        \"\"\"\n        array_min = array.min()\n        array_max = array.max()\n        val = (array - array_min) / (array_max - array_min)\n        return val\n\n    def _window(self, size: int = 256):\n        \"\"\"Dataset square window size/offsets.\n\n        Args:\n            size (int):\n                Size of the window in pixels. One value required which is used for both the x and y size. e.g.,\n                256 means a 256x256 window. Default is 256.\n\n        Yields:\n            tuple[int, int, int, int]:\n                (x-offset/column-index, y-offset/row-index, x-size, y-size).\n\n        Examples:\n            - Generate 2x2 windows over a 3x5 dataset:\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(3, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; tile_dimensions = list(dataset._window(2))\n              &gt;&gt;&gt; print(tile_dimensions)\n              [(0, 0, 2, 2), (2, 0, 2, 2), (4, 0, 1, 2), (0, 2, 2, 1), (2, 2, 2, 1), (4, 2, 1, 1)]\n\n              ```\n        \"\"\"\n        cols = self.columns\n        rows = self.rows\n        for yoff in range(0, rows, size):\n            ysize = size if size + yoff &lt;= rows else rows - yoff\n            for xoff in range(0, cols, size):\n                xsize = size if size + xoff &lt;= cols else cols - xoff\n                yield xoff, yoff, xsize, ysize\n\n    def get_tile(self, size=256) -&gt; Generator[np.ndarray, None, None]:\n        \"\"\"Get tile.\n\n        Args:\n            size (int):\n                Size of the window in pixels. One value is required which is used for both the x and y size. e.g., 256\n                means a 256x256 window. Default is 256.\n\n        Yields:\n            np.ndarray:\n                Dataset array with a shape `[band, y, x]`.\n\n        Examples:\n            - First, we will create a dataset with 3 rows and 5 columns.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(3, 5)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; print(dataset)\n              &lt;BLANKLINE&gt;\n                          Cell size: 0.05\n                          Dimension: 3 * 5\n                          EPSG: 4326\n                          Number of Bands: 1\n                          Band names: ['Band_1']\n                          Mask: -9999.0\n                          Data type: float64\n                          File:...\n              &lt;BLANKLINE&gt;\n\n              &gt;&gt;&gt; print(dataset.read_array())   # doctest: +SKIP\n              [[0.55332314 0.48364841 0.67794589 0.6901816  0.70516817]\n               [0.82518332 0.75657103 0.45693945 0.44331782 0.74677865]\n               [0.22231314 0.96283065 0.15201337 0.03522544 0.44616888]]\n\n              ```\n            - The `get_tile` method splits the domain into tiles of the specified `size` using the `_window` function.\n\n              ```python\n              &gt;&gt;&gt; tile_dimensions = list(dataset._window(2))\n              &gt;&gt;&gt; print(tile_dimensions)\n              [(0, 0, 2, 2), (2, 0, 2, 2), (4, 0, 1, 2), (0, 2, 2, 1), (2, 2, 2, 1), (4, 2, 1, 1)]\n\n              ```\n              ![get_tile](./../_images/dataset/get_tile.png)\n\n            - So the first two chunks are 2*2, 2*1 chunk, then two 1*2 chunks, and the last chunk is 1*1.\n            - The `get_tile` method returns a generator object that can be used to iterate over the smaller chunks of\n                the data.\n\n              ```python\n              &gt;&gt;&gt; tiles_generator = dataset.get_tile(size=2)\n              &gt;&gt;&gt; print(tiles_generator)  # doctest: +SKIP\n              &lt;generator object Dataset.get_tile at 0x00000145AA39E680&gt;\n              &gt;&gt;&gt; print(list(tiles_generator))  # doctest: +SKIP\n              [\n                  array([[0.55332314, 0.48364841],\n                         [0.82518332, 0.75657103]]),\n                  array([[0.67794589, 0.6901816 ],\n                         [0.45693945, 0.44331782]]),\n                  array([[0.70516817], [0.74677865]]),\n                  array([[0.22231314, 0.96283065]]),\n                  array([[0.15201337, 0.03522544]]),\n                  array([[0.44616888]])\n              ]\n\n              ```\n        \"\"\"\n        for xoff, yoff, xsize, ysize in self._window(size=size):\n            # read the array at certain indices\n            yield self.raster.ReadAsArray(\n                xoff=xoff, yoff=yoff, xsize=xsize, ysize=ysize\n            )\n\n    @staticmethod\n    def _group_neighbours(\n        array, i, j, lower_bound, upper_bound, position, values, count, cluster\n    ):\n        \"\"\"Group neighboring cells with the same values.\"\"\"\n        # bottom cell\n        if i + 1 &lt; array.shape[0]:\n            if lower_bound &lt;= array[i + 1, j] &lt;= upper_bound and cluster[i + 1, j] == 0:\n                position.append([i + 1, j])\n                values.append(array[i + 1, j])\n                cluster[i + 1, j] = count\n                Dataset._group_neighbours(\n                    array,\n                    i + 1,\n                    j,\n                    lower_bound,\n                    upper_bound,\n                    position,\n                    values,\n                    count,\n                    cluster,\n                )\n\n        # bottom right\n        if j + 1 &lt; array.shape[1]:\n            if i + 1 &lt; array.shape[0]:\n                if (\n                    lower_bound &lt;= array[i + 1, j + 1] &lt;= upper_bound\n                    and cluster[i + 1, j + 1] == 0\n                ):\n                    position.append([i + 1, j + 1])\n                    values.append(array[i + 1, j + 1])\n                    cluster[i + 1, j + 1] = count\n                    Dataset._group_neighbours(\n                        array,\n                        i + 1,\n                        j + 1,\n                        lower_bound,\n                        upper_bound,\n                        position,\n                        values,\n                        count,\n                        cluster,\n                    )\n\n        # right\n        if j + 1 &lt; array.shape[1]:\n            if lower_bound &lt;= array[i, j + 1] &lt;= upper_bound and cluster[i, j + 1] == 0:\n                position.append([i, j + 1])\n                values.append(array[i, j + 1])\n                cluster[i, j + 1] = count\n                Dataset._group_neighbours(\n                    array,\n                    i,\n                    j + 1,\n                    lower_bound,\n                    upper_bound,\n                    position,\n                    values,\n                    count,\n                    cluster,\n                )\n\n        # upper right\n        if j + 1 &lt; array.shape[1]:\n            if i - 1 &gt;= 0:\n                if (\n                    lower_bound &lt;= array[i - 1, j + 1] &lt;= upper_bound\n                    and cluster[i - 1, j + 1] == 0\n                ):\n                    position.append([i - 1, j + 1])\n                    values.append(array[i - 1, j + 1])\n                    cluster[i - 1, j + 1] = count\n                    Dataset._group_neighbours(\n                        array,\n                        i - 1,\n                        j + 1,\n                        lower_bound,\n                        upper_bound,\n                        position,\n                        values,\n                        count,\n                        cluster,\n                    )\n        # top\n        if i - 1 &gt;= 0:\n            if lower_bound &lt;= array[i - 1, j] &lt;= upper_bound and cluster[i - 1, j] == 0:\n                position.append([i - 1, j])\n                values.append(array[i - 1, j])\n                cluster[i - 1, j] = count\n                Dataset._group_neighbours(\n                    array,\n                    i - 1,\n                    j,\n                    lower_bound,\n                    upper_bound,\n                    position,\n                    values,\n                    count,\n                    cluster,\n                )\n\n        # top left\n        if i - 1 &gt;= 0:\n            if j - 1 &gt;= 0:\n                if (\n                    lower_bound &lt;= array[i - 1, j - 1] &lt;= upper_bound\n                    and cluster[i - 1, j - 1] == 0\n                ):\n                    position.append([i - 1, j - 1])\n                    values.append(array[i - 1, j - 1])\n                    cluster[i - 1, j - 1] = count\n                    Dataset._group_neighbours(\n                        array,\n                        i - 1,\n                        j - 1,\n                        lower_bound,\n                        upper_bound,\n                        position,\n                        values,\n                        count,\n                        cluster,\n                    )\n        # left\n        if j - 1 &gt;= 0:\n            if lower_bound &lt;= array[i, j - 1] &lt;= upper_bound and cluster[i, j - 1] == 0:\n                position.append([i, j - 1])\n                values.append(array[i, j - 1])\n                cluster[i, j - 1] = count\n                Dataset._group_neighbours(\n                    array,\n                    i,\n                    j - 1,\n                    lower_bound,\n                    upper_bound,\n                    position,\n                    values,\n                    count,\n                    cluster,\n                )\n\n        # bottom left\n        if j - 1 &gt;= 0:\n            if i + 1 &lt; array.shape[0]:\n                if (\n                    lower_bound &lt;= array[i + 1, j - 1] &lt;= upper_bound\n                    and cluster[i + 1, j - 1] == 0\n                ):\n                    position.append([i + 1, j - 1])\n                    values.append(array[i + 1, j - 1])\n                    cluster[i + 1, j - 1] = count\n                    Dataset._group_neighbours(\n                        array,\n                        i + 1,\n                        j - 1,\n                        lower_bound,\n                        upper_bound,\n                        position,\n                        values,\n                        count,\n                        cluster,\n                    )\n\n    def cluster(\n        self, lower_bound: Any, upper_bound: Any\n    ) -&gt; Tuple[np.ndarray, int, list, list]:\n        \"\"\"Group all the connected values between two bounds.\n\n        Args:\n            lower_bound (Number):\n                Lower bound of the cluster.\n            upper_bound (Number):\n                Upper bound of the cluster.\n\n        Returns:\n            tuple[np.ndarray, int, list, list]:\n                - cluster (np.ndarray):\n                    Array with integers representing the cluster number per cell.\n                - count (int):\n                    Number of clusters in the array.\n                - position (list[list[int, int]]):\n                    List of [row, col] indices for the position of each value.\n                - values (list[Number]):\n                    Values stored in each cell in the cluster.\n\n        Examples:\n            - First, we will create a dataset with 10 rows and 10 columns.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; np.random.seed(10)\n              &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n              &gt;&gt;&gt; print(arr) # doctest: +SKIP\n              [[2 3 3 2 3]\n               [3 4 1 1 1]\n               [1 3 3 2 2]\n               [4 1 1 3 2]\n               [2 4 2 3 2]]\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; dataset.plot(\n              ...     color_scale=4, bounds=[1, 1.9, 4.1, 5], display_cell_value=True, num_size=12,\n              ...     background_color_threshold=5\n              ... )  # doctest: +SKIP\n\n              ```\n              ![cluster](./../_images/dataset/cluster.png)\n\n            - Now let's cluster the values in the dataset that are between 2 and 4.\n\n              ```python\n              &gt;&gt;&gt; lower_value = 2\n              &gt;&gt;&gt; upper_value = 4\n              &gt;&gt;&gt; cluster_array, count, position, values = dataset.cluster(lower_value, upper_value)\n\n              ```\n            - The first returned output is a binary array with 1 indicating that the cell value is inside the cluster, and 0 is outside.\n\n              ```python\n              &gt;&gt;&gt; print(cluster_array)  # doctest: +SKIP\n              [[1. 1. 1. 1. 1.]\n               [1. 1. 0. 0. 0.]\n               [0. 1. 1. 1. 1.]\n               [1. 0. 0. 1. 1.]\n               [1. 1. 1. 1. 1.]]\n\n              ```\n            - The second returned value is the number of connected clusters.\n\n              ```python\n              &gt;&gt;&gt; print(count) # doctest: +SKIP\n              2\n\n              ```\n            - The third returned value is the indices of the cells that belong to the cluster.\n\n              ```python\n              &gt;&gt;&gt; print(position) # doctest: +SKIP\n              [[1, 0], [2, 1], [2, 2], [3, 3], [4, 3], [4, 4], [3, 4], [2, 4], [2, 3], [4, 2], [4, 1], [3, 0], [4, 0], [1, 1], [0, 2], [0, 3], [0, 4], [0, 1], [0, 0]]\n\n              ```\n            - The fourth returned value is a list of the values that are in the cluster (extracted from these cells).\n\n              ```python\n              &gt;&gt;&gt; print(values) # doctest: +SKIP\n              [3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 4, 3, 2, 3, 3, 2]\n\n              ```\n\n        \"\"\"\n        data = self.read_array()\n        position = []\n        values = []\n        count = 1\n        cluster = np.zeros(shape=(data.shape[0], data.shape[1]))\n\n        for i in range(data.shape[0]):\n            for j in range(data.shape[1]):\n                if lower_bound &lt;= data[i, j] &lt;= upper_bound and cluster[i, j] == 0:\n                    self._group_neighbours(\n                        data,\n                        i,\n                        j,\n                        lower_bound,\n                        upper_bound,\n                        position,\n                        values,\n                        count,\n                        cluster,\n                    )\n                    if cluster[i, j] == 0:\n                        position.append([i, j])\n                        values.append(data[i, j])\n                        cluster[i, j] = count\n                    count += 1\n\n        return cluster, count, position, values\n\n    def cluster2(\n        self,\n        band: Union[int, List[int]] = None,\n    ) -&gt; GeoDataFrame:\n        \"\"\"Cluster the connected equal cells into polygons.\n\n        - Creates vector polygons for all connected regions of pixels in the raster sharing a common\n            pixel value (group neighboring cells with the same value into one polygon).\n\n        Args:\n            band (int | List[int] | None):\n                Band index 0, 1, 2, 3, \u2026\n\n        Returns:\n            GeoDataFrame:\n                GeodataFrame containing polygon geomtries for all connected regions.\n\n        Examples:\n            - First, we will create a 10*10 dataset full of random integer between 1, and 5.\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; np.random.seed(200)\n              &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(10, 10))\n              &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n              [[3 2 1 1 3 4 1 4 2 3]\n               [4 2 2 4 3 3 1 2 4 4]\n               [4 2 4 2 3 4 2 1 4 3]\n               [3 2 1 4 3 3 4 1 1 4]\n               [1 2 4 2 2 1 3 2 3 1]\n               [1 4 4 4 1 1 4 2 1 1]\n               [1 3 2 3 3 4 1 3 1 3]\n               [4 1 3 3 3 4 1 4 1 1]\n               [2 1 3 3 4 2 2 1 3 4]\n               [2 3 2 2 4 2 1 3 2 2]]\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Now, let's cluster the connected equal cells into polygons.\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.cluster2()\n              &gt;&gt;&gt; print(gdf)  # doctest: +SKIP\n                  Band_1                                           geometry\n              0        3  POLYGON ((0 0, 0 -0.05, 0.05 -0.05, 0.05 0, 0 0))\n              1        1  POLYGON ((0.1 0, 0.1 -0.05, 0.2 -0.05, 0.2 0, ...\n              2        4  POLYGON ((0.25 0, 0.25 -0.05, 0.3 -0.05, 0.3 0...\n              3        4  POLYGON ((0.35 0, 0.35 -0.05, 0.4 -0.05, 0.4 0...\n              4        2  POLYGON ((0.4 0, 0.4 -0.05, 0.45 -0.05, 0.45 0...\n              5        3  POLYGON ((0.45 0, 0.45 -0.05, 0.5 -0.05, 0.5 0...\n              6        1  POLYGON ((0.3 0, 0.3 -0.1, 0.35 -0.1, 0.35 0, ...\n              7        4  POLYGON ((0.15 -0.05, 0.15 -0.1, 0.2 -0.1, 0.2...\n              8        2  POLYGON ((0.35 -0.05, 0.35 -0.1, 0.4 -0.1, 0.4...\n              9        4  POLYGON ((0 -0.05, 0 -0.15, 0.05 -0.15, 0.05 -...\n              10       4  POLYGON ((0.4 -0.05, 0.4 -0.15, 0.45 -0.15, 0....\n              11       4  POLYGON ((0.1 -0.1, 0.1 -0.15, 0.15 -0.15, 0.1...\n\n              ```\n\n        \"\"\"\n        if band is None:\n            band = 0\n\n        name = self.band_names[band]\n        gdf = self._band_to_polygon(band, name)\n\n        return gdf\n\n    @property\n    def overview_count(self) -&gt; List[int]:\n        \"\"\"Number of the overviews for each band.\"\"\"\n        overview_number = []\n        for i in range(self.band_count):\n            overview_number.append(self._iloc(i).GetOverviewCount())\n\n        return overview_number\n\n    def create_overviews(\n        self, resampling_method: str = \"nearest\", overview_levels: list = None\n    ) -&gt; None:\n        \"\"\"Create overviews for the dataset.\n\n        Args:\n            resampling_method (str):\n                The resampling method used to create the overviews. Possible values are\n                \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\",\n                \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".\n            overview_levels (list, optional):\n                The overview levels. Restricted to typical power-of-two reduction factors. Defaults to [2, 4, 8, 16,\n                32].\n\n        Returns:\n            None:\n                Creates internal or external overviews depending on the dataset access mode. See Notes.\n\n        Notes:\n            - External (.ovr file): If the dataset is read with `read_only=True` then the overviews file will be created\n              as an external .ovr file in the same directory of the dataset.\n            - Internal: If the dataset is read with `read_only=False` then the overviews will be created internally in\n              the dataset, and the dataset needs to be saved/flushed to persist the changes to disk.\n            - You can check the count per band via the `overview_count` property.\n\n        Examples:\n            - Create a Dataset with 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n            - Now, create overviews using the default parameters:\n\n              ```python\n              &gt;&gt;&gt; dataset.create_overviews()\n              &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n              [4, 4, 4, 4]\n\n              ```\n            - For each band, there are 4 overview levels you can use to plot the bands:\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=0) # doctest: +SKIP\n\n              ```\n              ![overviews-level-0](./../_images/dataset/overviews-level-0.png)\n\n            - However, the dataset originally is 10*10, but the first overview level (2) displays half of the cells by\n              aggregating all the cells using the nearest neighbor. The second level displays only 3 cells in each:\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=1)   # doctest: +SKIP\n\n              ```\n              ![overviews-level-1](./../_images/dataset/overviews-level-1.png)\n\n            - For the third overview level:\n\n              ```python\n              &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=2)       # doctest: +SKIP\n\n              ```\n              ![overviews-level-2](./../_images/dataset/overviews-level-2.png)\n\n        See Also:\n            - Dataset.recreate_overviews: Recreate the dataset overviews if they exist\n            - Dataset.get_overview: Get an overview of a band\n            - Dataset.overview_count: Number of overviews\n            - Dataset.read_overview_array: Read overview values\n            - Dataset.plot: Plot a band\n        \"\"\"\n        if overview_levels is None:\n            overview_levels = OVERVIEW_LEVELS\n        else:\n            if not isinstance(overview_levels, list):\n                raise TypeError(\"overview_levels should be a list\")\n\n            # if self.raster.HasArbitraryOverviews():\n            if not all(elem in OVERVIEW_LEVELS for elem in overview_levels):\n                raise ValueError(\n                    \"overview_levels are restricted to the typical power-of-two reduction factors \"\n                    \"(like 2, 4, 8, 16, etc.)\"\n                )\n\n        if resampling_method.upper() not in RESAMPLING_METHODS:\n            raise ValueError(f\"resampling_method should be one of {RESAMPLING_METHODS}\")\n        # Define the overview levels (the reduction factor).\n        # e.g., 2 means the overview will be half the resolution of the original dataset.\n\n        # Build overviews using nearest neighbor resampling\n        # NEAREST is the resampling method used. Other methods include AVERAGE, GAUSS, etc.\n        self.raster.BuildOverviews(resampling_method, overview_levels)\n\n    def recreate_overviews(self, resampling_method: str = \"nearest\"):\n        \"\"\"Recreate overviews for the dataset.\n\n        Args:\n            resampling_method (str): Resampling method used to recreate overviews. Possible values are\n                \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\",\n                \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".\n\n        Raises:\n            ValueError:\n                If resampling_method is not one of the allowed values above.\n            ReadOnlyError:\n                If overviews are internal and the dataset is opened read-only. Read with read_only=False.\n\n        See Also:\n            - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n            - Dataset.get_overview: Get an overview of a band.\n            - Dataset.overview_count: Number of overviews.\n            - Dataset.read_overview_array: Read overview values.\n            - Dataset.plot: Plot a band.\n        \"\"\"\n        if resampling_method.upper() not in RESAMPLING_METHODS:\n            raise ValueError(f\"resampling_method should be one of {RESAMPLING_METHODS}\")\n        # Build overviews using nearest neighbor resampling\n        # nearest is the resampling method used. Other methods include AVERAGE, GAUSS, etc.\n        try:\n            for i in range(self.band_count):\n                band = self._iloc(i)\n                for j in range(self.overview_count[i]):\n                    ovr = self.get_overview(i, j)\n                    # TODO: if this method takes a long time, we can use the gdal.RegenerateOverviews() method\n                    #  which is faster but it does not give the option to choose the resampling method. and the\n                    #  overviews has to be given to the function as a list.\n                    #  overviews = [band.GetOverview(i) for i in range(band.GetOverviewCount())]\n                    #  band.RegenerateOverviews(overviews) or gdal.RegenerateOverviews(overviews)\n                    gdal.RegenerateOverview(band, ovr, resampling_method)\n        except RuntimeError:\n            raise ReadOnlyError(\n                \"The Dataset is opened with a read only. Please read the dataset using read_only=False\"\n            )\n\n    def get_overview(self, band: int = 0, overview_index: int = 0) -&gt; gdal.Band:\n        \"\"\"Get an overview of a band.\n\n        Args:\n            band (int):\n                The band index. Defaults to 0.\n            overview_index (int):\n                Index of the overview. Defaults to 0.\n\n        Returns:\n            gdal.Band:\n                GDAL band object.\n\n        Examples:\n            - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n              &gt;&gt;&gt; print(arr[0, :, :]) # doctest: +SKIP\n              array([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n                     [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n                     [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n                     [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n                     [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n                     [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n                     [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n                     [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n                     [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n                     [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Now, create overviews using the default parameters and inspect them:\n\n              ```python\n              &gt;&gt;&gt; dataset.create_overviews()\n              &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n              [4, 4, 4, 4]\n\n              &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=0)\n              &gt;&gt;&gt; print(ovr)  # doctest: +SKIP\n              &lt;osgeo.gdal.Band; proxy of &lt;Swig Object of type 'GDALRasterBandShadow *' at 0x0000017E2B5AF1B0&gt; &gt;\n              &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n              array([[6, 3, 4, 4, 8],\n                     [5, 9, 7, 5, 6],\n                     [5, 3, 1, 7, 5],\n                     [1, 4, 2, 6, 9],\n                     [4, 3, 6, 5, 8]], dtype=int32)\n              &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=1)\n              &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n              array([[6, 7, 3],\n                     [2, 5, 6],\n                     [6, 9, 9]], dtype=int32)\n              &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=2)\n              &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n              array([[6, 8],\n                     [8, 5]], dtype=int32)\n              &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=3)\n              &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n              array([[6]], dtype=int32)\n\n              ```\n\n        See Also:\n            - Dataset.create_overviews: Create the dataset overviews if they exist.\n            - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n            - Dataset.overview_count: Number of overviews.\n            - Dataset.read_overview_array: Read overview values.\n            - Dataset.plot: Plot a band.\n        \"\"\"\n        band = self._iloc(band)\n        n_views = band.GetOverviewCount()\n        if n_views == 0:\n            raise ValueError(\n                \"The band has no overviews, please use the `create_overviews` method to build the overviews\"\n            )\n\n        if overview_index &gt;= n_views:\n            raise ValueError(f\"overview_level should be less than {n_views}\")\n\n        # TODO:find away to create a Dataset object from the overview band and to return the Dataset object instead\n        #  of the gdal band.\n        return band.GetOverview(overview_index)\n\n    def read_overview_array(\n        self, band: int = None, overview_index: int = 0\n    ) -&gt; np.ndarray:\n        \"\"\"Read overview values.\n\n            - Read the values stored in a given band or overview.\n\n        Args:\n            band (int | None):\n                The band to read. If None and multiple bands exist, reads all bands at the given overview.\n            overview_index (int):\n                Index of the overview. Defaults to 0.\n\n        Returns:\n            np.ndarray:\n                Array with the values in the raster.\n\n        Examples:\n            - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n              &gt;&gt;&gt; print(arr[0, :, :])     # doctest: +SKIP\n              array([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n                     [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n                     [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n                     [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n                     [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n                     [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n                     [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n                     [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n                     [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n                     [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Create overviews using the default parameters and read overview arrays:\n\n              ```python\n              &gt;&gt;&gt; dataset.create_overviews()\n              &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n              [4, 4, 4, 4]\n\n              &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=0)\n              &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n              array([[6, 3, 4, 4, 8],\n                     [5, 9, 7, 5, 6],\n                     [5, 3, 1, 7, 5],\n                     [1, 4, 2, 6, 9],\n                     [4, 3, 6, 5, 8]], dtype=int32)\n              &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=1)\n              &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n              array([[6, 7, 3],\n                     [2, 5, 6],\n                     [6, 9, 9]], dtype=int32)\n              &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=2)\n              &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n              array([[6, 8],\n                     [8, 5]], dtype=int32)\n              &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=3)\n              &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n              array([[6]], dtype=int32)\n\n              ```\n\n        See Also:\n            - Dataset.create_overviews: Create the dataset overviews.\n            - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n            - Dataset.get_overview: Get an overview of a band.\n            - Dataset.overview_count: Number of overviews.\n            - Dataset.plot: Plot a band.\n        \"\"\"\n        if band is None and self.band_count &gt; 1:\n            if any(elem == 0 for elem in self.overview_count):\n                raise ValueError(\n                    \"Some bands do not have overviews, please create overviews first\"\n                )\n            # read the array from the first overview to get the size of the array.\n            arr = self.get_overview(0, 0).ReadAsArray()\n            arr = np.ones(\n                (\n                    self.band_count,\n                    arr.shape[0],\n                    arr.shape[1],\n                ),\n                dtype=self.numpy_dtype[0],\n            )\n            for i in range(self.band_count):\n                arr[i, :, :] = self.get_overview(i, overview_index).ReadAsArray()\n        else:\n            if band is None:\n                band = 0\n            else:\n                if band &gt; self.band_count - 1:\n                    raise ValueError(\n                        f\"band index should be between 0 and {self.band_count - 1}\"\n                    )\n                if self.overview_count[band] == 0:\n                    raise ValueError(\n                        f\"band {band} has no overviews, please create overviews first\"\n                    )\n            arr = self.get_overview(band, overview_index).ReadAsArray()\n\n        return arr\n\n    @property\n    def band_color(self) -&gt; Dict[int, str]:\n        \"\"\"Band colors.\"\"\"\n        color_dict = {}\n        for i in range(self.band_count):\n            band_color = self._iloc(i).GetColorInterpretation()\n            band_color = band_color if band_color is not None else 0\n            color_dict[i] = gdal_constant_to_color_name(band_color)\n        return color_dict\n\n    @band_color.setter\n    def band_color(self, values: Dict[int, str]):\n        \"\"\"Assign color interpretation to dataset bands.\n\n        Args:\n            values (Dict[int, str]):\n                Dictionary with band index as key and color name as value.\n                e.g. {1: 'Red', 2: 'Green', 3: 'Blue'}. Possible values are\n                ['undefined', 'gray_index', 'palette_index', 'red', 'green', 'blue', 'alpha', 'hue', 'saturation',\n                'lightness', 'cyan', 'magenta', 'yellow', 'black', 'YCbCr_YBand', 'YCbCr_CbBand', 'YCbCr_CrBand']\n\n        Examples:\n            - Create `Dataset` consisting of 1 band, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Assign a color interpretation to the dataset band (i.e., gray, red, green, or blue) using a dictionary\n              with the band index as the key and the color interpretation as the value:\n\n              ```python\n              &gt;&gt;&gt; dataset.band_color = {0: 'gray_index'}\n\n              ```\n\n            - Assign RGB color interpretation to dataset bands:\n\n              ```python\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(3, 10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; dataset.band_color = {0: 'red', 1: 'green', 2: 'blue'}\n\n              ```\n        \"\"\"\n        for key, val in values.items():\n            if key &gt; self.band_count:\n                raise ValueError(\n                    f\"band index should be between 0 and {self.band_count}\"\n                )\n            gdal_const = color_name_to_gdal_constant(val)\n            self._iloc(key).SetColorInterpretation(gdal_const)\n\n    def get_band_by_color(self, color_name: str) -&gt; int:\n        \"\"\"Get the band associated with a given color.\n\n        Args:\n            color_name (str):\n                One of ['undefined', 'gray_index', 'palette_index', 'red', 'green', 'blue', 'alpha', 'hue',\n                'saturation', 'lightness', 'cyan', 'magenta', 'yellow', 'black', 'YCbCr_YBand', 'YCbCr_CbBand',\n                'YCbCr_CrBand'].\n\n        Returns:\n            int:\n                Band index.\n\n        Examples:\n            - Create `Dataset` consisting of 3 bands and assign RGB colors:\n\n              ```python\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(3, 10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n              &gt;&gt;&gt; dataset.band_color = {0: 'red', 1: 'green', 2: 'blue'}\n\n              ```\n\n            - Now use `get_band_by_color` to know which band is the red band, for example:\n\n              ```python\n              &gt;&gt;&gt; band_index = dataset.get_band_by_color('red')\n              &gt;&gt;&gt; print(band_index)\n              0\n\n              ```\n        \"\"\"\n        colors = list(self.band_color.values())\n        if color_name not in colors:\n            band_index = None\n        else:\n            band_index = colors.index(color_name)\n        return band_index\n\n    # TODO: find a better way to handle the color table in accordance with attribute_table\n    # and figure out how to take a color ramp and convert it to a color table.\n    # use the SetColorInterpretation method to assign the color (R/G/B) to a band.\n    @property\n    def color_table(self) -&gt; DataFrame:\n        \"\"\"Color table.\n\n        Returns:\n            DataFrame:\n                A DataFrame with columns: band, values, color.\n\n        Examples:\n            - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Set color table for band 1:\n\n              ```python\n              &gt;&gt;&gt; color_table = pd.DataFrame({\n              ...     \"band\": [1, 1, 1, 2, 2, 2],\n              ...     \"values\": [1, 2, 3, 1, 2, 3],\n              ...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"]\n              ... })\n              &gt;&gt;&gt; dataset.color_table = color_table\n              &gt;&gt;&gt; print(dataset.color_table)\n                band values  red green blue alpha\n              0    1      0    0     0    0     0\n              1    1      1  112   153   89   255\n              2    1      2  242   238  162   255\n              3    1      3  242   206  133   255\n              4    2      0    0     0    0     0\n              5    2      1  194   140  124   255\n              6    2      2  214   193  156   255\n              7    2      3  214   193  156   255\n\n              ```\n\n            - Define opacity per color by adding an 'alpha' column (0 transparent to 255 opaque). If 'alpha' is missing,\n              it will be assumed fully opaque (255):\n\n              ```python\n              &gt;&gt;&gt; color_table = pd.DataFrame({\n              ...     \"band\": [1, 1, 1, 2, 2, 2],\n              ...     \"values\": [1, 2, 3, 1, 2, 3],\n              ...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"],\n              ...     \"alpha\": [255, 128, 0, 255, 128, 0]\n              ... })\n              &gt;&gt;&gt; dataset.color_table = color_table\n              &gt;&gt;&gt; print(dataset.color_table)\n                band values  red green blue alpha\n              0    1      0    0     0    0     0\n              1    1      1  112   153   89   255\n              2    1      2  242   238  162   128\n              3    1      3  242   206  133     0\n              4    2      0    0     0    0     0\n              5    2      1  194   140  124   255\n              6    2      2  214   193  156   128\n              7    2      3  214   193  156     0\n\n              ```\n        \"\"\"\n        return self._get_color_table()\n\n    @color_table.setter\n    def color_table(self, df: DataFrame):\n        \"\"\"Get color table.\n\n        Args:\n            df (DataFrame):\n                DataFrame with columns: band, values, color. Example layout:\n                    ```python\n                    band  values    color  alpha\n                    0    1       1  #709959    255\n                    1    1       2  #F2EEA2    255\n                    2    1       3  #F2CE85    138\n                    3    2       1  #C28C7C    100\n                    4    2       2  #D6C19C    100\n                    5    2       3  #D6C19C    100\n\n                    ```\n\n        Examples:\n            - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; import pandas as pd\n              &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n              &gt;&gt;&gt; top_left_corner = (0, 0)\n              &gt;&gt;&gt; cell_size = 0.05\n              &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n              ```\n\n            - Set color table for band 1:\n\n              ```python\n              &gt;&gt;&gt; color_table = pd.DataFrame({\n              ...     \"band\": [1, 1, 1, 2, 2, 2],\n              ...     \"values\": [1, 2, 3, 1, 2, 3],\n              ...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"]\n              ... })\n              &gt;&gt;&gt; dataset.color_table = color_table\n              &gt;&gt;&gt; print(dataset.color_table)\n                band values  red green blue alpha\n              0    1      0    0     0    0     0\n              1    1      1  112   153   89   255\n              2    1      2  242   238  162   255\n              3    1      3  242   206  133   255\n              4    2      0    0     0    0     0\n              5    2      1  194   140  124   255\n              6    2      2  214   193  156   255\n              7    2      3  214   193  156   255\n\n              ```\n\n            - You can also define the opacity of each color by adding a value between 0 (fully transparent) and 255 (fully opaque)\n              to the DataFrame for each color. If the 'alpha' column is not present, it will be assumed to be fully opaque (255):\n\n              ```python\n              &gt;&gt;&gt; color_table = pd.DataFrame({\n              ...     \"band\": [1, 1, 1, 2, 2, 2],\n              ...     \"values\": [1, 2, 3, 1, 2, 3],\n              ...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"],\n              ...     \"alpha\": [255, 128 0, 255, 128 0]\n              ... })\n              &gt;&gt;&gt; dataset.color_table = color_table\n              &gt;&gt;&gt; print(dataset.color_table)\n                band values  red green blue alpha\n              0    1      0    0     0    0     0\n              1    1      1  112   153   89   255\n              2    1      2  242   238  162   128\n              3    1      3  242   206  133     0\n              4    2      0    0     0    0     0\n              5    2      1  194   140  124   255\n              6    2      2  214   193  156   128\n              7    2      3  214   193  156     0\n\n              ```\n        \"\"\"\n        if not isinstance(df, DataFrame):\n            raise TypeError(f\"df should be a DataFrame not {type(df)}\")\n\n        if not {\"band\", \"values\", \"color\"}.issubset(df.columns):\n            raise ValueError(  # noqa\n                \"df should have the following columns: band, values, color\"\n            )\n\n        self._set_color_table(df, overwrite=True)\n\n    def _set_color_table(self, color_df: DataFrame, overwrite: bool = False):\n        \"\"\"_set_color_table.\n\n        Args:\n            color_df (DataFrame):\n                DataFrame with columns: band, values, color. Example:\n                ```python\n                band  values    color\n                0    1       1  #709959\n                1    1       2  #F2EEA2\n                2    1       3  #F2CE85\n                3    2       1  #C28C7C\n                4    2       2  #D6C19C\n                5    2       3  #D6C19C\n\n                ```\n            overwrite (bool):\n                True to overwrite the existing color table. Default is False.\n        \"\"\"\n        import_cleopatra(\n            \"The current function uses cleopatra package to for plotting, please install it manually, for more info\"\n            \" check https://github.com/Serapieum-of-alex/cleopatra\"\n        )\n        from cleopatra.colors import Colors\n\n        color = Colors(color_df[\"color\"].tolist())\n        color_rgb = color.to_rgb(normalized=False)\n        color_df = color_df.copy(deep=True)\n        color_df.loc[:, [\"red\", \"green\", \"blue\"]] = color_rgb\n\n        if \"alpha\" not in color_df.columns:\n            color_df.loc[:, \"alpha\"] = 255\n\n        for band_i, df_band in color_df.groupby(\"band\"):\n            band = self.raster.GetRasterBand(band_i)\n\n            if overwrite:\n                color_table = gdal.ColorTable()\n            else:\n                color_table = band.GetColorTable()\n\n            for i, row in df_band.iterrows():\n                color_table.SetColorEntry(\n                    row[\"values\"], (row[\"red\"], row[\"green\"], row[\"blue\"], row[\"alpha\"])\n                )\n\n            band.SetColorTable(color_table)\n            # band.SetRasterColorInterpretation(gdal.GCI_PaletteIndex)\n\n    def _get_color_table(self, band: int = None) -&gt; DataFrame:\n        \"\"\"Get color table.\n\n        Args:\n            band (int, optional):\n                Band index. Default is None.\n\n        Returns:\n            pandas.DataFrame:\n                A DataFrame with columns [\"band\", \"values\", \"red\", \"green\", \"blue\", \"alpha\"] describing the color table.\n        \"\"\"\n        df = pd.DataFrame(columns=[\"band\", \"values\", \"red\", \"green\", \"blue\", \"alpha\"])\n        bands = range(self.band_count) if band is None else band\n        row = 0\n        for band in bands:\n            color_table = self.raster.GetRasterBand(band + 1).GetRasterColorTable()\n            for i in range(color_table.GetCount()):\n                df.loc[row, [\"red\", \"green\", \"blue\", \"alpha\"]] = (\n                    color_table.GetColorEntry(i)\n                )\n                df.loc[row, [\"band\", \"values\"]] = band + 1, i\n                row += 1\n\n        return df\n\n    def get_histogram(\n        self,\n        band: int = 0,\n        bins: int = 6,\n        min_value: float = None,\n        max_value: float = None,\n        include_out_of_range: bool = False,\n        approx_ok: bool = False,\n    ) -&gt; tuple[list, list[tuple[Any, Any]]]:\n        \"\"\"Get histogram.\n\n        Args:\n            band (int, optional):\n                Band index. Default is 1.\n            bins (int, optional):\n                Number of bins. Default is 6.\n            min_value (float, optional):\n                Minimum value. Default is None.\n            max_value (float, optional):\n                Maximum value. Default is None.\n            include_out_of_range (bool, optional):\n                If True, add out-of-range values into the first and last buckets. Default is False.\n            approx_ok (bool, optional):\n                If True, compute an approximate histogram by using subsampling or overviews. Default is False.\n\n        Returns:\n            tuple[list, list[tuple[Any, Any]]]:\n                Histogram values and bin edges.\n\n        Hint:\n            - The value of the histogram will be stored in an xml file by the name of the raster file with the extension\n                of .aux.xml.\n\n            - The content of the file will be like the following:\n              ```xml\n\n                  &lt;PAMDataset&gt;\n                    &lt;PAMRasterBand band=\"1\"&gt;\n                      &lt;Description&gt;Band_1&lt;/Description&gt;\n                      &lt;Histograms&gt;\n                        &lt;HistItem&gt;\n                          &lt;HistMin&gt;0&lt;/HistMin&gt;\n                          &lt;HistMax&gt;88&lt;/HistMax&gt;\n                          &lt;BucketCount&gt;6&lt;/BucketCount&gt;\n                          &lt;IncludeOutOfRange&gt;0&lt;/IncludeOutOfRange&gt;\n                          &lt;Approximate&gt;0&lt;/Approximate&gt;\n                          &lt;HistCounts&gt;75|6|0|4|2|1&lt;/HistCounts&gt;\n                        &lt;/HistItem&gt;\n                      &lt;/Histograms&gt;\n                    &lt;/PAMRasterBand&gt;\n                  &lt;/PAMDataset&gt;\n\n              ```\n\n        Examples:\n            - Create `Dataset` consists of 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).\n\n              ```python\n              &gt;&gt;&gt; import numpy as np\n              &gt;&gt;&gt; arr = np.random.randint(1, 12, size=(10, 10))\n              &gt;&gt;&gt; print(arr)    # doctest: +SKIP\n              [[ 4  1  1  2  6  9  2  5  1  8]\n               [ 1 11  5  6  2  5  4  6  6  7]\n               [ 5  2 10  4  8 11  4 11 11  1]\n               [ 2  3  6  3  1  5 11 10 10  7]\n               [ 8  2 11  3  1  3  5  4 10 10]\n               [ 1  2  1  6 10  3  6  4  2  8]\n               [ 9  5  7  9  7  8  1 11  4  4]\n               [ 7  7  2  2  5  3  7  2  9  9]\n               [ 2 10  3  2  1 11  5  9  8 11]\n               [ 1  5  6 11  3  3  8  1  2  1]]\n               &gt;&gt;&gt; top_left_corner = (0, 0)\n               &gt;&gt;&gt; cell_size = 0.05\n               &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n               ```\n\n            - Now, let's get the histogram of the first band using the `get_histogram` method with the default\n                parameters:\n                ```python\n                &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0)\n                &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n                [28, 17, 10, 15, 13, 7]\n                &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n                [(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n\n                ```\n            - we can also exclude values from the histogram by using the `min_value` and `max_value`:\n                ```python\n                &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, min_value=5, max_value=10)\n                &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n                [10, 8, 7, 7, 6, 0]\n                &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n                [(1.0, 1.835), (1.835, 2.67), (2.67, 3.5), (3.5, 4.34), (4.34, 5.167), (5.167, 6.0)]\n\n                ```\n            - For datasets with big dimensions, computing the histogram can take some time; approximating the computation\n                of the histogram can save a lot of computation time. When using the parameter `approx_ok` with a `True`\n                value the histogram will be calculated from resampling the band or from the overviews if they exist.\n                ```python\n                &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, approx_ok=True)\n                &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n                [28, 17, 10, 15, 13, 7]\n                &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n                [(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n\n                ```\n            - As you see for small datasets, the approximation of the histogram will be the same as without approximation.\n\n        \"\"\"\n        band = self._iloc(band)\n        min_val, max_val = band.ComputeRasterMinMax()\n        if min_value is None:\n            min_value = min_val\n        if max_value is None:\n            max_value = max_val\n\n        bin_width = (max_value - min_value) / bins\n        ranges = [\n            (min_val + i * bin_width, min_val + (i + 1) * bin_width)\n            for i in range(bins)\n        ]\n\n        hist = band.GetHistogram(\n            min=min_value,\n            max=max_value,\n            buckets=bins,\n            include_out_of_range=include_out_of_range,\n            approx_ok=approx_ok,\n        )\n        return hist, ranges\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.access","title":"<code>access</code>  <code>property</code>","text":"<p>Access mode.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The access mode of the dataset (read_only/write).</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.raster","title":"<code>raster</code>  <code>property</code> <code>writable</code>","text":"<p>Base GDAL Dataset.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.values","title":"<code>values</code>  <code>property</code>","text":"<p>Values of all the bands.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: the values of all the bands in the raster as a 3D numpy array (bands, rows, columns).</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.rows","title":"<code>rows</code>  <code>property</code>","text":"<p>Number of rows in the raster array.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Number of columns in the raster array.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape (bands, rows, columns).</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.geotransform","title":"<code>geotransform</code>  <code>property</code>","text":"<p>WKT projection.</p> <p>(top left corner X/lon coordinate, cell_size, 0, top left corner y/lat coordinate, 0, -cell_size).</p> <p>Examples:</p> <ul> <li>Create a dataset:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>To check the geotransform of the dataset, call the <code>geotransform</code> property:</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.geotransform)\n(0.0, 0.05, 0.0, 0.0, 0.0, -0.05)\n</code></pre> See Also <ul> <li>Dataset.top_left_corner: Coordinate of the top left corner of the dataset.</li> <li>Dataset.epsg: EPSG number of the dataset coordinate reference system.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.epsg","title":"<code>epsg</code>  <code>property</code> <code>writable</code>","text":"<p>EPSG number.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.crs","title":"<code>crs</code>  <code>property</code> <code>writable</code>","text":"<p>Coordinate reference system.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the coordinate reference system of the dataset.</p> <p>Examples:</p> <ul> <li>Create a dataset:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Now, to check the coordinate reference system, call the <code>crs</code> property:</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.crs)\nGEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.cell_size","title":"<code>cell_size</code>  <code>property</code>","text":"<p>Cell size.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.band_count","title":"<code>band_count</code>  <code>property</code>","text":"<p>Number of bands in the raster.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.band_names","title":"<code>band_names</code>  <code>property</code> <code>writable</code>","text":"<p>Band names.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.band_units","title":"<code>band_units</code>  <code>property</code> <code>writable</code>","text":"<p>Band units.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.no_data_value","title":"<code>no_data_value</code>  <code>property</code> <code>writable</code>","text":"<p>No data value that marks the cells out of the domain.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.meta_data","title":"<code>meta_data</code>  <code>property</code> <code>writable</code>","text":"<p>Meta-data.</p> Hint <ul> <li>This property does not need the Dataset to be opened in a write mode to be set.</li> <li>The value of the offset will be stored in an xml file by the name of the raster file with the extension of     .aux.xml,</li> </ul> <p>the content of the file will be like the following:</p> <pre><code>    &lt;PAMDataset&gt;\n      &lt;Metadata&gt;\n        &lt;MDI key=\"key\"&gt;value&lt;/MDI&gt;\n      &lt;/Metadata&gt;\n    &lt;/PAMDataset&gt;\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.block_size","title":"<code>block_size</code>  <code>property</code> <code>writable</code>","text":"<p>Block Size.</p> <p>The block size is the size of the block that the raster is divided into, the block size is used to read and write the raster data in blocks.</p> <p>Examples:</p> <ul> <li>Create a dataset and print block size:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset.block_size)\n[[5, 1]]\n</code></pre> See Also <ul> <li>Dataset.get_block_arrangement: Get block arrangement to read the dataset in chunks.</li> <li>Dataset.get_tile: Get tiles.</li> <li>Dataset.read_array: Read the data stored in the dataset bands.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.file_name","title":"<code>file_name</code>  <code>property</code>","text":"<p>File name.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.driver_type","title":"<code>driver_type</code>  <code>property</code>","text":"<p>Driver Type.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.scale","title":"<code>scale</code>  <code>property</code> <code>writable</code>","text":"<p>Scale.</p> <p>The value of the scale is used to convert the pixel values to the real-world values.</p> Hint <ul> <li>This property does not need the Dataset to be opened in a write mode to be set.</li> <li>The value of the offset will be stored in an xml file by the name of the raster file with the extension of     .aux.xml.</li> </ul> <p>the content of the file will be like the following:</p> <pre><code>    &lt;PAMDataset&gt;\n      &lt;PAMRasterBand band=\"1\"&gt;\n        &lt;Description&gt;Band_1&lt;/Description&gt;\n        &lt;UnitType&gt;m&lt;/UnitType&gt;\n        &lt;Offset&gt;100&lt;/Offset&gt;\n        &lt;Scale&gt;2&lt;/Scale&gt;\n      &lt;/PAMRasterBand&gt;\n    &lt;/PAMDataset&gt;\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.offset","title":"<code>offset</code>  <code>property</code> <code>writable</code>","text":"<p>Offset.</p> <p>The value of the offset is used to convert the pixel values to the real-world values.</p> Hint <ul> <li>This property does not need the Dataset to be opened in a write mode to be set.</li> <li>The value of the offset will be stored in xml file by the name of the raster file with the extension of     .aux.xml.</li> </ul> <p>the content of the file will be like the following:</p> <pre><code>    &lt;PAMDataset&gt;\n      &lt;PAMRasterBand band=\"1\"&gt;\n        &lt;Description&gt;Band_1&lt;/Description&gt;\n        &lt;UnitType&gt;m&lt;/UnitType&gt;\n        &lt;Offset&gt;100&lt;/Offset&gt;\n        &lt;Scale&gt;2&lt;/Scale&gt;\n      &lt;/PAMRasterBand&gt;\n    &lt;/PAMDataset&gt;\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.top_left_corner","title":"<code>top_left_corner</code>  <code>property</code>","text":"<p>Top left corner coordinates.</p> See Also <ul> <li>Dataset.geotransform: Dataset geotransform.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Bounds - the bbox as a geodataframe with a polygon geometry.</p> <p>Examples:</p> <ul> <li>Create a Dataset (1 band, 10 rows, 10 columns) at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the bounds of the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; bounds = dataset.bounds\n&gt;&gt;&gt; print(bounds) # doctest: +SKIP\n                                        geometry\n0  POLYGON ((0 0, 0 -0.5, 0.5 -0.5, 0.5 0, 0 0))\n</code></pre> See Also <ul> <li>Dataset.bbox:     Dataset bounding box.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.bbox","title":"<code>bbox</code>  <code>property</code>","text":"<p>Bound box [xmin, ymin, xmax, ymax].</p> <p>Examples:</p> <ul> <li>Create a Dataset (1 band, 10 rows, 10 columns) at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the bounding box of the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; bbox = dataset.bbox\n&gt;&gt;&gt; print(bbox) # doctest: +SKIP\n[0.0, -0.5, 0.5, 0.0]\n</code></pre> See Also <ul> <li>Dataset.bounds: Dataset bounding polygon.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.lon","title":"<code>lon</code>  <code>property</code>","text":"<p>Longitude coordinates.</p> <p>Examples:</p> <ul> <li> <p>Create a Dataset (1 band, 5 rows, 5 columns) at lon/lat (0, 0):   <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre></p> </li> <li> <p>Get the longitude/x coordinates of the center of all cells in the dataset:</p> </li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.lon)\n[0.025 0.075 0.125 0.175 0.225]\n&gt;&gt;&gt; print(dataset.x)\n[0.025 0.075 0.125 0.175 0.225]\n</code></pre> See Also <ul> <li>Dataset.x: Dataset x coordinates.</li> <li>Dataset.lat: Dataset latitude.</li> <li>Dataset.lon: Dataset longitude.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.lat","title":"<code>lat</code>  <code>property</code>","text":"<p>Latitude-coordinate.</p> <p>Examples:</p> <ul> <li>Create a Dataset (1 band, 5 rows, 5 columns) at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the latitude/y coordinates of the center of all cells in the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.lat)\n[-0.025 -0.075 -0.125 -0.175 -0.225]\n&gt;&gt;&gt; print(dataset.y)\n[-0.025 -0.075 -0.125 -0.175 -0.225]\n</code></pre> See Also <ul> <li>Dataset.x: Dataset x coordinates.</li> <li>Dataset.y: Dataset y coordinates.</li> <li>Dataset.lon: Dataset longitude.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.x","title":"<code>x</code>  <code>property</code>","text":"<p>X-coordinate/Longitude.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0).</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>To get the longitude/x coordinates of the center of all cells in the dataset.</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.lon)\n[0.025 0.075 0.125 0.175 0.225]\n&gt;&gt;&gt; print(dataset.x)\n[0.025 0.075 0.125 0.175 0.225]\n</code></pre> See Also <ul> <li>Dataset.lat: Dataset latitude.</li> <li>Dataset.y: Dataset y coordinates.</li> <li>Dataset.lon: Dataset longitude.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.y","title":"<code>y</code>  <code>property</code>","text":"<p>Y-coordinate/Latitude.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0).</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,2, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>to get the longitude/x coordinates of the center of all cells in the dataset.</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.lat)\n[-0.025 -0.075 -0.125 -0.175 -0.225]\n&gt;&gt;&gt; print(dataset.y)\n[-0.025 -0.075 -0.125 -0.175 -0.225]\n</code></pre> See Also <ul> <li>Dataset.x: Dataset y coordinates.</li> <li>Dataset.lat: Dataset latitude.</li> <li>Dataset.lon: Dataset longitude.</li> </ul>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.gdal_dtype","title":"<code>gdal_dtype</code>  <code>property</code>","text":"<p>Data Type.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.numpy_dtype","title":"<code>numpy_dtype</code>  <code>property</code>","text":"<p>List of the numpy data Type of each band, the data type is a numpy function.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>List of the data Type of each band as strings.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.overview_count","title":"<code>overview_count</code>  <code>property</code>","text":"<p>Number of the overviews for each band.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.band_color","title":"<code>band_color</code>  <code>property</code> <code>writable</code>","text":"<p>Band colors.</p>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.color_table","title":"<code>color_table</code>  <code>property</code> <code>writable</code>","text":"<p>Color table.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with columns: band, values, color.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Set color table for band 1:</li> </ul> <pre><code>&gt;&gt;&gt; color_table = pd.DataFrame({\n...     \"band\": [1, 1, 1, 2, 2, 2],\n...     \"values\": [1, 2, 3, 1, 2, 3],\n...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"]\n... })\n&gt;&gt;&gt; dataset.color_table = color_table\n&gt;&gt;&gt; print(dataset.color_table)\n  band values  red green blue alpha\n0    1      0    0     0    0     0\n1    1      1  112   153   89   255\n2    1      2  242   238  162   255\n3    1      3  242   206  133   255\n4    2      0    0     0    0     0\n5    2      1  194   140  124   255\n6    2      2  214   193  156   255\n7    2      3  214   193  156   255\n</code></pre> <ul> <li>Define opacity per color by adding an 'alpha' column (0 transparent to 255 opaque). If 'alpha' is missing,   it will be assumed fully opaque (255):</li> </ul> <pre><code>&gt;&gt;&gt; color_table = pd.DataFrame({\n...     \"band\": [1, 1, 1, 2, 2, 2],\n...     \"values\": [1, 2, 3, 1, 2, 3],\n...     \"color\": [\"#709959\", \"#F2EEA2\", \"#F2CE85\", \"#C28C7C\", \"#D6C19C\", \"#D6C19C\"],\n...     \"alpha\": [255, 128, 0, 255, 128, 0]\n... })\n&gt;&gt;&gt; dataset.color_table = color_table\n&gt;&gt;&gt; print(dataset.color_table)\n  band values  red green blue alpha\n0    1      0    0     0    0     0\n1    1      1  112   153   89   255\n2    1      2  242   238  162   128\n3    1      3  242   206  133     0\n4    2      0    0     0    0     0\n5    2      1  194   140  124   255\n6    2      2  214   193  156   128\n7    2      3  214   193  156     0\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.__init__","title":"<code>__init__(src, access='read_only')</code>","text":"<p>init.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def __init__(self, src: gdal.Dataset, access: str = \"read_only\"):\n    \"\"\"__init__.\"\"\"\n    self.logger = logging.getLogger(__name__)\n    super().__init__(src, access=access)\n\n    self._no_data_value = [\n        src.GetRasterBand(i).GetNoDataValue() for i in range(1, self.band_count + 1)\n    ]\n    self._band_names = self._get_band_names()\n    self._band_units = [\n        src.GetRasterBand(i).GetUnitType() for i in range(1, self.band_count + 1)\n    ]\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.__str__","title":"<code>__str__()</code>","text":"<p>str.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def __str__(self):\n    \"\"\"__str__.\"\"\"\n    message = f\"\"\"\n        Cell size: {self.cell_size}\n        Dimension: {self.rows} * {self.columns}\n        EPSG: {self.epsg}\n        Number of Bands: {self.band_count}\n        Band names: {self.band_names}\n        Mask: {self._no_data_value[0]}\n        Data type: {self.dtype[0]}\n        File: {self.file_name}\n    \"\"\"\n    return message\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.__repr__","title":"<code>__repr__()</code>","text":"<p>repr.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def __repr__(self):\n    \"\"\"__repr__.\"\"\"\n    message = \"\"\"\n        Cell size: {0}\n        Dimension: {1} * {2}\n        EPSG: {3}\n        Number of Bands: {4}\n        Band names: {5}\n        Mask: {6}\n        Data type: {7}\n        projection: {8}\n        Metadata: {9}\n        File: {10}\n    \"\"\".format(\n        self.cell_size,\n        self.rows,\n        self.columns,\n        self.epsg,\n        self.band_count,\n        self.band_names,\n        (\n            self._no_data_value\n            if self._no_data_value == []\n            else self._no_data_value[0]\n        ),\n        self.dtype if self.dtype == [] else self.dtype[0],\n        self.crs,\n        self.meta_data,\n        self.file_name,\n    )\n    return message\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.read_file","title":"<code>read_file(path, read_only=True, file_i=0)</code>  <code>classmethod</code>","text":"<p>read_file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of file to open.</p> required <code>read_only</code> <code>bool</code> <p>File mode, set to False, to open in \"update\" mode.</p> <code>True</code> <code>file_i</code> <code>int</code> <p>Index to the file inside the compressed file you want to read, if the compressed file has only one file. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Opened dataset instance.</p> <p>Examples:</p> <p>Zip files: - Internal Zip file path (one/multiple files inside the compressed file):     if the path contains a zip but does not end with zip (compressed-file-name.zip/1.asc), so the path contains         the internal path inside the zip file, so just ad</p> <pre><code>```python\n&gt;&gt;&gt; rdir = \"tests/data/virtual-file-system\"\n&gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip/1.asc\")\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 4000.0\n            Dimension: 13 * 14\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -3.4028230607370965e+38\n            Data type: float32\n            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n&lt;BLANKLINE&gt;\n\n```\n</code></pre> <ul> <li> <p>Only the Zip file path (one/multiple files inside the compressed file):     If you provide the name of the zip file with multiple files inside it, it will return the path to the first     file.</p> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\")\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 4000.0\n            Dimension: 13 * 14\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -3.4028230607370965e+38\n            Data type: float32\n            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n&lt;BLANKLINE&gt;\n</code></pre> </li> <li> <p>Zip file path and an index (one/multiple files inside the compressed file):     if you provide the path to the zip file and an index to the file inside the compressed file you want to     read.</p> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\", file_i=1)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 4000.0\n            Dimension: 13 * 14\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -3.4028230607370965e+38\n            Data type: float32\n            File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/2.asc\n&lt;BLANKLINE&gt;\n</code></pre> </li> </ul> See Also <ul> <li>Dataset.read_array: Read the values stored in a dataset band.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>@classmethod\ndef read_file(\n    cls,\n    path: str,\n    read_only=True,\n    file_i: int = 0,\n) -&gt; \"Dataset\":\n    \"\"\"read_file.\n\n    Args:\n        path (str):\n            Path of file to open.\n        read_only (bool):\n            File mode, set to False, to open in \"update\" mode.\n        file_i (int):\n            Index to the file inside the compressed file you want to read, if the compressed file has only one file. Default is 0.\n\n    Returns:\n        Dataset:\n            Opened dataset instance.\n\n    Examples:\n        Zip files:\n        - Internal Zip file path (one/multiple files inside the compressed file):\n            if the path contains a zip but does not end with zip (compressed-file-name.zip/1.asc), so the path contains\n                the internal path inside the zip file, so just ad\n\n\n            ```python\n            &gt;&gt;&gt; rdir = \"tests/data/virtual-file-system\"\n            &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip/1.asc\")\n            &gt;&gt;&gt; print(dataset)\n            &lt;BLANKLINE&gt;\n                        Cell size: 4000.0\n                        Dimension: 13 * 14\n                        EPSG: 4326\n                        Number of Bands: 1\n                        Band names: ['Band_1']\n                        Mask: -3.4028230607370965e+38\n                        Data type: float32\n                        File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n            &lt;BLANKLINE&gt;\n\n            ```\n\n        - Only the Zip file path (one/multiple files inside the compressed file):\n            If you provide the name of the zip file with multiple files inside it, it will return the path to the first\n            file.\n\n\n            ```python\n            &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\")\n            &gt;&gt;&gt; print(dataset)\n            &lt;BLANKLINE&gt;\n                        Cell size: 4000.0\n                        Dimension: 13 * 14\n                        EPSG: 4326\n                        Number of Bands: 1\n                        Band names: ['Band_1']\n                        Mask: -3.4028230607370965e+38\n                        Data type: float32\n                        File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/1.asc\n            &lt;BLANKLINE&gt;\n\n            ```\n\n        - Zip file path and an index (one/multiple files inside the compressed file):\n            if you provide the path to the zip file and an index to the file inside the compressed file you want to\n            read.\n\n\n            ```python\n            &gt;&gt;&gt; dataset = Dataset.read_file(f\"{rdir}/multiple_compressed_files.zip\", file_i=1)\n            &gt;&gt;&gt; print(dataset)\n            &lt;BLANKLINE&gt;\n                        Cell size: 4000.0\n                        Dimension: 13 * 14\n                        EPSG: 4326\n                        Number of Bands: 1\n                        Band names: ['Band_1']\n                        Mask: -3.4028230607370965e+38\n                        Data type: float32\n                        File: /vsizip/tests/data/virtual-file-system/multiple_compressed_files.zip/2.asc\n            &lt;BLANKLINE&gt;\n\n            ```\n\n    See Also:\n        - Dataset.read_array: Read the values stored in a dataset band.\n    \"\"\"\n    src = _io.read_file(path, read_only=read_only, file_i=file_i)\n    return cls(src, access=\"read_only\" if read_only else \"write\")\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.read_array","title":"<code>read_array(band=None, window=None)</code>","text":"<p>Read the values stored in a given band.</p> <p>Data Chuncks/blocks     When a raster dataset is stored on disk, it might not be stored as one continuous chunk of data. Instead,     it can be divided into smaller rectangular blocks or tiles. These blocks can be individually accessed,     which is particularly useful for large datasets:</p> <pre><code>    - Efficiency: Reading or writing small blocks requires less memory than dealing with the entire dataset\n          at once. This is especially beneficial when only a small portion of the data needs to be processed.\n    - Performance: For certain file formats and operations, working with optimal block sizes can significantly\n          improve performance. For example, if the block size matches the reading or processing window,\n          Pyramids can minimize disk access and data transfer.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>The band you want to get its data. If None, data of all bands will be read. Default is None.</p> <code>None</code> <code>window</code> <code>List[int] | GeoDataFrame</code> <p>Specify a block of data to read from the dataset. The window can be specified in two ways:</p> <ul> <li> <p>List:     Window specified as a list of 4 integers [offset_x, offset_y, window_columns, window_rows].</p> <ul> <li>offset_x/column index: x offset of the block.</li> <li>offset_y/row index: y offset of the block.</li> <li>window_columns: number of columns in the block.</li> <li>window_rows: number of rows in the block.</li> </ul> </li> <li> <p>GeoDataFrame:     GeoDataFrame with a geometry column filled with polygon geometries; the function will get the     total_bounds of the GeoDataFrame and use it as a window to read the raster.</p> </li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: array with all the values in the raster.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 4 bands, 5 rows, and 5 columns at the point lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Read all the values stored in a given band:</li> </ul> <pre><code>&gt;&gt;&gt; arr = dataset.read_array(band=0) # doctest: +SKIP\narray([[0.50482225, 0.45678043, 0.53294294, 0.28862223, 0.66753579],\n       [0.38471912, 0.14617829, 0.05045189, 0.00761358, 0.25501918],\n       [0.32689036, 0.37358843, 0.32233918, 0.75450564, 0.45197608],\n       [0.22944676, 0.2780928 , 0.71605189, 0.71859309, 0.61896933],\n       [0.47740168, 0.76490779, 0.07679277, 0.16142599, 0.73630836]])\n</code></pre> <ul> <li>Read a 2x2 block from the first band. The block starts at the 2nd column (index 1) and 2nd row (index 1)     (the first index is the column index):</li> </ul> <pre><code>&gt;&gt;&gt; arr = dataset.read_array(band=0, window=[1, 1, 2, 2])\n&gt;&gt;&gt; print(arr) # doctest: +SKIP\narray([[0.14617829, 0.05045189],\n       [0.37358843, 0.32233918]])\n</code></pre> <ul> <li> <p>If you check the values of the 2x2 block, you will find them the same as the values in the entire array     of band 0, starting at the 2nd row and 2nd column.</p> </li> <li> <p>Read a block using a GeoDataFrame polygon that covers the same area as the window above:</p> </li> </ul> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; poly = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n&gt;&gt;&gt; arr = dataset.read_array(band=0, window=poly)\n&gt;&gt;&gt; print(arr) # doctest: +SKIP\narray([[0.14617829, 0.05045189],\n       [0.37358843, 0.32233918]])\n</code></pre> See Also <ul> <li>Dataset.get_tile: Read the dataset in chunks.</li> <li>Dataset.get_block_arrangement: Get block arrangement to read the dataset in chunks.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def read_array(\n    self, band: int = None, window: Union[GeoDataFrame, List[int]] = None\n) -&gt; np.ndarray:\n    \"\"\"Read the values stored in a given band.\n\n    Data Chuncks/blocks\n        When a raster dataset is stored on disk, it might not be stored as one continuous chunk of data. Instead,\n        it can be divided into smaller rectangular blocks or tiles. These blocks can be individually accessed,\n        which is particularly useful for large datasets:\n\n            - Efficiency: Reading or writing small blocks requires less memory than dealing with the entire dataset\n                  at once. This is especially beneficial when only a small portion of the data needs to be processed.\n            - Performance: For certain file formats and operations, working with optimal block sizes can significantly\n                  improve performance. For example, if the block size matches the reading or processing window,\n                  Pyramids can minimize disk access and data transfer.\n\n    Args:\n        band (int, optional):\n            The band you want to get its data. If None, data of all bands will be read. Default is None.\n        window (List[int] | GeoDataFrame, optional):\n            Specify a block of data to read from the dataset. The window can be specified in two ways:\n\n            - List:\n                Window specified as a list of 4 integers [offset_x, offset_y, window_columns, window_rows].\n\n                - offset_x/column index: x offset of the block.\n                - offset_y/row index: y offset of the block.\n                - window_columns: number of columns in the block.\n                - window_rows: number of rows in the block.\n\n            - GeoDataFrame:\n                GeoDataFrame with a geometry column filled with polygon geometries; the function will get the\n                total_bounds of the GeoDataFrame and use it as a window to read the raster.\n\n    Returns:\n        np.ndarray:\n            array with all the values in the raster.\n\n    Examples:\n        - Create `Dataset` consisting of 4 bands, 5 rows, and 5 columns at the point lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Read all the values stored in a given band:\n\n          ```python\n          &gt;&gt;&gt; arr = dataset.read_array(band=0) # doctest: +SKIP\n          array([[0.50482225, 0.45678043, 0.53294294, 0.28862223, 0.66753579],\n                 [0.38471912, 0.14617829, 0.05045189, 0.00761358, 0.25501918],\n                 [0.32689036, 0.37358843, 0.32233918, 0.75450564, 0.45197608],\n                 [0.22944676, 0.2780928 , 0.71605189, 0.71859309, 0.61896933],\n                 [0.47740168, 0.76490779, 0.07679277, 0.16142599, 0.73630836]])\n\n          ```\n\n        - Read a 2x2 block from the first band. The block starts at the 2nd column (index 1) and 2nd row (index 1)\n            (the first index is the column index):\n\n          ```python\n          &gt;&gt;&gt; arr = dataset.read_array(band=0, window=[1, 1, 2, 2])\n          &gt;&gt;&gt; print(arr) # doctest: +SKIP\n          array([[0.14617829, 0.05045189],\n                 [0.37358843, 0.32233918]])\n\n          ```\n\n        - If you check the values of the 2x2 block, you will find them the same as the values in the entire array\n            of band 0, starting at the 2nd row and 2nd column.\n\n        - Read a block using a GeoDataFrame polygon that covers the same area as the window above:\n\n          ```python\n          &gt;&gt;&gt; import geopandas as gpd\n          &gt;&gt;&gt; from shapely.geometry import Polygon\n          &gt;&gt;&gt; poly = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n          &gt;&gt;&gt; arr = dataset.read_array(band=0, window=poly)\n          &gt;&gt;&gt; print(arr) # doctest: +SKIP\n          array([[0.14617829, 0.05045189],\n                 [0.37358843, 0.32233918]])\n\n          ```\n\n    See Also:\n        - Dataset.get_tile: Read the dataset in chunks.\n        - Dataset.get_block_arrangement: Get block arrangement to read the dataset in chunks.\n    \"\"\"\n    if band is None and self.band_count &gt; 1:\n        rows = self.rows if window is None else window[3]\n        columns = self.columns if window is None else window[2]\n        arr = np.ones(\n            (\n                self.band_count,\n                rows,\n                columns,\n            ),\n            dtype=self.numpy_dtype[0],\n        )\n\n        for i in range(self.band_count):\n            if window is None:\n                # this line could be replaced with the following line\n                # arr[i, :, :] = self._iloc(i).ReadAsArray()\n                arr[i, :, :] = self._raster.GetRasterBand(i + 1).ReadAsArray()\n            else:\n                arr[i, :, :] = self._read_block(i, window)\n    else:\n        # given band number or the raster has only one band\n        if band is None:\n            band = 0\n        else:\n            if band &gt; self.band_count - 1:\n                raise ValueError(\n                    f\"band index should be between 0 and {self.band_count - 1}\"\n                )\n        if window is None:\n            arr = self._iloc(band).ReadAsArray()\n        else:\n            arr = self._read_block(band, window)\n\n    return arr\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_x_lon_dimension_array","title":"<code>get_x_lon_dimension_array(pivot_x, cell_size, columns)</code>  <code>staticmethod</code>","text":"<p>Get X/Lon coordinates.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>@staticmethod\ndef get_x_lon_dimension_array(pivot_x, cell_size, columns) -&gt; List[float]:\n    \"\"\"Get X/Lon coordinates.\"\"\"\n    x_coords = [pivot_x + i * cell_size + cell_size / 2 for i in range(columns)]\n    return x_coords\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_y_lat_dimension_array","title":"<code>get_y_lat_dimension_array(pivot_y, cell_size, rows)</code>  <code>staticmethod</code>","text":"<p>Get Y/Lat coordinates.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>@staticmethod\ndef get_y_lat_dimension_array(pivot_y, cell_size, rows) -&gt; List[float]:\n    \"\"\"Get Y/Lat coordinates.\"\"\"\n    y_coords = [pivot_y - i * cell_size - cell_size / 2 for i in range(rows)]\n    return y_coords\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_block_arrangement","title":"<code>get_block_arrangement(band=0, x_block_size=None, y_block_size=None)</code>","text":"<p>Get Block Arrangement.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>band index, by default 0</p> <code>0</code> <code>x_block_size</code> <code>int</code> <p>x block size/number of columns, by default None</p> <code>None</code> <code>y_block_size</code> <code>int</code> <p>y block size/number of rows, by default None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>with the following columns: [x_offset, y_offset, window_xsize, window_ysize]</p> <p>Examples:</p> <ul> <li>Example of getting block arrangement:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(13, 14)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; df = dataset.get_block_arrangement(x_block_size=5, y_block_size=5)\n&gt;&gt;&gt; print(df)\n   x_offset  y_offset  window_xsize  window_ysize\n0         0         0             5             5\n1         5         0             5             5\n2        10         0             4             5\n3         0         5             5             5\n4         5         5             5             5\n5        10         5             4             5\n6         0        10             5             3\n7         5        10             5             3\n8        10        10             4             3\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_block_arrangement(\n    self, band: int = 0, x_block_size: int = None, y_block_size: int = None\n) -&gt; DataFrame:\n    \"\"\"Get Block Arrangement.\n\n    Args:\n        band (int, optional):\n            band index, by default 0\n        x_block_size (int, optional):\n            x block size/number of columns, by default None\n        y_block_size (int, optional):\n            y block size/number of rows, by default None\n\n    Returns:\n        DataFrame:\n            with the following columns: [x_offset, y_offset, window_xsize, window_ysize]\n\n    Examples:\n        - Example of getting block arrangement:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(13, 14)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; df = dataset.get_block_arrangement(x_block_size=5, y_block_size=5)\n          &gt;&gt;&gt; print(df)\n             x_offset  y_offset  window_xsize  window_ysize\n          0         0         0             5             5\n          1         5         0             5             5\n          2        10         0             4             5\n          3         0         5             5             5\n          4         5         5             5             5\n          5        10         5             4             5\n          6         0        10             5             3\n          7         5        10             5             3\n          8        10        10             4             3\n\n          ```\n    \"\"\"\n    block_sizes = self.block_size[band]\n    x_block_size = block_sizes[0] if x_block_size is None else x_block_size\n    y_block_size = block_sizes[1] if y_block_size is None else y_block_size\n\n    df = pd.DataFrame(\n        [\n            {\n                \"x_offset\": x,\n                \"y_offset\": y,\n                \"window_xsize\": min(x_block_size, self.columns - x),\n                \"window_ysize\": min(y_block_size, self.rows - y),\n            }\n            for y in range(0, self.rows, y_block_size)\n            for x in range(0, self.columns, x_block_size)\n        ],\n        columns=[\"x_offset\", \"y_offset\", \"window_xsize\", \"window_ysize\"],\n    )\n    return df\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.copy","title":"<code>copy(path=None)</code>","text":"<p>Deep copy.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Destination path to save the copied dataset. If None is passed, the copied dataset will be created in memory.</p> <code>None</code> <p>Examples:</p> <ul> <li>First, we will create a dataset with 1 band, 3 rows and 5 columns.</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(3, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 3 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>Now, we will create a copy of the dataset.</li> </ul> <pre><code>&gt;&gt;&gt; copied_dataset = dataset.copy(path=\"copy-dataset.tif\")\n&gt;&gt;&gt; print(copied_dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 3 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File: copy-dataset.tif\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>Now close the dataset.</li> </ul> <pre><code>&gt;&gt;&gt; copied_dataset.close()\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def copy(self, path: str = None) -&gt; \"Dataset\":\n    \"\"\"Deep copy.\n\n    Args:\n        path (str, optional):\n            Destination path to save the copied dataset. If None is passed, the copied dataset will be created in memory.\n\n    Examples:\n        - First, we will create a dataset with 1 band, 3 rows and 5 columns.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(3, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 3 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - Now, we will create a copy of the dataset.\n\n          ```python\n          &gt;&gt;&gt; copied_dataset = dataset.copy(path=\"copy-dataset.tif\")\n          &gt;&gt;&gt; print(copied_dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 3 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File: copy-dataset.tif\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - Now close the dataset.\n\n          ```python\n          &gt;&gt;&gt; copied_dataset.close()\n\n          ```\n\n    \"\"\"\n    if path is None:\n        path = \"\"\n        driver = \"MEM\"\n    else:\n        driver = \"GTiff\"\n\n    src = gdal.GetDriverByName(driver).CreateCopy(path, self._raster)\n\n    return Dataset(src, access=\"write\")\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.close","title":"<code>close()</code>","text":"<p>Close the dataset.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def close(self):\n    \"\"\"Close the dataset.\"\"\"\n    self._raster.FlushCache()\n    self._raster = None\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_attribute_table","title":"<code>get_attribute_table(band=0)</code>","text":"<p>Get the attribute table for a given band.</p> <pre><code>- Get the attribute table of a band.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index, the index starts from 1.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the attribute table.</p> <p>Examples:</p> <ul> <li>Read a dataset and fetch its attribute table:</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/south-america-mswep_1979010100.tif\")\n&gt;&gt;&gt; df = dataset.get_attribute_table()\n&gt;&gt;&gt; print(df)\n  Precipitation Range (mm)   Category              Description\n0                     0-50        Low   Very low precipitation\n1                   51-100   Moderate   Moderate precipitation\n2                  101-200       High       High precipitation\n3                  201-500  Very High  Very high precipitation\n4                     &gt;500    Extreme    Extreme precipitation\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_attribute_table(self, band: int = 0) -&gt; DataFrame:\n    \"\"\"Get the attribute table for a given band.\n\n        - Get the attribute table of a band.\n\n    Args:\n        band (int):\n            Band index, the index starts from 1.\n\n    Returns:\n        DataFrame:\n            DataFrame with the attribute table.\n\n    Examples:\n        - Read a dataset and fetch its attribute table:\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/south-america-mswep_1979010100.tif\")\n          &gt;&gt;&gt; df = dataset.get_attribute_table()\n          &gt;&gt;&gt; print(df)\n            Precipitation Range (mm)   Category              Description\n          0                     0-50        Low   Very low precipitation\n          1                   51-100   Moderate   Moderate precipitation\n          2                  101-200       High       High precipitation\n          3                  201-500  Very High  Very high precipitation\n          4                     &gt;500    Extreme    Extreme precipitation\n\n          ```\n    \"\"\"\n    band = self._iloc(band)\n    rat = band.GetDefaultRAT()\n    if rat is None:\n        df = None\n    else:\n        df = self._attribute_table_to_df(rat)\n\n    return df\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.set_attribute_table","title":"<code>set_attribute_table(df, band=None)</code>","text":"<p>Set the attribute table for a band.</p> <p>The attribute table can be used to associate tabular data with the values of a raster band. This is particularly useful for categorical raster data, such as land cover classifications, where each pixel value corresponds to a category that has additional attributes (e.g., class name, color description).</p> Notes <ul> <li>The attribute table is stored in an xml file by the name of the raster file with the extension of .aux.xml.</li> <li>Setting an attribute table to a band will overwrite the existing attribute table if it exists.</li> <li>Setting an attribute table to a band does not need the dataset to be opened in a write mode.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with the attribute table.</p> required <code>band</code> <code>int</code> <p>Band index.</p> <code>None</code> <p>Examples:</p> <ul> <li>First create a dataset:</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.create(\n... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n... epsg=4326, no_data_value=-9999\n... )\n</code></pre> <ul> <li>Create a DataFrame with the attribute table:</li> </ul> <pre><code>&gt;&gt;&gt; data = {\n...     \"Value\": [1, 2, 3],\n...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data)\n</code></pre> <ul> <li>Set the attribute table to the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; dataset.set_attribute_table(df, band=0)\n</code></pre> <ul> <li>Then the attribute table can be retrieved using the <code>get_attribute_table</code> method.</li> <li>The content of the attribute table will be stored in an xml file by the name of the raster file with   the extension of .aux.xml. The content of the file will be like the following:</li> </ul> <pre><code>    &lt;PAMDataset&gt;\n      &lt;PAMRasterBand band=\"1\"&gt;\n        &lt;GDALRasterAttributeTable tableType=\"thematic\"&gt;\n          &lt;FieldDefn index=\"0\"&gt;\n            &lt;Name&gt;Precipitation Range (mm)&lt;/Name&gt;\n            &lt;Type&gt;2&lt;/Type&gt;\n            &lt;Usage&gt;0&lt;/Usage&gt;\n          &lt;/FieldDefn&gt;\n          &lt;FieldDefn index=\"1\"&gt;\n            &lt;Name&gt;Category&lt;/Name&gt;\n            &lt;Type&gt;2&lt;/Type&gt;\n            &lt;Usage&gt;0&lt;/Usage&gt;\n          &lt;/FieldDefn&gt;\n          &lt;FieldDefn index=\"2\"&gt;\n            &lt;Name&gt;Description&lt;/Name&gt;\n            &lt;Type&gt;2&lt;/Type&gt;\n            &lt;Usage&gt;0&lt;/Usage&gt;\n          &lt;/FieldDefn&gt;\n          &lt;Row index=\"0\"&gt;\n            &lt;F&gt;0-50&lt;/F&gt;\n            &lt;F&gt;Low&lt;/F&gt;\n            &lt;F&gt;Very low precipitation&lt;/F&gt;\n          &lt;/Row&gt;\n          &lt;Row index=\"1\"&gt;\n            &lt;F&gt;51-100&lt;/F&gt;\n            &lt;F&gt;Moderate&lt;/F&gt;\n            &lt;F&gt;Moderate precipitation&lt;/F&gt;\n          &lt;/Row&gt;\n          &lt;Row index=\"2\"&gt;\n            &lt;F&gt;101-200&lt;/F&gt;\n            &lt;F&gt;High&lt;/F&gt;\n            &lt;F&gt;High precipitation&lt;/F&gt;\n          &lt;/Row&gt;\n          &lt;Row index=\"3\"&gt;\n            &lt;F&gt;201-500&lt;/F&gt;\n            &lt;F&gt;Very High&lt;/F&gt;\n            &lt;F&gt;Very high precipitation&lt;/F&gt;\n          &lt;/Row&gt;\n          &lt;Row index=\"4\"&gt;\n            &lt;F&gt;&amp;gt;500&lt;/F&gt;\n            &lt;F&gt;Extreme&lt;/F&gt;\n            &lt;F&gt;Extreme precipitation&lt;/F&gt;\n          &lt;/Row&gt;\n        &lt;/GDALRasterAttributeTable&gt;\n      &lt;/PAMRasterBand&gt;\n    &lt;/PAMDataset&gt;\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def set_attribute_table(self, df: DataFrame, band: int = None) -&gt; None:\n    \"\"\"Set the attribute table for a band.\n\n    The attribute table can be used to associate tabular data with the values of a raster band.\n    This is particularly useful for categorical raster data, such as land cover classifications, where each pixel\n    value corresponds to a category that has additional attributes (e.g., class name, color description).\n\n    Notes:\n        - The attribute table is stored in an xml file by the name of the raster file with the extension of .aux.xml.\n        - Setting an attribute table to a band will overwrite the existing attribute table if it exists.\n        - Setting an attribute table to a band does not need the dataset to be opened in a write mode.\n\n    Args:\n        df (DataFrame):\n            DataFrame with the attribute table.\n        band (int):\n            Band index.\n\n    Examples:\n        - First create a dataset:\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.create(\n          ... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n          ... epsg=4326, no_data_value=-9999\n          ... )\n\n          ```\n\n        - Create a DataFrame with the attribute table:\n\n          ```python\n          &gt;&gt;&gt; data = {\n          ...     \"Value\": [1, 2, 3],\n          ...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n          ...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n          ... }\n          &gt;&gt;&gt; df = pd.DataFrame(data)\n\n          ```\n\n        - Set the attribute table to the dataset:\n\n          ```python\n          &gt;&gt;&gt; dataset.set_attribute_table(df, band=0)\n\n          ```\n\n        - Then the attribute table can be retrieved using the `get_attribute_table` method.\n        - The content of the attribute table will be stored in an xml file by the name of the raster file with\n          the extension of .aux.xml. The content of the file will be like the following:\n\n          ```xml\n\n              &lt;PAMDataset&gt;\n                &lt;PAMRasterBand band=\"1\"&gt;\n                  &lt;GDALRasterAttributeTable tableType=\"thematic\"&gt;\n                    &lt;FieldDefn index=\"0\"&gt;\n                      &lt;Name&gt;Precipitation Range (mm)&lt;/Name&gt;\n                      &lt;Type&gt;2&lt;/Type&gt;\n                      &lt;Usage&gt;0&lt;/Usage&gt;\n                    &lt;/FieldDefn&gt;\n                    &lt;FieldDefn index=\"1\"&gt;\n                      &lt;Name&gt;Category&lt;/Name&gt;\n                      &lt;Type&gt;2&lt;/Type&gt;\n                      &lt;Usage&gt;0&lt;/Usage&gt;\n                    &lt;/FieldDefn&gt;\n                    &lt;FieldDefn index=\"2\"&gt;\n                      &lt;Name&gt;Description&lt;/Name&gt;\n                      &lt;Type&gt;2&lt;/Type&gt;\n                      &lt;Usage&gt;0&lt;/Usage&gt;\n                    &lt;/FieldDefn&gt;\n                    &lt;Row index=\"0\"&gt;\n                      &lt;F&gt;0-50&lt;/F&gt;\n                      &lt;F&gt;Low&lt;/F&gt;\n                      &lt;F&gt;Very low precipitation&lt;/F&gt;\n                    &lt;/Row&gt;\n                    &lt;Row index=\"1\"&gt;\n                      &lt;F&gt;51-100&lt;/F&gt;\n                      &lt;F&gt;Moderate&lt;/F&gt;\n                      &lt;F&gt;Moderate precipitation&lt;/F&gt;\n                    &lt;/Row&gt;\n                    &lt;Row index=\"2\"&gt;\n                      &lt;F&gt;101-200&lt;/F&gt;\n                      &lt;F&gt;High&lt;/F&gt;\n                      &lt;F&gt;High precipitation&lt;/F&gt;\n                    &lt;/Row&gt;\n                    &lt;Row index=\"3\"&gt;\n                      &lt;F&gt;201-500&lt;/F&gt;\n                      &lt;F&gt;Very High&lt;/F&gt;\n                      &lt;F&gt;Very high precipitation&lt;/F&gt;\n                    &lt;/Row&gt;\n                    &lt;Row index=\"4\"&gt;\n                      &lt;F&gt;&amp;gt;500&lt;/F&gt;\n                      &lt;F&gt;Extreme&lt;/F&gt;\n                      &lt;F&gt;Extreme precipitation&lt;/F&gt;\n                    &lt;/Row&gt;\n                  &lt;/GDALRasterAttributeTable&gt;\n                &lt;/PAMRasterBand&gt;\n              &lt;/PAMDataset&gt;\n\n          ```\n    \"\"\"\n    rat = self._df_to_attribute_table(df)\n    band = self._iloc(band)\n    band.SetDefaultRAT(rat)\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.add_band","title":"<code>add_band(array, unit=None, attribute_table=None, inplace=False)</code>","text":"<p>Add a new band to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>2D array to add as a new band.</p> required <code>unit</code> <code>Any</code> <p>Unit of the values in the new band.</p> <code>None</code> <code>attribute_table</code> <code>DataFrame</code> <p>Attribute table provides a way to associate tabular data with the values of a raster band. This is particularly useful for categorical raster data, such as land cover classifications, where each pixel value corresponds to a category that has additional attributes (e.g., class name, color, description). Default is None.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the new band will be added to the current dataset, if False the new band will be added to a new dataset. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[None, Dataset]</code> <p>None</p> <p>Examples:</p> <ul> <li>First create a dataset:</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.create(\n... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n... epsg=4326, no_data_value=-9999\n... )\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float32\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>Create a 2D array to add as a new band:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array = np.random.rand(10, 10)\n</code></pre> <ul> <li>Add the new band to the dataset inplace:</li> </ul> <pre><code>&gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=None, inplace=True)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 2\n            Band names: ['Band_1', 'Band_2']\n            Mask: -9999.0\n            Data type: float32\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>The new band will be added to the dataset inplace.</li> <li>You can also add an attribute table to the band when you add a new band to the dataset.</li> </ul> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = {\n...     \"Value\": [1, 2, 3],\n...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=df, inplace=True)\n</code></pre> See Also <p>Dataset.create_from_array: create a new dataset from an array. Dataset.create: create a new dataset with an empty band. Dataset.dataset_like: create a new dataset from another dataset. Dataset.get_attribute_table: get the attribute table for a specific band. Dataset.set_attribute_table: Set the attribute table for a specific band.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def add_band(\n    self,\n    array: np.ndarray,\n    unit: Any = None,\n    attribute_table: DataFrame = None,\n    inplace: bool = False,\n) -&gt; Union[None, \"Dataset\"]:\n    \"\"\"Add a new band to the dataset.\n\n    Args:\n        array (np.ndarray):\n            2D array to add as a new band.\n        unit (Any, optional):\n            Unit of the values in the new band.\n        attribute_table (DataFrame, optional):\n            Attribute table provides a way to associate tabular data with the values of a raster band. This is\n            particularly useful for categorical raster data, such as land cover classifications, where each pixel\n            value corresponds to a category that has additional attributes (e.g., class name, color, description).\n            Default is None.\n        inplace (bool, optional):\n            If True the new band will be added to the current dataset, if False the new band will be added to a\n            new dataset. Default is False.\n\n    Returns:\n        None\n\n    Examples:\n        - First create a dataset:\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.create(\n          ... cell_size=0.05, rows=10, columns=10, dtype=\"float32\", bands=1, top_left_corner=(0, 0),\n          ... epsg=4326, no_data_value=-9999\n          ... )\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float32\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - Create a 2D array to add as a new band:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; array = np.random.rand(10, 10)\n\n          ```\n\n        - Add the new band to the dataset inplace:\n\n          ```python\n          &gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=None, inplace=True)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 2\n                      Band names: ['Band_1', 'Band_2']\n                      Mask: -9999.0\n                      Data type: float32\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - The new band will be added to the dataset inplace.\n        - You can also add an attribute table to the band when you add a new band to the dataset.\n\n          ```python\n          &gt;&gt;&gt; import pandas as pd\n          &gt;&gt;&gt; data = {\n          ...     \"Value\": [1, 2, 3],\n          ...     \"ClassName\": [\"Forest\", \"Water\", \"Urban\"],\n          ...     \"Color\": [\"#008000\", \"#0000FF\", \"#808080\"],\n          ... }\n          &gt;&gt;&gt; df = pd.DataFrame(data)\n          &gt;&gt;&gt; dataset.add_band(array, unit=\"m\", attribute_table=df, inplace=True)\n\n          ```\n\n    See Also:\n        Dataset.create_from_array: create a new dataset from an array.\n        Dataset.create: create a new dataset with an empty band.\n        Dataset.dataset_like: create a new dataset from another dataset.\n        Dataset.get_attribute_table: get the attribute table for a specific band.\n        Dataset.set_attribute_table: Set the attribute table for a specific band.\n    \"\"\"\n    # check the dimensions of the new array\n    if array.ndim != 2:\n        raise ValueError(\"The array must be 2D.\")\n    if array.shape[0] != self.rows or array.shape[1] != self.columns:\n        raise ValueError(\n            f\"The array must have the same dimensions as the raster.{self.rows} {self.columns}\"\n        )\n    # check if the dataset is opened in a write mode\n    if inplace:\n        if self.access == \"read_only\":\n            raise ValueError(\"The dataset is not opened in a write mode.\")\n        else:\n            src = self._raster\n    else:\n        src = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self._raster)\n\n    dtype = numpy_to_gdal_dtype(array.dtype)\n    num_bands = src.RasterCount\n    src.AddBand(dtype, [])\n    band = src.GetRasterBand(num_bands + 1)\n\n    if unit is not None:\n        band.SetUnitType(unit)\n\n    if attribute_table is not None:\n        # Attach the RAT to the raster band\n        rat = Dataset._df_to_attribute_table(attribute_table)\n        band.SetDefaultRAT(rat)\n\n    band.WriteArray(array)\n\n    if inplace:\n        self.__init__(src, self.access)\n    else:\n        return Dataset(src, self.access)\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.stats","title":"<code>stats(band=None, mask=None)</code>","text":"<p>Get statistics of a band [Min, max, mean, std].</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. If None, the statistics of all bands will be returned.</p> <code>None</code> <code>mask</code> <code>Polygon GeoDataFrame or Dataset</code> <p>GeodataFrame with a geometry of polygon type.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame wit the stats of each band, the dataframe has the following columns [min, max, mean, std], the index of the dataframe is the band names.</p> <pre><code>                   Min         max        mean       std\n    Band_1  270.369720  270.762299  270.551361  0.154270\n    Band_2  269.611938  269.744751  269.673645  0.043788\n    Band_3  273.641479  274.168823  273.953979  0.198447\n    Band_4  273.991516  274.540344  274.310669  0.205754\n</code></pre> Notes <ul> <li>The value of the stats will be stored in an xml file by the name of the raster file with the extension of   .aux.xml.</li> <li>The content of the file will be like the following:</li> </ul> <pre><code>    &lt;PAMDataset&gt;\n      &lt;PAMRasterBand band=\"1\"&gt;\n        &lt;Description&gt;Band_1&lt;/Description&gt;\n        &lt;Metadata&gt;\n          &lt;MDI key=\"RepresentationType\"&gt;ATHEMATIC&lt;/MDI&gt;\n          &lt;MDI key=\"STATISTICS_MAXIMUM\"&gt;88&lt;/MDI&gt;\n          &lt;MDI key=\"STATISTICS_MEAN\"&gt;7.9662921348315&lt;/MDI&gt;\n          &lt;MDI key=\"STATISTICS_MINIMUM\"&gt;0&lt;/MDI&gt;\n          &lt;MDI key=\"STATISTICS_STDDEV\"&gt;18.294377743948&lt;/MDI&gt;\n          &lt;MDI key=\"STATISTICS_VALID_PERCENT\"&gt;48.9&lt;/MDI&gt;\n        &lt;/Metadata&gt;\n      &lt;/PAMRasterBand&gt;\n    &lt;/PAMDataset&gt;\n</code></pre> <p>Examples:</p> <ul> <li>Get the statistics of all bands in the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n&gt;&gt;&gt; print(dataset.stats()) # doctest: +SKIP\n             min       max      mean       std\nBand_1  0.006443  0.942943  0.468935  0.266634\nBand_2  0.020377  0.978130  0.477189  0.306864\nBand_3  0.019652  0.992184  0.537215  0.286502\nBand_4  0.011955  0.984313  0.503616  0.295852\n&gt;&gt;&gt; print(dataset.stats(band=1))  # doctest: +SKIP\n             min      max      mean       std\nBand_2  0.020377  0.97813  0.477189  0.306864\n</code></pre> <ul> <li> <p>Get the statistics of all the bands using a mask polygon.</p> </li> <li> <p>Create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2,     0.2 -0.1] to cover the 4 cells.   <pre><code>&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])],crs=4326)\n&gt;&gt;&gt; print(dataset.stats(mask=mask))  # doctest: +SKIP\n             min       max      mean       std\nBand_1  0.193441  0.702108  0.541478  0.202932\nBand_2  0.281281  0.932573  0.665602  0.239410\nBand_3  0.031395  0.982235  0.493086  0.377608\nBand_4  0.079562  0.930965  0.591025  0.341578\n</code></pre></p> </li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def stats(self, band: int = None, mask: GeoDataFrame = None) -&gt; DataFrame:\n    \"\"\"Get statistics of a band [Min, max, mean, std].\n\n    Args:\n        band (int, optional):\n            Band index. If None, the statistics of all bands will be returned.\n        mask (Polygon GeoDataFrame or Dataset, optional):\n            GeodataFrame with a geometry of polygon type.\n\n    Returns:\n        DataFrame:\n            DataFrame wit the stats of each band, the dataframe has the following columns\n            [min, max, mean, std], the index of the dataframe is the band names.\n\n            ```text\n\n                               Min         max        mean       std\n                Band_1  270.369720  270.762299  270.551361  0.154270\n                Band_2  269.611938  269.744751  269.673645  0.043788\n                Band_3  273.641479  274.168823  273.953979  0.198447\n                Band_4  273.991516  274.540344  274.310669  0.205754\n            ```\n\n    Notes:\n        - The value of the stats will be stored in an xml file by the name of the raster file with the extension of\n          .aux.xml.\n        - The content of the file will be like the following:\n\n          ```xml\n\n              &lt;PAMDataset&gt;\n                &lt;PAMRasterBand band=\"1\"&gt;\n                  &lt;Description&gt;Band_1&lt;/Description&gt;\n                  &lt;Metadata&gt;\n                    &lt;MDI key=\"RepresentationType\"&gt;ATHEMATIC&lt;/MDI&gt;\n                    &lt;MDI key=\"STATISTICS_MAXIMUM\"&gt;88&lt;/MDI&gt;\n                    &lt;MDI key=\"STATISTICS_MEAN\"&gt;7.9662921348315&lt;/MDI&gt;\n                    &lt;MDI key=\"STATISTICS_MINIMUM\"&gt;0&lt;/MDI&gt;\n                    &lt;MDI key=\"STATISTICS_STDDEV\"&gt;18.294377743948&lt;/MDI&gt;\n                    &lt;MDI key=\"STATISTICS_VALID_PERCENT\"&gt;48.9&lt;/MDI&gt;\n                  &lt;/Metadata&gt;\n                &lt;/PAMRasterBand&gt;\n              &lt;/PAMDataset&gt;\n\n          ```\n\n    Examples:\n        - Get the statistics of all bands in the dataset:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n          &gt;&gt;&gt; print(dataset.stats()) # doctest: +SKIP\n                       min       max      mean       std\n          Band_1  0.006443  0.942943  0.468935  0.266634\n          Band_2  0.020377  0.978130  0.477189  0.306864\n          Band_3  0.019652  0.992184  0.537215  0.286502\n          Band_4  0.011955  0.984313  0.503616  0.295852\n          &gt;&gt;&gt; print(dataset.stats(band=1))  # doctest: +SKIP\n                       min      max      mean       std\n          Band_2  0.020377  0.97813  0.477189  0.306864\n\n          ```\n\n        - Get the statistics of all the bands using a mask polygon.\n\n          - Create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2,\n            0.2 -0.1] to cover the 4 cells.\n          ```python\n          &gt;&gt;&gt; from shapely.geometry import Polygon\n          &gt;&gt;&gt; import geopandas as gpd\n          &gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])],crs=4326)\n          &gt;&gt;&gt; print(dataset.stats(mask=mask))  # doctest: +SKIP\n                       min       max      mean       std\n          Band_1  0.193441  0.702108  0.541478  0.202932\n          Band_2  0.281281  0.932573  0.665602  0.239410\n          Band_3  0.031395  0.982235  0.493086  0.377608\n          Band_4  0.079562  0.930965  0.591025  0.341578\n\n          ```\n\n    \"\"\"\n    if mask is not None:\n        dst = self.crop(mask, touch=True)\n\n    if band is None:\n        df = pd.DataFrame(\n            index=self.band_names,\n            columns=[\"min\", \"max\", \"mean\", \"std\"],\n            dtype=np.float32,\n        )\n        for i in range(self.band_count):\n            if mask is not None:\n                df.iloc[i, :] = dst._get_stats(i)\n            else:\n                df.iloc[i, :] = self._get_stats(i)\n    else:\n        df = pd.DataFrame(\n            index=[self.band_names[band]],\n            columns=[\"min\", \"max\", \"mean\", \"std\"],\n            dtype=np.float32,\n        )\n        if mask is not None:\n            df.iloc[0, :] = dst._get_stats(band)\n        else:\n            df.iloc[0, :] = self._get_stats(band)\n\n    return df\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.plot","title":"<code>plot(band=None, exclude_value=None, rgb=None, surface_reflectance=10000, cutoff=None, overview=False, overview_index=0, **kwargs)</code>","text":"<p>Plot the values/overviews of a given band.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>The band you want to get its data. Default is 0.</p> <code>None</code> <code>exclude_value</code> <code>Any</code> <p>Value to exclude from the plot. Default is None.</p> <code>None</code> <code>rgb</code> <code>List[int]</code> <p>The <code>plot</code> method will check if the RGB bands are defined in the raster file; if all three bands (red, green, blue) are defined, the method will use them to plot the real image; otherwise, the RGB bands will be considered as [2, 1, 0].</p> <code>None</code> <code>surface_reflectance</code> <code>int</code> <p>Default is 10,000.</p> <code>10000</code> <code>cutoff</code> <code>List</code> <p>Clip the range of pixel values for each band (take only the pixel values from 0 to the value of the cutoff and scale them back to between 0 and 1). Default is None.</p> <code>None</code> <code>overview</code> <code>bool</code> <p>True if you want to plot the overview. Default is False.</p> <code>False</code> <code>overview_index</code> <code>int</code> <p>Index of the overview. Default is 0.</p> <code>0</code> <code>kwargs</code> <code>Any</code> Parameter Type Description <code>points</code> array 3 column array with the first column as the value to display for the point, the second as the row index, and the third as the column index in the array. The second and third columns tell the location of the point. <code>point_color</code> str Color of the point. <code>point_size</code> Any Size of the point. <code>pid_color</code> str Color of the annotation of the point. Default is blue. <code>pid_size</code> Any Size of the point annotation. <code>figsize</code> tuple, optional Figure size. Default is <code>(8, 8)</code>. <code>title</code> str, optional Title of the plot. Default is <code>'Total Discharge'</code>. <code>title_size</code> int, optional Title size. Default is <code>15</code>. <code>orientation</code> str, optional Orientation of the color bar (<code>horizontal</code> or <code>vertical</code>). Default is <code>'vertical'</code>. <code>rotation</code> number, optional Rotation of the color bar label. Default is <code>-90</code>. <code>cbar_length</code> float, optional Ratio to control the height of the color bar. Default is <code>0.75</code>. <code>ticks_spacing</code> int, optional Spacing between color bar ticks. Default is <code>2</code>. <code>cbar_label_size</code> int, optional Size of the color bar label. Default is <code>12</code>. <code>cbar_label</code> str, optional Label of the color bar. Default is <code>'Discharge m\u00b3/s'</code>. <code>color_scale</code> int, optional Scale mode for colors. Options: 1 = normal, 2 = power, 3 = SymLogNorm, 4 = PowerNorm, 5 = BoundaryNorm. Default is <code>1</code>. <code>gamma</code> float, optional Value needed for color scale option 2. Default is <code>1/2</code>. <code>line_threshold</code> float, optional Value needed for color scale option 3. Default is <code>0.0001</code>. <code>line_scale</code> float, optional Value needed for color scale option 3. Default is <code>0.001</code>. <code>bounds</code> list, optional Discrete bounds for color scale option 4. Default is <code>None</code>. <code>midpoint</code> float, optional Value needed for color scale option 5. Default is <code>0</code>. <code>cmap</code> str, optional Color map style. Default is <code>'coolwarm_r'</code>. <code>display_cell_value</code> bool, optional Whether to display cell values as text. <code>num_size</code> int, optional Size of numbers plotted on top of each cell. Default is <code>8</code>. <code>background_color_threshold</code> float or int, optional Threshold for deciding text color over cells: if value &gt; threshold \u2192 black text; else white text. If <code>None</code>, max value / 2 is used. Default is <code>None</code>. <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple[Any, Any]: The axes of the matplotlib figure and the figure object.</p> <p>Examples:</p> <ul> <li>Plot a certain band:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n&gt;&gt;&gt; dataset.plot(band=0)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> <ul> <li>plot using power scale.</li> </ul> <pre><code>&gt;&gt;&gt; dataset.plot(band=0, color_scale=2)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> <ul> <li>plot using SymLogNorm scale.</li> </ul> <pre><code>&gt;&gt;&gt; dataset.plot(band=0, color_scale=3)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> <ul> <li>plot using PowerNorm scale.</li> </ul> <pre><code>&gt;&gt;&gt; dataset.plot(band=0, color_scale=4, bounds=[0, 0.2, 0.4, 0.6, 0.8, 1])\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> <ul> <li>plot using BoundaryNorm scale.</li> </ul> <pre><code>&gt;&gt;&gt; dataset.plot(band=0, color_scale=5)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def plot(\n    self,\n    band: int = None,\n    exclude_value: Any = None,\n    rgb: List[int] = None,\n    surface_reflectance: int = 10000,\n    cutoff: List = None,\n    overview: bool = False,\n    overview_index: int = 0,\n    **kwargs: Any,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"Plot the values/overviews of a given band.\n\n    Args:\n        band (int, optional):\n            The band you want to get its data. Default is 0.\n        exclude_value (Any, optional):\n            Value to exclude from the plot. Default is None.\n        rgb (List[int], optional):\n            The `plot` method will check if the RGB bands are defined in the raster file; if all three bands\n            (red, green, blue) are defined, the method will use them to plot the real image; otherwise, the\n            RGB bands will be considered as [2, 1, 0].\n        surface_reflectance (int, optional):\n            Default is 10,000.\n        cutoff (List, optional):\n            Clip the range of pixel values for each band (take only the pixel values from 0 to the value of the cutoff\n            and scale them back to between 0 and 1). Default is None.\n        overview (bool, optional):\n            True if you want to plot the overview. Default is False.\n        overview_index (int, optional):\n            Index of the overview. Default is 0.\n        kwargs:\n            | Parameter                   | Type                | Description |\n            |-----------------------------|---------------------|-------------|\n            | `points`                    | array               | 3 column array with the first column as the value to display for the point, the second as the row index, and the third as the column index in the array. The second and third columns tell the location of the point. |\n            | `point_color`               | str                 | Color of the point. |\n            | `point_size`                | Any                 | Size of the point. |\n            | `pid_color`                 | str                 | Color of the annotation of the point. Default is blue. |\n            | `pid_size`                  | Any                 | Size of the point annotation. |\n            | `figsize`                   | tuple, optional     | Figure size. Default is `(8, 8)`. |\n            | `title`                     | str, optional       | Title of the plot. Default is `'Total Discharge'`. |\n            | `title_size`                | int, optional       | Title size. Default is `15`. |\n            | `orientation`               | str, optional       | Orientation of the color bar (`horizontal` or `vertical`). Default is `'vertical'`. |\n            | `rotation`                  | number, optional    | Rotation of the color bar label. Default is `-90`. |\n            | `cbar_length`               | float, optional     | Ratio to control the height of the color bar. Default is `0.75`. |\n            | `ticks_spacing`             | int, optional       | Spacing between color bar ticks. Default is `2`. |\n            | `cbar_label_size`           | int, optional       | Size of the color bar label. Default is `12`. |\n            | `cbar_label`                | str, optional       | Label of the color bar. Default is `'Discharge m\u00b3/s'`. |\n            | `color_scale`               | int, optional       | Scale mode for colors. Options: 1 = normal, 2 = power, 3 = SymLogNorm, 4 = PowerNorm, 5 = BoundaryNorm. Default is `1`. |\n            | `gamma`                     | float, optional     | Value needed for color scale option 2. Default is `1/2`. |\n            | `line_threshold`            | float, optional     | Value needed for color scale option 3. Default is `0.0001`. |\n            | `line_scale`                | float, optional     | Value needed for color scale option 3. Default is `0.001`. |\n            | `bounds`                    | list, optional      | Discrete bounds for color scale option 4. Default is `None`. |\n            | `midpoint`                  | float, optional     | Value needed for color scale option 5. Default is `0`. |\n            | `cmap`                      | str, optional       | Color map style. Default is `'coolwarm_r'`. |\n            | `display_cell_value`        | bool, optional      | Whether to display cell values as text. |\n            | `num_size`                  | int, optional       | Size of numbers plotted on top of each cell. Default is `8`. |\n            | `background_color_threshold`| float or int, optional | Threshold for deciding text color over cells: if value &gt; threshold \u2192 black text; else white text. If `None`, max value / 2 is used. Default is `None`. |\n\n\n    Returns:\n        Tuple[Any, Any]:\n            The axes of the matplotlib figure and the figure object.\n\n    Examples:\n        - Plot a certain band:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n          &gt;&gt;&gt; dataset.plot(band=0)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n\n        - plot using power scale.\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, color_scale=2)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n\n        - plot using SymLogNorm scale.\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, color_scale=3)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n\n        - plot using PowerNorm scale.\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, color_scale=4, bounds=[0, 0.2, 0.4, 0.6, 0.8, 1])\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n\n        - plot using BoundaryNorm scale.\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, color_scale=5)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n    \"\"\"\n    import_cleopatra(\n        \"The current function uses cleopatra package to for plotting, please install it manually, for more info \"\n        \"check https://github.com/Serapieum-of-alex/cleopatra\"\n    )\n    from cleopatra.array_glyph import ArrayGlyph\n\n    no_data_value = [np.nan if i is None else i for i in self.no_data_value]\n    if overview:\n        arr = self.read_overview_array(band=band, overview_index=overview_index)\n    else:\n        arr = self.read_array(band=band)\n    # if the raster has three bands or more.\n    if self.band_count &gt;= 3:\n        if band is None:\n            if rgb is None:\n                rgb = [\n                    self.get_band_by_color(\"red\"),\n                    self.get_band_by_color(\"green\"),\n                    self.get_band_by_color(\"blue\"),\n                ]\n                if None in rgb:\n                    rgb = [2, 1, 0]\n            # first make the band index the first band in the rgb list (red band)\n            band = rgb[0]\n    # elif self.band_count == 1:\n    #     band = 0\n    else:\n        if band is None:\n            band = 0\n\n    exclude_value = (\n        [no_data_value[band], exclude_value]\n        if exclude_value is not None\n        else [no_data_value[band]]\n    )\n\n    cleo = ArrayGlyph(\n        arr,\n        exclude_value=exclude_value,\n        extent=self.bbox,\n        rgb=rgb,\n        surface_reflectance=surface_reflectance,\n        cutoff=cutoff,\n        **kwargs,\n    )\n    fig, ax = cleo.plot(**kwargs)\n    return fig, ax\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.create","title":"<code>create(cell_size, rows, columns, dtype, bands, top_left_corner, epsg, no_data_value=None, path=None)</code>  <code>classmethod</code>","text":"<p>Create a new dataset and fill it with the no_data_value.</p> <p>The new dataset will have an array filled with the no_data_value.</p> <p>Parameters:</p> Name Type Description Default <code>cell_size</code> <code>int | float</code> <p>Cell size.</p> required <code>rows</code> <code>int</code> <p>Number of rows.</p> required <code>columns</code> <code>int</code> <p>Number of columns.</p> required <code>dtype</code> <code>str</code> <p>Data type. One of: None, \"byte\", \"uint16\", \"int16\", \"uint32\", \"int32\", \"float32\", \"float64\", \"complex-int16\", \"complex-int32\", \"complex-float32\", \"complex-float64\", \"uint64\", \"int64\", \"int8\", \"count\".</p> required <code>bands</code> <code>int | None</code> <p>Number of bands to create in the output raster.</p> required <code>top_left_corner</code> <code>Tuple</code> <p>Coordinates of the top left corner point.</p> required <code>epsg</code> <code>int</code> <p>EPSG number to identify the projection of the coordinates in the created raster.</p> required <code>no_data_value</code> <code>float | None</code> <p>No data value.</p> <code>None</code> <code>path</code> <code>str</code> <p>Path on disk; if None, the dataset is created in memory. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>A new dataset</p> Hint <ul> <li>The no_data_value will be filled in the array of the output dataset.</li> <li>The coordinates of the top left corner point should be in the same projection as the epsg.</li> <li>The cell size should be in the same unit as the coordinates.</li> <li>The number of rows and columns should be positive integers.</li> </ul> <p>Examples:</p> <ul> <li>To create a dataset using the <code>create</code> method you need to provide all the information needed to locate the  dataset in space <code>top_left_corner</code> and <code>epsg</code>, then the information needed to specify the data to be stored  in the dataset like <code>dtype</code>, <code>rows</code>, <code>columns</code>, <code>cell_size</code>, <code>bands</code> and <code>no_data_value</code>.</li> </ul> <pre><code>&gt;&gt;&gt; cell_size = 10\n&gt;&gt;&gt; rows = 5\n&gt;&gt;&gt; columns = 5\n&gt;&gt;&gt; dtype = \"float32\"\n&gt;&gt;&gt; bands = 1\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; epsg = 32618\n&gt;&gt;&gt; no_data_value = -9999\n&gt;&gt;&gt; path = \"create-new-dataset.tif\"\n&gt;&gt;&gt; dataset = Dataset.create(cell_size, rows, columns, dtype, bands, top_left_corner, epsg, no_data_value, path)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 10.0\n            Dimension: 5 * 5\n            EPSG: 32618\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float32\n            File: create-new-dataset.tif\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>If you check the value stored in the band using the <code>read_array</code> method, you will find that the band is     full of the <code>no_data_value</code> value which we used here as -9999.</li> </ul> <pre><code>&gt;&gt;&gt; print(dataset.read_array(band=0))\n[[-9999. -9999. -9999. -9999. -9999.]\n [-9999. -9999. -9999. -9999. -9999.]\n [-9999. -9999. -9999. -9999. -9999.]\n [-9999. -9999. -9999. -9999. -9999.]\n [-9999. -9999. -9999. -9999. -9999.]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    cell_size: Union[int, float],\n    rows: int,\n    columns: int,\n    dtype: str,\n    bands: int,\n    top_left_corner: Tuple,\n    epsg: int,\n    no_data_value: Any = None,\n    path: str = None,\n) -&gt; \"Dataset\":\n    \"\"\"Create a new dataset and fill it with the no_data_value.\n\n    The new dataset will have an array filled with the no_data_value.\n\n    Args:\n        cell_size (int|float):\n            Cell size.\n        rows (int):\n            Number of rows.\n        columns (int):\n            Number of columns.\n        dtype (str):\n            Data type. One of: None, \"byte\", \"uint16\", \"int16\", \"uint32\", \"int32\", \"float32\", \"float64\",\n            \"complex-int16\", \"complex-int32\", \"complex-float32\", \"complex-float64\", \"uint64\", \"int64\", \"int8\",\n            \"count\".\n        bands (int|None):\n            Number of bands to create in the output raster.\n        top_left_corner (Tuple):\n            Coordinates of the top left corner point.\n        epsg (int):\n            EPSG number to identify the projection of the coordinates in the created raster.\n        no_data_value (float|None):\n            No data value.\n        path (str, optional):\n            Path on disk; if None, the dataset is created in memory. Default is None.\n\n    Returns:\n        Dataset: A new dataset\n\n    Hint:\n        - The no_data_value will be filled in the array of the output dataset.\n        - The coordinates of the top left corner point should be in the same projection as the epsg.\n        - The cell size should be in the same unit as the coordinates.\n        - The number of rows and columns should be positive integers.\n\n    Examples:\n        - To create a dataset using the `create` method you need to provide all the information needed to locate the\n         dataset in space `top_left_corner` and `epsg`, then the information needed to specify the data to be stored\n         in the dataset like `dtype`, `rows`, `columns`, `cell_size`, `bands` and `no_data_value`.\n\n          ```python\n          &gt;&gt;&gt; cell_size = 10\n          &gt;&gt;&gt; rows = 5\n          &gt;&gt;&gt; columns = 5\n          &gt;&gt;&gt; dtype = \"float32\"\n          &gt;&gt;&gt; bands = 1\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; epsg = 32618\n          &gt;&gt;&gt; no_data_value = -9999\n          &gt;&gt;&gt; path = \"create-new-dataset.tif\"\n          &gt;&gt;&gt; dataset = Dataset.create(cell_size, rows, columns, dtype, bands, top_left_corner, epsg, no_data_value, path)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 10.0\n                      Dimension: 5 * 5\n                      EPSG: 32618\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float32\n                      File: create-new-dataset.tif\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - If you check the value stored in the band using the `read_array` method, you will find that the band is\n            full of the `no_data_value` value which we used here as -9999.\n\n          ```python\n          &gt;&gt;&gt; print(dataset.read_array(band=0))\n          [[-9999. -9999. -9999. -9999. -9999.]\n           [-9999. -9999. -9999. -9999. -9999.]\n           [-9999. -9999. -9999. -9999. -9999.]\n           [-9999. -9999. -9999. -9999. -9999.]\n           [-9999. -9999. -9999. -9999. -9999.]]\n\n          ```\n    \"\"\"\n    # Create the driver.\n    dtype = numpy_to_gdal_dtype(dtype)\n    dst = Dataset._create_dataset(columns, rows, bands, dtype, path=path)\n    sr = Dataset._create_sr_from_epsg(epsg)\n    geotransform = (\n        top_left_corner[0],\n        cell_size,\n        0,\n        top_left_corner[1],\n        0,\n        -1 * cell_size,\n    )\n    dst.SetGeoTransform(geotransform)\n    # Set the projection.\n    dst.SetProjection(sr.ExportToWkt())\n\n    dst = cls(dst, access=\"write\")\n    if no_data_value is not None:\n        dst._set_no_data_value(no_data_value=no_data_value)\n\n    return dst\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.create_from_array","title":"<code>create_from_array(arr, top_left_corner=None, cell_size=None, geo=None, epsg=4326, no_data_value=DEFAULT_NO_DATA_VALUE, driver_type='MEM', path=None)</code>  <code>classmethod</code>","text":"<p>Create a new dataset from an array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Numpy array.</p> required <code>top_left_corner</code> <code>Tuple[float, float]</code> <p>The coordinates of the top left corner of the dataset.</p> <code>None</code> <code>cell_size</code> <code>int | float</code> <p>Cell size in the same units of the coordinate reference system defined by the <code>epsg</code> parameter.</p> <code>None</code> <code>geo</code> <code>Tuple[float, float, float, float, float, float]</code> <p>Geotransform tuple (minimum lon/x, pixel-size, rotation, maximum lat/y, rotation, pixel-size).</p> <code>None</code> <code>epsg</code> <code>int</code> <p>Integer reference number to the projection (https://epsg.io/).</p> <code>4326</code> <code>no_data_value</code> <code>Any</code> <p>No data value to mask the cells out of the domain. The default is -9999.</p> <code>DEFAULT_NO_DATA_VALUE</code> <code>driver_type</code> <code>str</code> <p>Driver type [\"GTiff\", \"MEM\", \"netcdf\"]. Default is \"MEM\".</p> <code>'MEM'</code> <code>path</code> <code>str</code> <p>Path to save the driver.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset object will be returned.</p> Hint <ul> <li>The <code>geo</code> parameter can replace both the <code>cell_size</code> and the <code>top_left_corner</code> parameters.</li> <li>The function checks first if the <code>geo</code> parameter is defined; it will ignore the <code>cell_size</code> and the <code>top_left_corner</code> parameters if given.</li> </ul> <p>Examples:</p> <ul> <li>Create dataset using the <code>cell_size</code> and <code>top_left_corner</code> parameters.</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File: ...\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li> <p>Create dataset using the <code>geo</code> parameter.</p> </li> <li> <p>To create the same dataset using the <code>geotransform</code> parameter, we will use the dataset <code>top_left_corner</code>     coordinates and the <code>cell_size</code> to create it.</p> </li> </ul> <pre><code>&gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File: ...\n&lt;BLANKLINE&gt;\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>@classmethod\ndef create_from_array(\n    cls,\n    arr: np.ndarray,\n    top_left_corner: Tuple[float, float] = None,\n    cell_size: Union[int, float] = None,\n    geo: Tuple[float, float, float, float, float, float] = None,\n    epsg: Union[str, int] = 4326,\n    no_data_value: Union[Any, list] = DEFAULT_NO_DATA_VALUE,\n    driver_type: str = \"MEM\",\n    path: str = None,\n) -&gt; \"Dataset\":\n    \"\"\"Create a new dataset from an array.\n\n    Args:\n        arr (np.ndarray):\n            Numpy array.\n        top_left_corner (Tuple[float, float], optional):\n            The coordinates of the top left corner of the dataset.\n        cell_size (int|float, optional):\n            Cell size in the same units of the coordinate reference system defined by the `epsg` parameter.\n        geo (Tuple[float, float, float, float, float, float], optional):\n            Geotransform tuple (minimum lon/x, pixel-size, rotation, maximum lat/y, rotation, pixel-size).\n        epsg (int):\n            Integer reference number to the projection (https://epsg.io/).\n        no_data_value (Any, optional):\n            No data value to mask the cells out of the domain. The default is -9999.\n        driver_type (str, optional):\n            Driver type [\"GTiff\", \"MEM\", \"netcdf\"]. Default is \"MEM\".\n        path (str, optional):\n            Path to save the driver.\n\n    Returns:\n        Dataset:\n            Dataset object will be returned.\n\n    Hint:\n        - The `geo` parameter can replace both the `cell_size` and the `top_left_corner` parameters.\n        - The function checks first if the `geo` parameter is defined; it will ignore the `cell_size` and the `top_left_corner` parameters if given.\n\n    Examples:\n        - Create dataset using the `cell_size` and `top_left_corner` parameters.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File: ...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - Create dataset using the `geo` parameter.\n\n          - To create the same dataset using the `geotransform` parameter, we will use the dataset `top_left_corner`\n            coordinates and the `cell_size` to create it.\n\n          ```python\n          &gt;&gt;&gt; geotransform = (0, 0.05, 0, 0, 0, -0.05)\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, geo=geotransform, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File: ...\n          &lt;BLANKLINE&gt;\n\n          ```\n    \"\"\"\n    if geo is None:\n        if top_left_corner is None or cell_size is None:\n            raise ValueError(\n                \"Either top_left_corner and cell_size or geo should be provided.\"\n            )\n        geo = (\n            top_left_corner[0],\n            cell_size,\n            0,\n            top_left_corner[1],\n            0,\n            -1 * cell_size,\n        )\n\n    if arr.ndim == 2:\n        bands = 1\n        rows = int(arr.shape[0])\n        cols = int(arr.shape[1])\n    else:\n        bands = arr.shape[0]\n        rows = int(arr.shape[1])\n        cols = int(arr.shape[2])\n\n    dst_obj = cls._create_gtiff_from_array(\n        arr,\n        cols,\n        rows,\n        bands,\n        geo,\n        epsg,\n        no_data_value,\n        driver_type=driver_type,\n        path=path,\n    )\n\n    return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.dataset_like","title":"<code>dataset_like(src, array, path=None)</code>  <code>classmethod</code>","text":"<p>Create a new dataset like another dataset.</p> <p>dataset_like method creates a Dataset from an array like another source dataset. The new dataset will have the same <code>projection</code>, <code>coordinates</code> or the <code>top left corner</code> of the original dataset, <code>cell size</code>, <code>no_data_velue</code>, and number of <code>rows</code> and <code>columns</code>. the array and the source dataset should have the same number of columns and rows</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Dataset</code> <p>source raster to get the spatial information</p> required <code>array</code> <code>ndarray</code> <p>data to store in the new dataset.</p> required <code>path</code> <code>str</code> <p>path to save the new dataset, if not given, the method will return in-memory dataset.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>if the <code>path</code> is given, the method will save the new raster to the given path, else the method will return an in-memory dataset.</p> Hint <ul> <li>If the given array is 3D, the bands have to be the first dimension, the x/lon has to be the second   dimension, and the y/lon has to be the third dimension of the array.</li> </ul> <p>Examples:</p> <ul> <li>Create a source dataset and then create another dataset like it:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Now let's create another <code>dataset</code> from the previous dataset using the <code>dataset_like</code>:</li> </ul> <pre><code>&gt;&gt;&gt; new_arr = np.random.rand(5, 5)\n&gt;&gt;&gt; dataset_new = Dataset.dataset_like(dataset, new_arr)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 5 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>@classmethod\ndef dataset_like(\n    cls,\n    src: \"Dataset\",\n    array: np.ndarray,\n    path: str = None,\n) -&gt; \"Dataset\":\n    \"\"\"Create a new dataset like another dataset.\n\n    dataset_like method creates a Dataset from an array like another source dataset. The new dataset\n    will have the same `projection`, `coordinates` or the `top left corner` of the original dataset,\n    `cell size`, `no_data_velue`, and number of `rows` and `columns`.\n    the array and the source dataset should have the same number of columns and rows\n\n    Args:\n        src (Dataset):\n            source raster to get the spatial information\n        array (ndarray):\n            data to store in the new dataset.\n        path (str, optional):\n            path to save the new dataset, if not given, the method will return in-memory dataset.\n\n    Returns:\n        Dataset:\n            if the `path` is given, the method will save the new raster to the given path, else the method will\n            return an in-memory dataset.\n\n    Hint:\n        - If the given array is 3D, the bands have to be the first dimension, the x/lon has to be the second\n          dimension, and the y/lon has to be the third dimension of the array.\n\n    Examples:\n        - Create a source dataset and then create another dataset like it:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Now let's create another `dataset` from the previous dataset using the `dataset_like`:\n\n          ```python\n          &gt;&gt;&gt; new_arr = np.random.rand(5, 5)\n          &gt;&gt;&gt; dataset_new = Dataset.dataset_like(dataset, new_arr)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 5 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\"array should be of type numpy array\")\n\n    if array.ndim == 2:\n        bands = 1\n    else:\n        bands = array.shape[0]\n\n    dtype = numpy_to_gdal_dtype(array)\n\n    dst = Dataset._create_dataset(src.columns, src.rows, bands, dtype, path=path)\n\n    dst.SetGeoTransform(src.geotransform)\n    dst.SetProjection(src.crs)\n    # setting the NoDataValue does not accept double precision numbers\n    dst_obj = cls(dst, access=\"write\")\n    dst_obj._set_no_data_value(no_data_value=src.no_data_value[0])\n\n    if bands == 1:\n        dst_obj.raster.GetRasterBand(1).WriteArray(array)\n    else:\n        for band_i in range(bands):\n            dst_obj.raster.GetRasterBand(band_i + 1).WriteArray(array[band_i, :, :])\n\n    if path is not None:\n        dst_obj.raster.FlushCache()\n\n    return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.write_array","title":"<code>write_array(array, top_left_corner=None)</code>","text":"<p>Write an array to the dataset at the given xoff, yoff position.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The array to write</p> required <code>top_left_corner</code> <code>List[float, float]</code> <p>indices [row, column]/[y_offset, x_offset] of the cell to write the array to. If None, the array will be written to the top left corner of the dataset.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the array is not written successfully.</p> Hint <ul> <li>The <code>Dataset</code> has to be opened in a write mode <code>read_only=False</code>.</li> </ul> <p>Returns: None</p> <p>Examples:</p> <ul> <li>First, create a dataset on disk:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; path = 'write_array.tif'\n&gt;&gt;&gt; dataset = Dataset.create_from_array(\n...     arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326, path=path\n... )\n&gt;&gt;&gt; dataset = None\n</code></pre> <ul> <li>In a later session you can read the dataset in a <code>write</code> mode and update it:</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(path, read_only=False)\n&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; dataset.write_array(arr, top_left_corner=[1, 1])\n&gt;&gt;&gt; dataset.read_array()    # doctest: +SKIP\narray([[0.77359738, 0.64789596, 0.37912658, 0.03673771, 0.69571106],\n       [0.60804387, 1.        , 2.        , 0.501909  , 0.99597122],\n       [0.83879291, 3.        , 4.        , 0.33058081, 0.59824467],\n       [0.774213  , 0.94338147, 0.16443719, 0.28041457, 0.61914179],\n       [0.97201104, 0.81364799, 0.35157525, 0.65554998, 0.8589739 ]])\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def write_array(self, array: np.array, top_left_corner: List[Any] = None):\n    \"\"\"Write an array to the dataset at the given xoff, yoff position.\n\n    Args:\n        array (np.ndarray):\n            The array to write\n        top_left_corner (List[float, float]):\n            indices [row, column]/[y_offset, x_offset] of the cell to write the array to. If None, the array will\n            be written to the top left corner of the dataset.\n\n    Raises:\n        Exception: If the array is not written successfully.\n\n    Hint:\n        - The `Dataset` has to be opened in a write mode `read_only=False`.\n\n    Returns:\n    None\n\n    Examples:\n        - First, create a dataset on disk:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; path = 'write_array.tif'\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(\n          ...     arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326, path=path\n          ... )\n          &gt;&gt;&gt; dataset = None\n\n          ```\n\n        - In a later session you can read the dataset in a `write` mode and update it:\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.read_file(path, read_only=False)\n          &gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n          &gt;&gt;&gt; dataset.write_array(arr, top_left_corner=[1, 1])\n          &gt;&gt;&gt; dataset.read_array()    # doctest: +SKIP\n          array([[0.77359738, 0.64789596, 0.37912658, 0.03673771, 0.69571106],\n                 [0.60804387, 1.        , 2.        , 0.501909  , 0.99597122],\n                 [0.83879291, 3.        , 4.        , 0.33058081, 0.59824467],\n                 [0.774213  , 0.94338147, 0.16443719, 0.28041457, 0.61914179],\n                 [0.97201104, 0.81364799, 0.35157525, 0.65554998, 0.8589739 ]])\n\n          ```\n    \"\"\"\n    yoff, xoff = top_left_corner\n    try:\n        self._raster.WriteArray(array, xoff=xoff, yoff=yoff)\n        self._raster.FlushCache()\n    except Exception as e:\n        raise e\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.set_crs","title":"<code>set_crs(crs=None, epsg=None)</code>","text":"<p>Set the Coordinate Reference System (CRS).</p> <pre><code>Set the Coordinate Reference System (CRS) of a\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>str</code> <p>Optional if epsg is specified. WKT string. i.e.     <pre><code>'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\", 6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"],\nAUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",\n0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],\nAUTHORITY[\"EPSG\",\"4326\"]]'\n</code></pre></p> <code>None</code> <code>epsg</code> <code>int</code> <p>Optional if crs is specified. EPSG code specifying the projection.</p> <code>None</code> Source code in <code>pyramids/dataset.py</code> <pre><code>def set_crs(self, crs: Optional = None, epsg: int = None):\n    \"\"\"Set the Coordinate Reference System (CRS).\n\n        Set the Coordinate Reference System (CRS) of a\n\n    Args:\n        crs (str):\n            Optional if epsg is specified. WKT string. i.e.\n                ```\n                'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\", 6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"],\n                AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",\n                0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],\n                AUTHORITY[\"EPSG\",\"4326\"]]'\n                ```\n        epsg (int):\n            Optional if crs is specified. EPSG code specifying the projection.\n    \"\"\"\n    # first change the projection of the gdal dataset object\n    # second change the epsg attribute of the Dataset object\n    if self.driver_type == \"ascii\":\n        raise TypeError(\n            \"Setting CRS for ASCII file is not possible, you can save the files to a geotiff and then reset the crs\"\n        )\n    else:\n        if crs is not None:\n            self.raster.SetProjection(crs)\n            self._epsg = FeatureCollection.get_epsg_from_prj(crs)\n        else:\n            sr = Dataset._create_sr_from_epsg(epsg)\n            self.raster.SetProjection(sr.ExportToWkt())\n            self._epsg = epsg\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.to_crs","title":"<code>to_crs(to_epsg, method='nearest neighbor', maintain_alignment=False, inplace=False)</code>","text":"<p>Reproject the dataset to any projection.</p> <pre><code>(default the WGS84 web mercator projection, without resampling)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>to_epsg</code> <code>int</code> <p>reference number to the new projection (https://epsg.io/). Default 3857 is the reference number of WGS84 web mercator.</p> required <code>method</code> <code>str</code> <p>resampling method. Default is \"nearest neighbor\". See https://gisgeography.com/raster-resampling/. Allowed values: \"nearest neighbor\", \"cubic\", \"bilinear\".</p> <code>'nearest neighbor'</code> <code>maintain_alignment</code> <code>bool</code> <p>True to maintain the number of rows and columns of the raster the same after reprojection. Default is False.</p> <code>False</code> <code>inplace</code> <code>bool</code> <p>True to make changes inplace. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, None]</code> <p>Dataset object, if inplace is True, the method returns None.</p> <p>Examples:</p> <ul> <li>Create a dataset and reproject it:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 5 * 5\n            EPSG: 4326\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n&gt;&gt;&gt; print(dataset.crs)\nGEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n&gt;&gt;&gt; print(dataset.epsg)\n4326\n&gt;&gt;&gt; reprojected_dataset = dataset.to_crs(to_epsg=3857)\n&gt;&gt;&gt; print(reprojected_dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 5565.983370404396\n            Dimension: 5 * 5\n            EPSG: 3857\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n&gt;&gt;&gt; print(reprojected_dataset.crs)\nPROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]\n&gt;&gt;&gt; print(reprojected_dataset.epsg)\n3857\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def to_crs(\n    self,\n    to_epsg: int,\n    method: str = \"nearest neighbor\",\n    maintain_alignment: int = False,\n    inplace: bool = False,\n) -&gt; Union[\"Dataset\", None]:\n    \"\"\"Reproject the dataset to any projection.\n\n        (default the WGS84 web mercator projection, without resampling)\n\n    Args:\n        to_epsg (int):\n            reference number to the new projection (https://epsg.io/). Default 3857 is the reference number of WGS84\n            web mercator.\n        method (str):\n            resampling method. Default is \"nearest neighbor\". See https://gisgeography.com/raster-resampling/.\n            Allowed values: \"nearest neighbor\", \"cubic\", \"bilinear\".\n        maintain_alignment (bool):\n            True to maintain the number of rows and columns of the raster the same after reprojection.\n            Default is False.\n        inplace (bool):\n            True to make changes inplace. Default is False.\n\n    Returns:\n        Dataset:\n            Dataset object, if inplace is True, the method returns None.\n\n    Examples:\n        - Create a dataset and reproject it:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 5 * 5\n                      EPSG: 4326\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n          &gt;&gt;&gt; print(dataset.crs)\n          GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n          &gt;&gt;&gt; print(dataset.epsg)\n          4326\n          &gt;&gt;&gt; reprojected_dataset = dataset.to_crs(to_epsg=3857)\n          &gt;&gt;&gt; print(reprojected_dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 5565.983370404396\n                      Dimension: 5 * 5\n                      EPSG: 3857\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n          &gt;&gt;&gt; print(reprojected_dataset.crs)\n          PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]\n          &gt;&gt;&gt; print(reprojected_dataset.epsg)\n          3857\n\n          ```\n\n    \"\"\"\n    if not isinstance(to_epsg, int):\n        raise TypeError(\n            \"please enter correct integer number for to_epsg more information \"\n            f\"https://epsg.io/, given {type(to_epsg)}\"\n        )\n    if not isinstance(method, str):\n        raise TypeError(\n            \"Please enter a correct method, for more information, see documentation \"\n        )\n    if method not in INTERPOLATION_METHODS.keys():\n        raise ValueError(\n            f\"The given interpolation method: {method} does not exist, existing methods are {INTERPOLATION_METHODS.keys()}\"\n        )\n\n    method = INTERPOLATION_METHODS.get(method)\n\n    if maintain_alignment:\n        dst_obj = self._reproject_with_ReprojectImage(to_epsg, method)\n    else:\n        dst = gdal.Warp(\"\", self.raster, dstSRS=f\"EPSG:{to_epsg}\", format=\"VRT\")\n        dst_obj = Dataset(dst)\n\n    if inplace:\n        self.__init__(dst_obj.raster)\n    else:\n        return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.count_domain_cells","title":"<code>count_domain_cells(band=0)</code>","text":"<p>Count cells inside the domain.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of cells.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def count_domain_cells(self, band: int = 0) -&gt; int:\n    \"\"\"Count cells inside the domain.\n\n    Args:\n        band (int):\n            Band index. Default is 0.\n\n    Returns:\n        int:\n            Number of cells.\n    \"\"\"\n    arr = self.read_array(band=band)\n    domain_count = np.size(arr[:, :]) - np.count_nonzero(\n        (arr[np.isclose(arr, self.no_data_value[band], rtol=0.001)])\n    )\n    return domain_count\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.change_no_data_value","title":"<code>change_no_data_value(new_value, old_value=None)</code>","text":"<p>Change No Data Value.</p> <pre><code>- Set the no data value in all raster bands.\n- Fill the whole raster with the no_data_value.\n- Change the no_data_value in the array in all bands.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>new_value</code> <code>numeric</code> <p>No data value to set in the raster bands.</p> required <code>old_value</code> <code>numeric</code> <p>Old no data value that is already in the raster bands.</p> <code>None</code> Warning <p>The <code>change_no_data_value</code> method creates a new dataset in memory in order to change the <code>no_data_value</code> in the raster bands.</p> <p>Examples:</p> <ul> <li>Create a Dataset (4 bands, 10 rows, 10 columns) at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.create(\n...     cell_size=0.05, rows=3, columns=3, bands=1, top_left_corner=(0, 0),dtype=\"float32\",\n...     epsg=4326, no_data_value=-9\n... )\n&gt;&gt;&gt; arr = dataset.read_array()\n&gt;&gt;&gt; print(arr)\n[[-9. -9. -9.]\n [-9. -9. -9.]\n [-9. -9. -9.]]\n&gt;&gt;&gt; print(dataset.no_data_value) # doctest: +SKIP\n[-9.0]\n</code></pre> <ul> <li>The dataset is full of the no_data_value. Now change it using <code>change_no_data_value</code>:</li> </ul> <pre><code>&gt;&gt;&gt; new_dataset = dataset.change_no_data_value(-10, -9)\n&gt;&gt;&gt; arr = new_dataset.read_array()\n&gt;&gt;&gt; print(arr)\n[[-10. -10. -10.]\n [-10. -10. -10.]\n [-10. -10. -10.]]\n&gt;&gt;&gt; print(new_dataset.no_data_value) # doctest: +SKIP\n[-10.0]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def change_no_data_value(self, new_value: Any, old_value: Any = None):\n    \"\"\"Change No Data Value.\n\n        - Set the no data value in all raster bands.\n        - Fill the whole raster with the no_data_value.\n        - Change the no_data_value in the array in all bands.\n\n    Args:\n        new_value (numeric):\n            No data value to set in the raster bands.\n        old_value (numeric):\n            Old no data value that is already in the raster bands.\n\n    Warning:\n        The `change_no_data_value` method creates a new dataset in memory in order to change the `no_data_value` in the raster bands.\n\n    Examples:\n        - Create a Dataset (4 bands, 10 rows, 10 columns) at lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.create(\n          ...     cell_size=0.05, rows=3, columns=3, bands=1, top_left_corner=(0, 0),dtype=\"float32\",\n          ...     epsg=4326, no_data_value=-9\n          ... )\n          &gt;&gt;&gt; arr = dataset.read_array()\n          &gt;&gt;&gt; print(arr)\n          [[-9. -9. -9.]\n           [-9. -9. -9.]\n           [-9. -9. -9.]]\n          &gt;&gt;&gt; print(dataset.no_data_value) # doctest: +SKIP\n          [-9.0]\n\n          ```\n\n        - The dataset is full of the no_data_value. Now change it using `change_no_data_value`:\n\n          ```python\n          &gt;&gt;&gt; new_dataset = dataset.change_no_data_value(-10, -9)\n          &gt;&gt;&gt; arr = new_dataset.read_array()\n          &gt;&gt;&gt; print(arr)\n          [[-10. -10. -10.]\n           [-10. -10. -10.]\n           [-10. -10. -10.]]\n          &gt;&gt;&gt; print(new_dataset.no_data_value) # doctest: +SKIP\n          [-10.0]\n\n          ```\n    \"\"\"\n    if not isinstance(new_value, list):\n        new_value = [new_value] * self.band_count\n\n    if old_value is not None and not isinstance(old_value, list):\n        old_value = [old_value] * self.band_count\n\n    dst = gdal.GetDriverByName(\"MEM\").CreateCopy(\"\", self.raster, 0)\n    # create a new dataset\n    new_dataset = Dataset(dst, \"write\")\n    # the new_value could change inside the _set_no_data_value method before it is used to set the no_data_value\n    # attribute in the gdal object/pyramids object and to fill the band.\n    new_dataset._set_no_data_value(new_value)\n    # now we have to use the no_data_value value in the no_data_value attribute in the Dataset object as it is\n    # updated.\n    new_value = new_dataset.no_data_value\n    for band in range(self.band_count):\n        arr = self.read_array(band)\n        try:\n            if old_value is not None:\n                arr[np.isclose(arr, old_value, rtol=0.001)] = new_value[band]\n            else:\n                arr[np.isnan(arr)] = new_value[band]\n        except TypeError:\n            raise NoDataValueError(\n                f\"The dtype of the given no_data_value: {new_value[band]} differs from the dtype of the \"\n                f\"band: {gdal_to_numpy_dtype(self.gdal_dtype[band])}\"\n            )\n        new_dataset.raster.GetRasterBand(band + 1).WriteArray(arr)\n    return new_dataset\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_cell_coords","title":"<code>get_cell_coords(location='center', mask=False)</code>","text":"<p>Get coordinates for the center/corner of cells inside the dataset domain.</p> <p>Returns the coordinates of the cell centers inside the domain (only the cells that do not have nodata value)</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Location of the coordinates. Use <code>center</code> for the center of a cell, <code>corner</code> for the corner of the cell (top-left corner).</p> <code>'center'</code> <code>mask</code> <code>bool</code> <p>True to exclude the cells out of the domain. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array with a list of the coordinates to be interpolated, without the NaN.</p> <code>ndarray</code> <p>np.ndarray: Array with all the centers of cells in the domain of the DEM.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 1 bands, 3 rows, 3 columns, at the point lon/lat (0, 0).</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the coordinates of the center of cells inside the domain.</li> </ul> <pre><code>&gt;&gt;&gt; coords = dataset.get_cell_coords()\n&gt;&gt;&gt; print(coords)\n[[ 0.025 -0.025]\n [ 0.075 -0.025]\n [ 0.125 -0.025]\n [ 0.025 -0.075]\n [ 0.075 -0.075]\n [ 0.125 -0.075]\n [ 0.025 -0.125]\n [ 0.075 -0.125]\n [ 0.125 -0.125]]\n</code></pre> <ul> <li>Get the coordinates of the top left corner of cells inside the domain.</li> </ul> <pre><code>&gt;&gt;&gt; coords = dataset.get_cell_coords(location=\"corner\")\n&gt;&gt;&gt; print(coords)\n[[ 0.    0.  ]\n [ 0.05  0.  ]\n [ 0.1   0.  ]\n [ 0.   -0.05]\n [ 0.05 -0.05]\n [ 0.1  -0.05]\n [ 0.   -0.1 ]\n [ 0.05 -0.1 ]\n [ 0.1  -0.1 ]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_cell_coords(\n    self, location: str = \"center\", mask: bool = False\n) -&gt; np.ndarray:\n    \"\"\"Get coordinates for the center/corner of cells inside the dataset domain.\n\n    Returns the coordinates of the cell centers inside the domain (only the cells that\n    do not have nodata value)\n\n    Args:\n        location (str):\n            Location of the coordinates. Use `center` for the center of a cell, `corner` for the corner of the\n            cell (top-left corner).\n        mask (bool):\n            True to exclude the cells out of the domain. Default is False.\n\n    Returns:\n        np.ndarray:\n            Array with a list of the coordinates to be interpolated, without the NaN.\n        np.ndarray:\n            Array with all the centers of cells in the domain of the DEM.\n\n    Examples:\n        - Create `Dataset` consists of 1 bands, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Get the coordinates of the center of cells inside the domain.\n\n          ```python\n          &gt;&gt;&gt; coords = dataset.get_cell_coords()\n          &gt;&gt;&gt; print(coords)\n          [[ 0.025 -0.025]\n           [ 0.075 -0.025]\n           [ 0.125 -0.025]\n           [ 0.025 -0.075]\n           [ 0.075 -0.075]\n           [ 0.125 -0.075]\n           [ 0.025 -0.125]\n           [ 0.075 -0.125]\n           [ 0.125 -0.125]]\n\n          ```\n\n        - Get the coordinates of the top left corner of cells inside the domain.\n\n          ```python\n          &gt;&gt;&gt; coords = dataset.get_cell_coords(location=\"corner\")\n          &gt;&gt;&gt; print(coords)\n          [[ 0.    0.  ]\n           [ 0.05  0.  ]\n           [ 0.1   0.  ]\n           [ 0.   -0.05]\n           [ 0.05 -0.05]\n           [ 0.1  -0.05]\n           [ 0.   -0.1 ]\n           [ 0.05 -0.1 ]\n           [ 0.1  -0.1 ]]\n\n          ```\n    \"\"\"\n    # check the location parameter\n    location = location.lower()\n    if location not in [\"center\", \"corner\"]:\n        raise ValueError(\n            \"The location parameter can have one of these values: 'center', 'corner', \"\n            f\"but the value: {location} is given.\"\n        )\n\n    if location == \"center\":\n        # Adding 0.5*cell size to get the center\n        add_value = 0.5\n    else:\n        add_value = 0\n    # Getting data for the whole grid\n    (\n        x_init,\n        cell_size_x,\n        xy_span,\n        y_init,\n        yy_span,\n        cell_size_y,\n    ) = self.geotransform\n    if cell_size_x != cell_size_y:\n        if np.abs(cell_size_x) != np.abs(cell_size_y):\n            logger.warning(\n                f\"The given raster does not have a square cells, the cell size is {cell_size_x}*{cell_size_y} \"\n            )\n\n    # data in the array\n    no_val = self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n    arr = self.read_array(band=0)\n    if mask is not None and no_val not in arr:\n        logger.warning(\n            \"The no data value does not exist in the band, so all the cells will be considered, and the \"\n            \"mask will not be considered.\"\n        )\n\n    if mask:\n        mask = [no_val]\n    else:\n        mask = None\n    indices = get_indices2(arr, mask=mask)\n\n    # exclude the no_data_values cells.\n    f1 = [i[0] for i in indices]\n    f2 = [i[1] for i in indices]\n    x = [x_init + cell_size_x * (i + add_value) for i in f2]\n    y = [y_init + cell_size_y * (i + add_value) for i in f1]\n    coords = np.array(list(zip(x, y)))\n\n    return coords\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_cell_polygons","title":"<code>get_cell_polygons(mask=False)</code>","text":"<p>Get a polygon shapely geometry for the raster cells.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>bool</code> <p>True to get the polygons of the cells inside the domain.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <code>GeoDataFrame</code> <p>With two columns, geometry, and id.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the coordinates of the center of cells inside the domain.</li> </ul> <pre><code>&gt;&gt;&gt; gdf = dataset.get_cell_polygons()\n&gt;&gt;&gt; print(gdf)\n                                       geometry  id\n0  POLYGON ((0 0, 0.05 0, 0.05 -0.05, 0 -0.05, 0 0))   0\n1  POLYGON ((0.05 0, 0.1 0, 0.1 -0.05, 0.05 -0.05...   1\n2  POLYGON ((0.1 0, 0.15 0, 0.15 -0.05, 0.1 -0.05...   2\n3  POLYGON ((0 -0.05, 0.05 -0.05, 0.05 -0.1, 0 -0...   3\n4  POLYGON ((0.05 -0.05, 0.1 -0.05, 0.1 -0.1, 0.0...   4\n5  POLYGON ((0.1 -0.05, 0.15 -0.05, 0.15 -0.1, 0....   5\n6  POLYGON ((0 -0.1, 0.05 -0.1, 0.05 -0.15, 0 -0....   6\n7  POLYGON ((0.05 -0.1, 0.1 -0.1, 0.1 -0.15, 0.05...   7\n8  POLYGON ((0.1 -0.1, 0.15 -0.1, 0.15 -0.15, 0.1...   8\n&gt;&gt;&gt; fig, ax = dataset.plot()\n&gt;&gt;&gt; gdf.plot(ax=ax, facecolor='none', edgecolor=\"gray\", linewidth=2)\n&lt;Axes: &gt;\n</code></pre> <p></p> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_cell_polygons(self, mask: bool = False) -&gt; GeoDataFrame:\n    \"\"\"Get a polygon shapely geometry for the raster cells.\n\n    Args:\n        mask (bool):\n            True to get the polygons of the cells inside the domain.\n\n    Returns:\n        GeoDataFrame:\n            With two columns, geometry, and id.\n\n    Examples:\n        - Create `Dataset` consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Get the coordinates of the center of cells inside the domain.\n\n          ```python\n          &gt;&gt;&gt; gdf = dataset.get_cell_polygons()\n          &gt;&gt;&gt; print(gdf)\n                                                 geometry  id\n          0  POLYGON ((0 0, 0.05 0, 0.05 -0.05, 0 -0.05, 0 0))   0\n          1  POLYGON ((0.05 0, 0.1 0, 0.1 -0.05, 0.05 -0.05...   1\n          2  POLYGON ((0.1 0, 0.15 0, 0.15 -0.05, 0.1 -0.05...   2\n          3  POLYGON ((0 -0.05, 0.05 -0.05, 0.05 -0.1, 0 -0...   3\n          4  POLYGON ((0.05 -0.05, 0.1 -0.05, 0.1 -0.1, 0.0...   4\n          5  POLYGON ((0.1 -0.05, 0.15 -0.05, 0.15 -0.1, 0....   5\n          6  POLYGON ((0 -0.1, 0.05 -0.1, 0.05 -0.15, 0 -0....   6\n          7  POLYGON ((0.05 -0.1, 0.1 -0.1, 0.1 -0.15, 0.05...   7\n          8  POLYGON ((0.1 -0.1, 0.15 -0.1, 0.15 -0.15, 0.1...   8\n          &gt;&gt;&gt; fig, ax = dataset.plot()\n          &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='none', edgecolor=\"gray\", linewidth=2)\n          &lt;Axes: &gt;\n\n          ```\n\n    ![get_cell_polygons](./../_images/dataset/get_cell_polygons.png)\n    \"\"\"\n    coords = self.get_cell_coords(location=\"corner\", mask=mask)\n    cell_size = self.geotransform[1]\n    epsg = self._get_epsg()\n    x = np.zeros((coords.shape[0], 4))\n    y = np.zeros((coords.shape[0], 4))\n    # fill the top left corner point\n    x[:, 0] = coords[:, 0]\n    y[:, 0] = coords[:, 1]\n    # fill the top right\n    x[:, 1] = x[:, 0] + cell_size\n    y[:, 1] = y[:, 0]\n    # fill the bottom right\n    x[:, 2] = x[:, 0] + cell_size\n    y[:, 2] = y[:, 0] - cell_size\n\n    # fill the bottom left\n    x[:, 3] = x[:, 0]\n    y[:, 3] = y[:, 0] - cell_size\n\n    coords_tuples = [list(zip(x[:, i], y[:, i])) for i in range(4)]\n    polys_coords = [\n        (\n            coords_tuples[0][i],\n            coords_tuples[1][i],\n            coords_tuples[2][i],\n            coords_tuples[3][i],\n        )\n        for i in range(len(x))\n    ]\n    polygons = list(map(FeatureCollection.create_polygon, polys_coords))\n    gdf = gpd.GeoDataFrame(geometry=polygons)\n    gdf.set_crs(epsg=epsg, inplace=True)\n    gdf[\"id\"] = gdf.index\n    return gdf\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_cell_points","title":"<code>get_cell_points(location='center', mask=False)</code>","text":"<p>Get a point shapely geometry for the raster cells center point.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Location of the point, [\"corner\", \"center\"]. Default is \"center\".</p> <code>'center'</code> <code>mask</code> <code>bool</code> <p>True to get the polygons of the cells inside the domain.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <code>GeoDataFrame</code> <p>With two columns, geometry, and id.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Get the coordinates of the center of cells inside the domain.</li> </ul> <pre><code>&gt;&gt;&gt; gdf = dataset.get_cell_points()\n&gt;&gt;&gt; print(gdf)\n               geometry  id\n0  POINT (0.025 -0.025)   0\n1  POINT (0.075 -0.025)   1\n2  POINT (0.125 -0.025)   2\n3  POINT (0.025 -0.075)   3\n4  POINT (0.075 -0.075)   4\n5  POINT (0.125 -0.075)   5\n6  POINT (0.025 -0.125)   6\n7  POINT (0.075 -0.125)   7\n8  POINT (0.125 -0.125)   8\n&gt;&gt;&gt; fig, ax = dataset.plot()\n&gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=2)\n&lt;Axes: &gt;\n</code></pre> <p></p> <ul> <li>Get the coordinates of the top left corner of cells inside the domain.</li> </ul> <pre><code>&gt;&gt;&gt; gdf = dataset.get_cell_points(location=\"corner\")\n&gt;&gt;&gt; print(gdf)\n            geometry  id\n0         POINT (0 0)   0\n1      POINT (0.05 0)   1\n2       POINT (0.1 0)   2\n3     POINT (0 -0.05)   3\n4  POINT (0.05 -0.05)   4\n5   POINT (0.1 -0.05)   5\n6      POINT (0 -0.1)   6\n7   POINT (0.05 -0.1)   7\n8    POINT (0.1 -0.1)   8\n&gt;&gt;&gt; fig, ax = dataset.plot()\n&gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=4)\n&lt;Axes: &gt;\n</code></pre> <p></p> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_cell_points(self, location: str = \"center\", mask=False) -&gt; GeoDataFrame:\n    \"\"\"Get a point shapely geometry for the raster cells center point.\n\n    Args:\n        location (str):\n            Location of the point, [\"corner\", \"center\"]. Default is \"center\".\n        mask (bool):\n            True to get the polygons of the cells inside the domain.\n\n    Returns:\n        GeoDataFrame:\n            With two columns, geometry, and id.\n\n    Examples:\n        - Create `Dataset` consists of 1 band, 3 rows, 3 columns, at the point lon/lat (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1,3, size=(3, 3))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Get the coordinates of the center of cells inside the domain.\n\n          ```python\n          &gt;&gt;&gt; gdf = dataset.get_cell_points()\n          &gt;&gt;&gt; print(gdf)\n                         geometry  id\n          0  POINT (0.025 -0.025)   0\n          1  POINT (0.075 -0.025)   1\n          2  POINT (0.125 -0.025)   2\n          3  POINT (0.025 -0.075)   3\n          4  POINT (0.075 -0.075)   4\n          5  POINT (0.125 -0.075)   5\n          6  POINT (0.025 -0.125)   6\n          7  POINT (0.075 -0.125)   7\n          8  POINT (0.125 -0.125)   8\n          &gt;&gt;&gt; fig, ax = dataset.plot()\n          &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=2)\n          &lt;Axes: &gt;\n\n          ```\n\n        ![get_cell_points](./../_images/dataset/get_cell_points.png)\n\n        - Get the coordinates of the top left corner of cells inside the domain.\n\n          ```python\n          &gt;&gt;&gt; gdf = dataset.get_cell_points(location=\"corner\")\n          &gt;&gt;&gt; print(gdf)\n                      geometry  id\n          0         POINT (0 0)   0\n          1      POINT (0.05 0)   1\n          2       POINT (0.1 0)   2\n          3     POINT (0 -0.05)   3\n          4  POINT (0.05 -0.05)   4\n          5   POINT (0.1 -0.05)   5\n          6      POINT (0 -0.1)   6\n          7   POINT (0.05 -0.1)   7\n          8    POINT (0.1 -0.1)   8\n          &gt;&gt;&gt; fig, ax = dataset.plot()\n          &gt;&gt;&gt; gdf.plot(ax=ax, facecolor='black', linewidth=4)\n          &lt;Axes: &gt;\n\n          ```\n\n        ![get_cell_points-corner](./../_images/dataset/get_cell_points-corner.png)\n    \"\"\"\n    coords = self.get_cell_coords(location=location, mask=mask)\n    epsg = self._get_epsg()\n\n    coords_tuples = list(zip(coords[:, 0], coords[:, 1]))\n    points = FeatureCollection.create_point(coords_tuples)\n    gdf = gpd.GeoDataFrame(geometry=points)\n    gdf.set_crs(epsg=epsg, inplace=True)\n    gdf[\"id\"] = gdf.index\n    return gdf\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.to_file","title":"<code>to_file(path, band=0, tile_length=None)</code>","text":"<p>Save dataset to tiff file.</p> <pre><code>`to_file` saves a raster to disk, the type of the driver (georiff/netcdf/ascii) will be implied from the\nextension at the end of the given path.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>A path including the name of the dataset.</p> required <code>band</code> <code>int</code> <p>Band index, needed only in case of ascii drivers. Default is 0.</p> <code>0</code> <code>tile_length</code> <code>int</code> <p>Length of the tiles in the driver. Default is 256.</p> <code>None</code> <p>Examples:</p> <ul> <li>Create a Dataset with 4 bands, 5 rows, 5 columns, at the point lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset.file_name)\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>Now save the dataset as a geotiff file:</li> </ul> <pre><code>&gt;&gt;&gt; dataset.to_file(\"my-dataset.tif\")\n&gt;&gt;&gt; print(dataset.file_name)\nmy-dataset.tif\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def to_file(self, path: str, band: int = 0, tile_length: int = None) -&gt; None:\n    \"\"\"Save dataset to tiff file.\n\n        `to_file` saves a raster to disk, the type of the driver (georiff/netcdf/ascii) will be implied from the\n        extension at the end of the given path.\n\n    Args:\n        path (str):\n            A path including the name of the dataset.\n        band (int):\n            Band index, needed only in case of ascii drivers. Default is 0.\n        tile_length (int, optional):\n            Length of the tiles in the driver. Default is 256.\n\n    Examples:\n        - Create a Dataset with 4 bands, 5 rows, 5 columns, at the point lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset.file_name)\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - Now save the dataset as a geotiff file:\n\n          ```python\n          &gt;&gt;&gt; dataset.to_file(\"my-dataset.tif\")\n          &gt;&gt;&gt; print(dataset.file_name)\n          my-dataset.tif\n\n          ```\n    \"\"\"\n    if not isinstance(path, str):\n        raise TypeError(\"path input should be string type\")\n\n    extension = path.split(\".\")[-1]\n    driver = CATALOG.get_driver_name_by_extension(extension)\n    driver_name = CATALOG.get_gdal_name(driver)\n\n    if driver == \"ascii\":\n        arr = self.read_array(band=band)\n        no_data_value = self.no_data_value[band]\n        xmin, ymin, _, _ = self.bbox\n        _io.to_ascii(arr, self.cell_size, xmin, ymin, no_data_value, path)\n    else:\n        # saving rasters with color table fails with a runtime error\n        options = [\"COMPRESS=DEFLATE\"]\n        if tile_length is not None:\n            options += [\n                \"TILED=YES\",\n                f\"TILE_LENGTH={tile_length}\",\n            ]\n        if self._block_size is not None and self._block_size != []:\n            options += [\n                \"BLOCKXSIZE={}\".format(self._block_size[0][0]),\n                \"BLOCKYSIZE={}\".format(self._block_size[0][1]),\n            ]\n\n        try:\n            dst = gdal.GetDriverByName(driver_name).CreateCopy(\n                path, self.raster, 0, options=options\n            )\n            self.__init__(dst, \"write\")\n            # flush the data to the dataset on disk.\n            dst.FlushCache()\n        except RuntimeError:\n            if not os.path.exists(path):\n                raise FailedToSaveError(\n                    f\"Failed to save the {driver_name} raster to the path: {path}\"\n                )\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.convert_longitude","title":"<code>convert_longitude(inplace=False)</code>","text":"<p>Convert Longitude.</p> <ul> <li>convert the longitude from 0-360 to -180 - 180.</li> <li>currently the function works correctly if the raster covers the whole world, it means that the columns     in the rasters covers from longitude 0 to 360.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>True to make the changes in place.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Optional[Dataset]</code> <p>The converted dataset if inplace is False; otherwise None.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def convert_longitude(self, inplace: bool = False) -&gt; Optional[\"Dataset\"]:\n    \"\"\"Convert Longitude.\n\n    - convert the longitude from 0-360 to -180 - 180.\n    - currently the function works correctly if the raster covers the whole world, it means that the columns\n        in the rasters covers from longitude 0 to 360.\n\n    Args:\n        inplace (bool):\n            True to make the changes in place.\n\n    Returns:\n        Dataset:\n            The converted dataset if inplace is False; otherwise None.\n    \"\"\"\n    # dst = gdal.Warp(\n    #     \"\",\n    #     self.raster,\n    #     dstSRS=\"+proj=longlat +ellps=WGS84 +datum=WGS84 +lon_0=0 +over\",\n    #     format=\"VRT\",\n    # )\n    lon = self.lon\n    src = self.raster\n    # create a copy\n    drv = gdal.GetDriverByName(\"MEM\")\n    dst = drv.CreateCopy(\"\", src, 0)\n    # convert the 0 to 360 to -180 to 180\n    if lon[-1] &lt;= 180:\n        raise ValueError(\"The raster should cover the whole globe\")\n\n    first_to_translated = np.where(lon &gt; 180)[0][0]\n\n    ind = list(range(first_to_translated, len(lon)))\n    ind_2 = list(range(0, first_to_translated))\n\n    for band in range(self.band_count):\n        arr = self.read_array(band=band)\n        arr_rearranged = arr[:, ind + ind_2]\n        dst.GetRasterBand(band + 1).WriteArray(arr_rearranged)\n\n    # correct the geotransform\n    top_left_corner = self.top_left_corner\n    gt = list(self.geotransform)\n    if lon[-1] &gt; 180:\n        new_gt = top_left_corner[0] - 180\n        gt[0] = new_gt\n\n    dst.SetGeoTransform(gt)\n    if not inplace:\n        return Dataset(dst)\n    else:\n        self.__init__(dst)\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.to_feature_collection","title":"<code>to_feature_collection(vector_mask=None, add_geometry=None, tile=False, tile_size=256, touch=True)</code>","text":"<p>Convert a dataset to a vector.</p> The function does the following <ul> <li>Flatten the array in each band in the raster then mask the values if a vector_mask file is given     otherwise it will flatten all values.</li> <li>Put the values for each band in a column in a dataframe under the name of the raster band,     but if no meta-data in the raster band exists, an index number will be used [1, 2, 3, ...]</li> <li> <p>The function has an add_geometry parameter with two possible values [\"point\", \"polygon\"], which you can     specify the type of shapely geometry you want to create from each cell,</p> <ul> <li>If point is chosen, the created point will be at the center of each cell</li> <li>If a polygon is chosen, a square polygon will be created that covers the entire cell.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>vector_mask</code> <code>GeoDataFrame</code> <p>GeoDataFrame for the vector_mask. If given, it will be used to clip the raster.</p> <code>None</code> <code>add_geometry</code> <code>str</code> <p>\"Polygon\" or \"Point\" if you want to add a polygon geometry of the cells as column in dataframe. Default is None.</p> <code>None</code> <code>tile</code> <code>bool</code> <p>True to use tiles in extracting the values from the raster. Default is False.</p> <code>False</code> <code>tile_size</code> <code>int</code> <p>Tile size. Default is 1500.</p> <code>256</code> <code>touch</code> <code>bool</code> <p>Include the cells that touch the polygon not only those that lie entirely inside the polygon mask. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, GeoDataFrame]</code> <p>DataFrame | GeoDataFrame: The resulting frame will have the band value under the name of the band (if the raster file has metadata; if not, the bands will be indexed from 1 to the number of bands).</p> <p>Examples:</p> <ul> <li>Create a dataset from array with 2 bands and 3*3 array each:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(2, 3, 3)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset.read_array(band=0)) # doctest: +SKIP\n[[0.88625832 0.81804328 0.99372706]\n [0.85333054 0.35448201 0.78079262]\n [0.43887136 0.68166208 0.53170966]]\n&gt;&gt;&gt; print(dataset.read_array(band=1)) # doctest: +SKIP\n[[0.07051872 0.67650833 0.17625027]\n [0.41258071 0.38327938 0.18783139]\n [0.83741314 0.70446373 0.64913575]]\n</code></pre> <ul> <li>Convert the dataset to dataframe by calling the <code>to_feature_collection</code> method:</li> </ul> <pre><code>&gt;&gt;&gt; df = dataset.to_feature_collection()\n&gt;&gt;&gt; print(df) # doctest: +SKIP\n     Band_1    Band_2\n0  0.886258  0.070519\n1  0.818043  0.676508\n2  0.993727  0.176250\n3  0.853331  0.412581\n4  0.354482  0.383279\n5  0.780793  0.187831\n6  0.438871  0.837413\n7  0.681662  0.704464\n8  0.531710  0.649136\n</code></pre> <ul> <li> <p>Convert the dataset into geodataframe with either a polygon or a point geometry that represents each cell.     To specify the geometry type use the parameter <code>add_geometry</code>:</p> <pre><code>&gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"point\")\n&gt;&gt;&gt; print(gdf) # doctest: +SKIP\n     Band_1    Band_2                  geometry\n0  0.886258  0.070519  POINT (0.02500 -0.02500)\n1  0.818043  0.676508  POINT (0.07500 -0.02500)\n2  0.993727  0.176250  POINT (0.12500 -0.02500)\n3  0.853331  0.412581  POINT (0.02500 -0.07500)\n4  0.354482  0.383279  POINT (0.07500 -0.07500)\n5  0.780793  0.187831  POINT (0.12500 -0.07500)\n6  0.438871  0.837413  POINT (0.02500 -0.12500)\n7  0.681662  0.704464  POINT (0.07500 -0.12500)\n8  0.531710  0.649136  POINT (0.12500 -0.12500)\n&gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"polygon\")\n&gt;&gt;&gt; print(gdf) # doctest: +SKIP\n     Band_1    Band_2                                           geometry\n0  0.886258  0.070519  POLYGON ((0.00000 0.00000, 0.05000 0.00000, 0....\n1  0.818043  0.676508  POLYGON ((0.05000 0.00000, 0.10000 0.00000, 0....\n2  0.993727  0.176250  POLYGON ((0.10000 0.00000, 0.15000 0.00000, 0....\n3  0.853331  0.412581  POLYGON ((0.00000 -0.05000, 0.05000 -0.05000, ...\n4  0.354482  0.383279  POLYGON ((0.05000 -0.05000, 0.10000 -0.05000, ...\n5  0.780793  0.187831  POLYGON ((0.10000 -0.05000, 0.15000 -0.05000, ...\n6  0.438871  0.837413  POLYGON ((0.00000 -0.10000, 0.05000 -0.10000, ...\n7  0.681662  0.704464  POLYGON ((0.05000 -0.10000, 0.10000 -0.10000, ...\n8  0.531710  0.649136  POLYGON ((0.10000 -0.10000, 0.15000 -0.10000, ...\n</code></pre> </li> <li> <p>Use a mask to crop part of the dataset, and then convert the cropped part to a dataframe/geodataframe:</p> </li> <li> <p>Create a mask that covers only the cell in the middle of the dataset.</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; poly = gpd.GeoDataFrame(\n...             geometry=[Polygon([(0.05, -0.05), (0.05, -0.1), (0.1, -0.1), (0.1, -0.05)])], crs=4326\n... )\n&gt;&gt;&gt; df = dataset.to_feature_collection(vector_mask=poly)\n&gt;&gt;&gt; print(df) # doctest: +SKIP\n     Band_1    Band_2\n0  0.354482  0.383279\n</code></pre> </li> <li> <p>If you have a big dataset, and you want to convert it to dataframe in tiles (do not read the whole dataset     at once but in tiles), you can use the <code>tile</code> and the <code>tile_size</code> parameters. The values will be the     same as above; the difference is reading in chunks:</p> <pre><code>&gt;&gt;&gt; gdf = dataset.to_feature_collection(tile=True, tile_size=1)\n&gt;&gt;&gt; print(gdf) # doctest: +SKIP\n     Band_1    Band_2\n0  0.886258  0.070519\n1  0.818043  0.676508\n2  0.993727  0.176250\n3  0.853331  0.412581\n4  0.354482  0.383279\n5  0.780793  0.187831\n6  0.438871  0.837413\n7  0.681662  0.704464\n8  0.531710  0.649136\n</code></pre> </li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def to_feature_collection(\n    self,\n    vector_mask: GeoDataFrame = None,\n    add_geometry: str = None,\n    tile: bool = False,\n    tile_size: int = 256,\n    touch: bool = True,\n) -&gt; Union[DataFrame, GeoDataFrame]:\n    \"\"\"Convert a dataset to a vector.\n\n    The function does the following:\n        - Flatten the array in each band in the raster then mask the values if a vector_mask file is given\n            otherwise it will flatten all values.\n        - Put the values for each band in a column in a dataframe under the name of the raster band,\n            but if no meta-data in the raster band exists, an index number will be used [1, 2, 3, ...]\n        - The function has an add_geometry parameter with two possible values [\"point\", \"polygon\"], which you can\n            specify the type of shapely geometry you want to create from each cell,\n\n            - If point is chosen, the created point will be at the center of each cell\n            - If a polygon is chosen, a square polygon will be created that covers the entire cell.\n\n    Args:\n        vector_mask (GeoDataFrame, optional):\n            GeoDataFrame for the vector_mask. If given, it will be used to clip the raster.\n        add_geometry (str):\n            \"Polygon\" or \"Point\" if you want to add a polygon geometry of the cells as column in dataframe.\n            Default is None.\n        tile (bool):\n            True to use tiles in extracting the values from the raster. Default is False.\n        tile_size (int):\n            Tile size. Default is 1500.\n        touch (bool):\n            Include the cells that touch the polygon not only those that lie entirely inside the polygon mask.\n            Default is True.\n\n    Returns:\n        DataFrame | GeoDataFrame:\n            The resulting frame will have the band value under the name of the band (if the raster file has\n            metadata; if not, the bands will be indexed from 1 to the number of bands).\n\n    Examples:\n        - Create a dataset from array with 2 bands and 3*3 array each:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(2, 3, 3)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset.read_array(band=0)) # doctest: +SKIP\n          [[0.88625832 0.81804328 0.99372706]\n           [0.85333054 0.35448201 0.78079262]\n           [0.43887136 0.68166208 0.53170966]]\n          &gt;&gt;&gt; print(dataset.read_array(band=1)) # doctest: +SKIP\n          [[0.07051872 0.67650833 0.17625027]\n           [0.41258071 0.38327938 0.18783139]\n           [0.83741314 0.70446373 0.64913575]]\n\n          ```\n\n        - Convert the dataset to dataframe by calling the `to_feature_collection` method:\n\n          ```python\n          &gt;&gt;&gt; df = dataset.to_feature_collection()\n          &gt;&gt;&gt; print(df) # doctest: +SKIP\n               Band_1    Band_2\n          0  0.886258  0.070519\n          1  0.818043  0.676508\n          2  0.993727  0.176250\n          3  0.853331  0.412581\n          4  0.354482  0.383279\n          5  0.780793  0.187831\n          6  0.438871  0.837413\n          7  0.681662  0.704464\n          8  0.531710  0.649136\n\n          ```\n\n        - Convert the dataset into geodataframe with either a polygon or a point geometry that represents each cell.\n            To specify the geometry type use the parameter `add_geometry`:\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"point\")\n              &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                   Band_1    Band_2                  geometry\n              0  0.886258  0.070519  POINT (0.02500 -0.02500)\n              1  0.818043  0.676508  POINT (0.07500 -0.02500)\n              2  0.993727  0.176250  POINT (0.12500 -0.02500)\n              3  0.853331  0.412581  POINT (0.02500 -0.07500)\n              4  0.354482  0.383279  POINT (0.07500 -0.07500)\n              5  0.780793  0.187831  POINT (0.12500 -0.07500)\n              6  0.438871  0.837413  POINT (0.02500 -0.12500)\n              7  0.681662  0.704464  POINT (0.07500 -0.12500)\n              8  0.531710  0.649136  POINT (0.12500 -0.12500)\n              &gt;&gt;&gt; gdf = dataset.to_feature_collection(add_geometry=\"polygon\")\n              &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                   Band_1    Band_2                                           geometry\n              0  0.886258  0.070519  POLYGON ((0.00000 0.00000, 0.05000 0.00000, 0....\n              1  0.818043  0.676508  POLYGON ((0.05000 0.00000, 0.10000 0.00000, 0....\n              2  0.993727  0.176250  POLYGON ((0.10000 0.00000, 0.15000 0.00000, 0....\n              3  0.853331  0.412581  POLYGON ((0.00000 -0.05000, 0.05000 -0.05000, ...\n              4  0.354482  0.383279  POLYGON ((0.05000 -0.05000, 0.10000 -0.05000, ...\n              5  0.780793  0.187831  POLYGON ((0.10000 -0.05000, 0.15000 -0.05000, ...\n              6  0.438871  0.837413  POLYGON ((0.00000 -0.10000, 0.05000 -0.10000, ...\n              7  0.681662  0.704464  POLYGON ((0.05000 -0.10000, 0.10000 -0.10000, ...\n              8  0.531710  0.649136  POLYGON ((0.10000 -0.10000, 0.15000 -0.10000, ...\n\n              ```\n\n        - Use a mask to crop part of the dataset, and then convert the cropped part to a dataframe/geodataframe:\n\n          - Create a mask that covers only the cell in the middle of the dataset.\n\n              ```python\n              &gt;&gt;&gt; import geopandas as gpd\n              &gt;&gt;&gt; from shapely.geometry import Polygon\n              &gt;&gt;&gt; poly = gpd.GeoDataFrame(\n              ...             geometry=[Polygon([(0.05, -0.05), (0.05, -0.1), (0.1, -0.1), (0.1, -0.05)])], crs=4326\n              ... )\n              &gt;&gt;&gt; df = dataset.to_feature_collection(vector_mask=poly)\n              &gt;&gt;&gt; print(df) # doctest: +SKIP\n                   Band_1    Band_2\n              0  0.354482  0.383279\n\n              ```\n\n        - If you have a big dataset, and you want to convert it to dataframe in tiles (do not read the whole dataset\n            at once but in tiles), you can use the `tile` and the `tile_size` parameters. The values will be the\n            same as above; the difference is reading in chunks:\n\n              ```python\n              &gt;&gt;&gt; gdf = dataset.to_feature_collection(tile=True, tile_size=1)\n              &gt;&gt;&gt; print(gdf) # doctest: +SKIP\n                   Band_1    Band_2\n              0  0.886258  0.070519\n              1  0.818043  0.676508\n              2  0.993727  0.176250\n              3  0.853331  0.412581\n              4  0.354482  0.383279\n              5  0.780793  0.187831\n              6  0.438871  0.837413\n              7  0.681662  0.704464\n              8  0.531710  0.649136\n\n              ```\n\n    \"\"\"\n    # Get raster band names. open the dataset using gdal.Open\n    band_names = self.band_names\n\n    # Create a mask from the pixels touched by the vector_mask.\n    if vector_mask is not None:\n        src = self.crop(mask=vector_mask, touch=touch)\n    else:\n        src = self\n\n    if tile:\n        df_list = []  # DataFrames of each tile.\n        for arr in self.get_tile(tile_size):\n            # Assume multi-band\n            idx = (1, 2)\n            if arr.ndim == 2:\n                # Handle single band rasters\n                idx = (0, 1)\n\n            mask_arr = np.ones((arr.shape[idx[0]], arr.shape[idx[1]]))\n            pixels = get_pixels(arr, mask_arr).transpose()\n            df_list.append(pd.DataFrame(pixels, columns=band_names))\n\n        # Merge all the tiles.\n        df = pd.concat(df_list)\n    else:\n        arr = src.read_array()\n\n        if self.band_count == 1:\n            pixels = arr.flatten()\n        else:\n            pixels = (\n                arr.flatten()\n                .reshape(src.band_count, src.columns * src.rows)\n                .transpose()\n            )\n        df = pd.DataFrame(pixels, columns=band_names)\n        # mask no data values.\n        if src.no_data_value[0] is not None:\n            df.replace(src.no_data_value[0], np.nan, inplace=True)\n        df.dropna(axis=0, inplace=True, ignore_index=True)\n\n    if add_geometry:\n        if add_geometry.lower() == \"point\":\n            coords = src.get_cell_points(mask=True)\n        else:\n            coords = src.get_cell_polygons(mask=True)\n\n    df.drop(columns=[\"burn_value\", \"geometry\"], errors=\"ignore\", inplace=True)\n    if add_geometry:\n        df = gpd.GeoDataFrame(df.loc[:], geometry=coords[\"geometry\"].to_list())\n        df.set_crs(coords.crs.to_epsg())\n\n    return df\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.apply","title":"<code>apply(func, band=0)</code>","text":"<p>Apply a function to all domain cells.</p> <ul> <li>apply method executes a mathematical operation on the raster array.</li> <li>The apply method executes the function only on one cell at a time.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>function</code> <p>Defined function that takes one input (the cell value).</p> required <code>band</code> <code>int</code> <p>Band number.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset object.</p> <p>Examples:</p> <ul> <li>Create a dataset from an array filled with values between -1 and 1:</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.uniform(-1, 1, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n[[ 0.94997539 -0.80083622 -0.30948769 -0.77439961 -0.83836424]\n [-0.36810158 -0.23979251  0.88051216 -0.46882913  0.64511056]\n [ 0.50585374 -0.46905902  0.67856589  0.2779605   0.05589759]\n [ 0.63382852 -0.49259597  0.18471423 -0.49308984 -0.52840286]\n [-0.34076174 -0.53073014 -0.18485789 -0.40033474 -0.38962938]]\n</code></pre> <ul> <li>Apply the absolute function to the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; abs_dataset = dataset.apply(np.abs)\n&gt;&gt;&gt; print(abs_dataset.read_array()) # doctest: +SKIP\n[[0.94997539 0.80083622 0.30948769 0.77439961 0.83836424]\n [0.36810158 0.23979251 0.88051216 0.46882913 0.64511056]\n [0.50585374 0.46905902 0.67856589 0.2779605  0.05589759]\n [0.63382852 0.49259597 0.18471423 0.49308984 0.52840286]\n [0.34076174 0.53073014 0.18485789 0.40033474 0.38962938]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def apply(self, func, band: int = 0) -&gt; \"Dataset\":\n    \"\"\"Apply a function to all domain cells.\n\n    - apply method executes a mathematical operation on the raster array.\n    - The apply method executes the function only on one cell at a time.\n\n    Args:\n        func (function):\n            Defined function that takes one input (the cell value).\n        band (int):\n            Band number.\n\n    Returns:\n        Dataset:\n            Dataset object.\n\n    Examples:\n        - Create a dataset from an array filled with values between -1 and 1:\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.uniform(-1, 1, size=(5, 5))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n          [[ 0.94997539 -0.80083622 -0.30948769 -0.77439961 -0.83836424]\n           [-0.36810158 -0.23979251  0.88051216 -0.46882913  0.64511056]\n           [ 0.50585374 -0.46905902  0.67856589  0.2779605   0.05589759]\n           [ 0.63382852 -0.49259597  0.18471423 -0.49308984 -0.52840286]\n           [-0.34076174 -0.53073014 -0.18485789 -0.40033474 -0.38962938]]\n\n          ```\n\n        - Apply the absolute function to the dataset:\n\n          ```python\n          &gt;&gt;&gt; abs_dataset = dataset.apply(np.abs)\n          &gt;&gt;&gt; print(abs_dataset.read_array()) # doctest: +SKIP\n          [[0.94997539 0.80083622 0.30948769 0.77439961 0.83836424]\n           [0.36810158 0.23979251 0.88051216 0.46882913 0.64511056]\n           [0.50585374 0.46905902 0.67856589 0.2779605  0.05589759]\n           [0.63382852 0.49259597 0.18471423 0.49308984 0.52840286]\n           [0.34076174 0.53073014 0.18485789 0.40033474 0.38962938]]\n\n          ```\n    \"\"\"\n    if not callable(func):\n        raise TypeError(\"The second argument should be a function\")\n\n    no_data_value = self.no_data_value[band]\n    src_array = self.read_array(band)\n    dtype = self.gdal_dtype[band]\n\n    # fill the new array with the nodata value\n    new_array = np.ones((self.rows, self.columns)) * no_data_value\n    # execute the function on each cell\n    # TODO: optimize executing a function over a whole array\n    for i in range(self.rows):\n        for j in range(self.columns):\n            if not np.isclose(src_array[i, j], no_data_value, rtol=0.001):\n                new_array[i, j] = func(src_array[i, j])\n\n    # create the output raster\n    dst = Dataset._create_dataset(self.columns, self.rows, 1, dtype, driver=\"MEM\")\n    # set the geotransform\n    dst.SetGeoTransform(self.geotransform)\n    # set the projection\n    dst.SetProjection(self.crs)\n    dst_obj = Dataset(dst)\n    dst_obj._set_no_data_value(no_data_value=no_data_value)\n    dst_obj.raster.GetRasterBand(band + 1).WriteArray(new_array)\n\n    return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.fill","title":"<code>fill(value, inplace=False, path=None)</code>","text":"<p>Fill the domain cells with a certain value.</p> <pre><code>Fill takes a raster and fills it with one value\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | int</code> <p>Numeric value to fill.</p> required <code>inplace</code> <code>bool</code> <p>If True, the original dataset will be modified. If False, a new dataset will be created. Default is False.</p> <code>False</code> <code>path</code> <code>str</code> <p>Path including the extension (.tif).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Union[Dataset, None]</code> <p>The resulting dataset if inplace is False; otherwise None.</p> <p>Examples:</p> <ul> <li>Create a Dataset with 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n[[1 1 3 1 2]\n [2 2 2 1 2]\n [2 2 3 1 3]\n [3 4 3 3 4]\n [4 4 2 1 1]]\n&gt;&gt;&gt; new_dataset = dataset.fill(10)\n&gt;&gt;&gt; print(new_dataset.read_array())\n[[10 10 10 10 10]\n [10 10 10 10 10]\n [10 10 10 10 10]\n [10 10 10 10 10]\n [10 10 10 10 10]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def fill(\n    self, value: Union[float, int], inplace: bool = False, path: str = None\n) -&gt; Union[\"Dataset\", None]:\n    \"\"\"Fill the domain cells with a certain value.\n\n        Fill takes a raster and fills it with one value\n\n    Args:\n        value (float | int):\n            Numeric value to fill.\n        inplace (bool):\n            If True, the original dataset will be modified. If False, a new dataset will be created. Default is False.\n        path (str):\n            Path including the extension (.tif).\n\n    Returns:\n        Dataset:\n            The resulting dataset if inplace is False; otherwise None.\n\n    Examples:\n        - Create a Dataset with 1 band, 5 rows, 5 columns, at the point lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n          [[1 1 3 1 2]\n           [2 2 2 1 2]\n           [2 2 3 1 3]\n           [3 4 3 3 4]\n           [4 4 2 1 1]]\n          &gt;&gt;&gt; new_dataset = dataset.fill(10)\n          &gt;&gt;&gt; print(new_dataset.read_array())\n          [[10 10 10 10 10]\n           [10 10 10 10 10]\n           [10 10 10 10 10]\n           [10 10 10 10 10]\n           [10 10 10 10 10]]\n\n          ```\n    \"\"\"\n    no_data_value = self.no_data_value[0]\n    src_array = self.raster.ReadAsArray()\n\n    if no_data_value is None:\n        no_data_value = np.nan\n\n    if not np.isnan(no_data_value):\n        src_array[~np.isclose(src_array, no_data_value, rtol=0.000001)] = value\n    else:\n        src_array[~np.isnan(src_array)] = value\n\n    dst = Dataset.dataset_like(self, src_array, path=path)\n    if inplace:\n        self.__init__(dst.raster)\n    else:\n        return dst\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.resample","title":"<code>resample(cell_size, method='nearest neighbor')</code>","text":"<p>resample.</p> <p>resample method reprojects a raster to any projection (default the WGS84 web mercator projection, without resampling). The function returns a GDAL in-memory file object.</p> <p>Parameters:</p> Name Type Description Default <code>cell_size</code> <code>int</code> <p>New cell size to resample the raster. If None, raster will not be resampled.</p> required <code>method</code> <code>str</code> <p>Resampling method: \"nearest neighbor\", \"cubic\", or \"bilinear\". Default is \"nearest neighbor\".</p> <code>'nearest neighbor'</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Dataset object.</p> <p>Examples:</p> <ul> <li>Create a Dataset with 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File: ...\n&lt;BLANKLINE&gt;\n&gt;&gt;&gt; dataset.plot(band=0)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> </p> <ul> <li>Resample the raster to a new cell size of 0.1:</li> </ul> <p><pre><code>&gt;&gt;&gt; new_dataset = dataset.resample(cell_size=0.1)\n&gt;&gt;&gt; print(new_dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.1\n            Dimension: 5 * 5\n            EPSG: 4326\n            Number of Bands: 4\n            Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n&gt;&gt;&gt; new_dataset.plot(band=0)\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> </p> <ul> <li>Resampling the dataset from cell_size 0.05 to 0.1 degrees reduced the number of cells to 5 in each dimension instead of 10.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def resample(\n    self, cell_size: Union[int, float], method: str = \"nearest neighbor\"\n) -&gt; \"Dataset\":\n    \"\"\"resample.\n\n    resample method reprojects a raster to any projection (default the WGS84 web mercator projection,\n    without resampling). The function returns a GDAL in-memory file object.\n\n    Args:\n        cell_size (int):\n            New cell size to resample the raster. If None, raster will not be resampled.\n        method (str):\n            Resampling method: \"nearest neighbor\", \"cubic\", or \"bilinear\". Default is \"nearest neighbor\".\n\n    Returns:\n        Dataset:\n            Dataset object.\n\n    Examples:\n        - Create a Dataset with 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File: ...\n          &lt;BLANKLINE&gt;\n          &gt;&gt;&gt; dataset.plot(band=0)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n          ![resample-source](./../_images/dataset/resample-source.png)\n\n        - Resample the raster to a new cell size of 0.1:\n\n          ```python\n          &gt;&gt;&gt; new_dataset = dataset.resample(cell_size=0.1)\n          &gt;&gt;&gt; print(new_dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.1\n                      Dimension: 5 * 5\n                      EPSG: 4326\n                      Number of Bands: 4\n                      Band names: ['Band_1', 'Band_2', 'Band_3', 'Band_4']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n          &gt;&gt;&gt; new_dataset.plot(band=0)\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n          ![resample-new](./../_images/dataset/resample-new.png)\n\n        - Resampling the dataset from cell_size 0.05 to 0.1 degrees reduced the number of cells to 5 in each dimension instead of 10.\n    \"\"\"\n    if not isinstance(method, str):\n        raise TypeError(\n            \"Please enter a correct method, for more information, see documentation\"\n        )\n    if method not in INTERPOLATION_METHODS.keys():\n        raise ValueError(\n            f\"The given interpolation method does not exist, existing methods are {INTERPOLATION_METHODS.keys()}\"\n        )\n\n    method = INTERPOLATION_METHODS.get(method)\n\n    sr_src = osr.SpatialReference(wkt=self.crs)\n\n    ulx = self.geotransform[0]\n    uly = self.geotransform[3]\n    # transform the right lower corner point\n    lrx = self.geotransform[0] + self.geotransform[1] * self.columns\n    lry = self.geotransform[3] + self.geotransform[5] * self.rows\n\n    # new geotransform\n    new_geo = (\n        self.geotransform[0],\n        cell_size,\n        self.geotransform[2],\n        self.geotransform[3],\n        self.geotransform[4],\n        -1 * cell_size,\n    )\n    # create a new raster\n    cols = int(np.round(abs(lrx - ulx) / cell_size))\n    rows = int(np.round(abs(uly - lry) / cell_size))\n    dtype = self.gdal_dtype[0]\n    bands = self.band_count\n\n    dst = Dataset._create_dataset(cols, rows, bands, dtype)\n    # set the geotransform\n    dst.SetGeoTransform(new_geo)\n    # set the projection\n    dst.SetProjection(sr_src.ExportToWkt())\n    dst_obj = Dataset(dst, \"write\")\n    # set the no data value\n    dst_obj._set_no_data_value(self.no_data_value)\n    # perform the projection &amp; resampling\n    gdal.ReprojectImage(\n        self.raster,\n        dst_obj.raster,\n        sr_src.ExportToWkt(),\n        sr_src.ExportToWkt(),\n        method,\n    )\n\n    return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.fill_gaps","title":"<code>fill_gaps(mask, src_array)</code>","text":"<p>Fill gaps in src_array using nearest neighbors where mask indicates valid cells.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Dataset | ndarray</code> <p>Mask dataset or array used to determine valid cells.</p> required <code>src_array</code> <code>ndarray</code> <p>Source array whose gaps will be filled.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The source array with gaps filled where applicable.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def fill_gaps(self, mask, src_array: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Fill gaps in src_array using nearest neighbors where mask indicates valid cells.\n\n    Args:\n        mask (Dataset | np.ndarray):\n            Mask dataset or array used to determine valid cells.\n        src_array (np.ndarray):\n            Source array whose gaps will be filled.\n\n    Returns:\n        np.ndarray: The source array with gaps filled where applicable.\n    \"\"\"\n    # align function only equate the no of rows and columns only\n    # match no_data_value inserts no_data_value in src raster to all places like mask\n    # still places that has no_data_value in the src raster, but it is not no_data_value in the mask\n    # and now has to be filled with values\n    # compare no of element that is not no_data_value in both rasters to make sure they are matched\n    # if both inputs are rasters\n    mask_array = mask.read_array()\n    row = mask.rows\n    col = mask.columns\n    mask_noval = mask.no_data_value[0]\n\n    if isinstance(mask, Dataset) and isinstance(self, Dataset):\n        # there might be cells that are out of domain in the src but not out of domain in the mask\n        # so change all the src_noval to mask_noval in the src_array\n        # src_array[np.isclose(src_array, self.no_data_value[0], rtol=0.001)] = mask_noval\n        # then count them (out of domain cells) in the src_array\n        elem_src = src_array.size - np.count_nonzero(\n            (src_array[np.isclose(src_array, self.no_data_value[0], rtol=0.001)])\n        )\n        # count the out of domain cells in the mask\n        elem_mask = mask_array.size - np.count_nonzero(\n            (mask_array[np.isclose(mask_array, mask_noval, rtol=0.001)])\n        )\n\n        # if not equal, then store indices of those cells that don't match\n        if elem_mask &gt; elem_src:\n            rows = [\n                i\n                for i in range(row)\n                for j in range(col)\n                if np.isclose(src_array[i, j], self.no_data_value[0], rtol=0.001)\n                and not np.isclose(mask_array[i, j], mask_noval, rtol=0.001)\n            ]\n            cols = [\n                j\n                for i in range(row)\n                for j in range(col)\n                if np.isclose(src_array[i, j], self.no_data_value[0], rtol=0.001)\n                and not np.isclose(mask_array[i, j], mask_noval, rtol=0.001)\n            ]\n        # interpolate those missing cells by the nearest neighbor\n        if elem_mask &gt; elem_src:\n            src_array = Dataset._nearest_neighbour(\n                src_array, self.no_data_value[0], rows, cols\n            )\n        return src_array\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.align","title":"<code>align(alignment_src)</code>","text":"<p>Align the current dataset (rows and columns) to match a given dataset.</p> Copies spatial properties from alignment_src to the current raster <ul> <li>The coordinate system</li> <li>The number of rows and columns</li> <li>Cell size</li> </ul> <p>Then resamples values from the current dataset using the nearest neighbor interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>alignment_src</code> <code>Dataset</code> <p>Spatial information source raster to get the spatial information (coordinate system, number of rows and columns). The data values of the current dataset are resampled to this alignment.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>The aligned dataset.</p> <p>Examples:</p> <ul> <li>The source dataset has a <code>top_left_corner</code> at (0, 0) with a 5*5 alignment, and a 0.05 degree cell size.</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(5, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 5 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <ul> <li>The dataset to be aligned has a top_left_corner at (-0.1, 0.1) (i.e., it has two more rows on top of the   dataset, and two columns on the left of the dataset).</li> </ul> <pre><code>&gt;&gt;&gt; arr = np.random.rand(10, 10)\n&gt;&gt;&gt; top_left_corner = (-0.1, 0.1)\n&gt;&gt;&gt; cell_size = 0.07\n&gt;&gt;&gt; dataset_target = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,\n... epsg=4326)\n&gt;&gt;&gt; print(dataset_target)\n&lt;BLANKLINE&gt;\n            Cell size: 0.07\n            Dimension: 10 * 10\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <p></p> <ul> <li>Now call the <code>align</code> method and use the dataset as the alignment source.</li> </ul> <pre><code>&gt;&gt;&gt; aligned_dataset = dataset_target.align(dataset)\n&gt;&gt;&gt; print(aligned_dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 5 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n</code></pre> <p></p> Source code in <code>pyramids/dataset.py</code> <pre><code>def align(\n    self,\n    alignment_src: \"Dataset\",\n) -&gt; \"Dataset\":\n    \"\"\"Align the current dataset (rows and columns) to match a given dataset.\n\n    Copies spatial properties from alignment_src to the current raster:\n        - The coordinate system\n        - The number of rows and columns\n        - Cell size\n    Then resamples values from the current dataset using the nearest neighbor interpolation.\n\n    Args:\n        alignment_src (Dataset):\n            Spatial information source raster to get the spatial information (coordinate system, number of rows and\n            columns). The data values of the current dataset are resampled to this alignment.\n\n    Returns:\n        Dataset: The aligned dataset.\n\n    Examples:\n        - The source dataset has a `top_left_corner` at (0, 0) with a 5*5 alignment, and a 0.05 degree cell size.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(5, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 5 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        - The dataset to be aligned has a top_left_corner at (-0.1, 0.1) (i.e., it has two more rows on top of the\n          dataset, and two columns on the left of the dataset).\n\n          ```python\n          &gt;&gt;&gt; arr = np.random.rand(10, 10)\n          &gt;&gt;&gt; top_left_corner = (-0.1, 0.1)\n          &gt;&gt;&gt; cell_size = 0.07\n          &gt;&gt;&gt; dataset_target = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size,\n          ... epsg=4326)\n          &gt;&gt;&gt; print(dataset_target)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.07\n                      Dimension: 10 * 10\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        ![align-source-target](./../_images/dataset/align-source-target.png)\n\n        - Now call the `align` method and use the dataset as the alignment source.\n\n          ```python\n          &gt;&gt;&gt; aligned_dataset = dataset_target.align(dataset)\n          &gt;&gt;&gt; print(aligned_dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 5 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          ```\n\n        ![align-result](./../_images/dataset/align-result.png)\n    \"\"\"\n    if isinstance(alignment_src, Dataset):\n        src = alignment_src\n    else:\n        raise TypeError(\n            \"First parameter should be a Dataset read using Dataset.openRaster or a path to the raster, \"\n            f\"given {type(alignment_src)}\"\n        )\n\n    # reproject the raster to match the projection of alignment_src\n    if not self.epsg == src.epsg:\n        reprojected_RasterB = self.to_crs(src.epsg)\n    else:\n        reprojected_RasterB = self\n    # create a new raster\n    dst = Dataset._create_dataset(\n        src.columns, src.rows, self.band_count, src.gdal_dtype[0], driver=\"MEM\"\n    )\n    # set the geotransform\n    dst.SetGeoTransform(src.geotransform)\n    # set the projection\n    dst.SetProjection(src.crs)\n    # set the no data value\n    dst_obj = Dataset(dst)\n    dst_obj._set_no_data_value(self.no_data_value)\n    # perform the projection &amp; resampling\n    method = gdal.GRA_NearestNeighbour\n    # resample the reprojected_RasterB\n    gdal.ReprojectImage(\n        reprojected_RasterB.raster,\n        dst_obj.raster,\n        src.crs,\n        src.crs,\n        method,\n    )\n\n    return dst_obj\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.correct_wrap_cutline_error","title":"<code>correct_wrap_cutline_error(src)</code>  <code>staticmethod</code>","text":"<p>Correct wrap cutline error.</p> <p>https://github.com/Serapieum-of-alex/pyramids/issues/74</p> Source code in <code>pyramids/dataset.py</code> <pre><code>@staticmethod\ndef correct_wrap_cutline_error(src: \"Dataset\"):\n    \"\"\"Correct wrap cutline error.\n\n    https://github.com/Serapieum-of-alex/pyramids/issues/74\n    \"\"\"\n    big_array = src.read_array()\n    value_to_remove = src.no_data_value[0]\n    \"\"\"Remove rows and columns that are all filled with a certain value from a 2D array.\"\"\"\n    # Find rows and columns to be removed\n    if big_array.ndim == 2:\n        rows_to_remove = np.all(big_array == value_to_remove, axis=1)\n        cols_to_remove = np.all(big_array == value_to_remove, axis=0)\n        # Use boolean indexing to remove rows and columns\n        small_array = big_array[~rows_to_remove][:, ~cols_to_remove]\n    elif big_array.ndim == 3:\n        rows_to_remove = np.all(big_array == value_to_remove, axis=(0, 2))\n        cols_to_remove = np.all(big_array == value_to_remove, axis=(0, 1))\n        # Use boolean indexing to remove rows and columns\n        # first remove the rows then the columns\n        small_array = big_array[:, ~rows_to_remove, :]\n        small_array = small_array[:, :, ~cols_to_remove]\n        n_rows = np.count_nonzero(~rows_to_remove)\n        n_cols = np.count_nonzero(~cols_to_remove)\n        small_array = small_array.reshape((src.band_count, n_rows, n_cols))\n    else:\n        raise ValueError(\"Array must be 2D or 3D\")\n\n    x_ind = np.where(~rows_to_remove)[0][0]\n    y_ind = np.where(~cols_to_remove)[0][0]\n    new_x = src.x[y_ind] - src.cell_size / 2\n    new_y = src.y[x_ind] + src.cell_size / 2\n    new_gt = (new_x, src.cell_size, 0, new_y, 0, -src.cell_size)\n    new_src = src.create_from_array(\n        small_array, geo=new_gt, epsg=src.epsg, no_data_value=src.no_data_value\n    )\n    return new_src\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.crop","title":"<code>crop(mask, touch=True, inplace=False)</code>","text":"<p>Crop dataset using dataset/feature collection.</p> <pre><code>Crop/Clip the Dataset object using a polygon/raster.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>GeoDataFrame | Dataset</code> <p>GeoDataFrame with a polygon geometry, or a Dataset object.</p> required <code>touch</code> <code>bool</code> <p>Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask. Default is True.</p> <code>True</code> <code>inplace</code> <code>bool</code> <p>If True, apply changes in place. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Dataset, None]</code> <p>Dataset | None: The cropped raster. If inplace is True, the method will change the raster in place and return None.</p> Hint <ul> <li>If the mask is a dataset with multi-bands, the <code>crop</code> method will use the first band as the mask.</li> </ul> <p>Examples:</p> <ul> <li> <p>Crop the raster using a polygon mask.</p> </li> <li> <p>The polygon covers 4 cells in the 3rd and 4th rows and 3rd and 4th column <code>arr[2:4, 2:4]</code>, so the result     dataset will have the same number of bands <code>4</code>, 2 rows and 2 columns.</p> </li> <li>First, create the dataset to have 4 bands, 10 rows and 10 columns; the dataset has a cell size of 0.05     degree, the top left corner of the dataset is (0, 0).</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Polygon\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; dataset = Dataset.create_from_array(\n...         arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326\n... )\n</code></pre> - Second, create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2 -0.1]     to cover the 4 cells.</p> <pre><code>```python\n&gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n\n```\n</code></pre> <ul> <li>Pass the <code>geodataframe</code> to the crop method using the <code>mask</code> parameter.</li> </ul> <p><pre><code>&gt;&gt;&gt; cropped_dataset = dataset.crop(mask=mask)\n</code></pre> - Check the cropped dataset:</p> <p><pre><code>&gt;&gt;&gt; print(cropped_dataset.shape)\n(4, 2, 2)\n&gt;&gt;&gt; print(cropped_dataset.geotransform)\n(0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n&gt;&gt;&gt; print(cropped_dataset.read_array(band=0))# doctest: +SKIP\n[[0.00921161 0.90841171]\n [0.355636   0.18650262]]\n&gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n[[0.00921161 0.90841171]\n [0.355636   0.18650262]]\n</code></pre> - Crop a raster using another raster mask:</p> <ul> <li>Create a mask dataset with the same extent of the polygon we used in the previous example.</li> </ul> <p><pre><code>&gt;&gt;&gt; geotransform = (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n&gt;&gt;&gt; mask_dataset = Dataset.create_from_array(np.random.rand(2, 2), geo=geotransform, epsg=4326)\n</code></pre> - Then use the mask dataset to crop the dataset.</p> <p><pre><code>&gt;&gt;&gt; cropped_dataset_2 = dataset.crop(mask=mask_dataset)\n&gt;&gt;&gt; print(cropped_dataset_2.shape)\n(4, 2, 2)\n</code></pre> - Check the cropped dataset:</p> <pre><code>&gt;&gt;&gt; print(cropped_dataset_2.geotransform)\n(0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n&gt;&gt;&gt; print(cropped_dataset_2.read_array(band=0))# doctest: +SKIP\n[[0.00921161 0.90841171]\n [0.355636   0.18650262]]\n&gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n [[0.00921161 0.90841171]\n [0.355636   0.18650262]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def crop(\n    self,\n    mask: Union[GeoDataFrame, FeatureCollection],\n    touch: bool = True,\n    inplace: bool = False,\n) -&gt; Union[\"Dataset\", None]:\n    \"\"\"Crop dataset using dataset/feature collection.\n\n        Crop/Clip the Dataset object using a polygon/raster.\n\n    Args:\n        mask (GeoDataFrame | Dataset):\n            GeoDataFrame with a polygon geometry, or a Dataset object.\n        touch (bool):\n            Include the cells that touch the polygon, not only those that lie entirely inside the polygon mask.\n            Default is True.\n        inplace (bool):\n            If True, apply changes in place. Default is False.\n\n    Returns:\n        Dataset | None:\n            The cropped raster. If inplace is True, the method will change the raster in place and return None.\n\n    Hint:\n        - If the mask is a dataset with multi-bands, the `crop` method will use the first band as the mask.\n\n    Examples:\n        - Crop the raster using a polygon mask.\n\n          - The polygon covers 4 cells in the 3rd and 4th rows and 3rd and 4th column `arr[2:4, 2:4]`, so the result\n            dataset will have the same number of bands `4`, 2 rows and 2 columns.\n          - First, create the dataset to have 4 bands, 10 rows and 10 columns; the dataset has a cell size of 0.05\n            degree, the top left corner of the dataset is (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; import geopandas as gpd\n          &gt;&gt;&gt; from shapely.geometry import Polygon\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(\n          ...         arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326\n          ... )\n\n          ```\n        - Second, create the polygon using shapely polygon, and use the xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2 -0.1]\n            to cover the 4 cells.\n\n            ```python\n            &gt;&gt;&gt; mask = gpd.GeoDataFrame(geometry=[Polygon([(0.1, -0.1), (0.1, -0.2), (0.2, -0.2), (0.2, -0.1)])], crs=4326)\n\n            ```\n        - Pass the `geodataframe` to the crop method using the `mask` parameter.\n\n          ```python\n          &gt;&gt;&gt; cropped_dataset = dataset.crop(mask=mask)\n\n          ```\n        - Check the cropped dataset:\n\n          ```python\n          &gt;&gt;&gt; print(cropped_dataset.shape)\n          (4, 2, 2)\n          &gt;&gt;&gt; print(cropped_dataset.geotransform)\n          (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n          &gt;&gt;&gt; print(cropped_dataset.read_array(band=0))# doctest: +SKIP\n          [[0.00921161 0.90841171]\n           [0.355636   0.18650262]]\n          &gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n          [[0.00921161 0.90841171]\n           [0.355636   0.18650262]]\n\n          ```\n        - Crop a raster using another raster mask:\n\n          - Create a mask dataset with the same extent of the polygon we used in the previous example.\n\n          ```python\n          &gt;&gt;&gt; geotransform = (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n          &gt;&gt;&gt; mask_dataset = Dataset.create_from_array(np.random.rand(2, 2), geo=geotransform, epsg=4326)\n\n          ```\n        - Then use the mask dataset to crop the dataset.\n\n          ```python\n          &gt;&gt;&gt; cropped_dataset_2 = dataset.crop(mask=mask_dataset)\n          &gt;&gt;&gt; print(cropped_dataset_2.shape)\n          (4, 2, 2)\n\n          ```\n        - Check the cropped dataset:\n\n          ```python\n          &gt;&gt;&gt; print(cropped_dataset_2.geotransform)\n          (0.1, 0.05, 0.0, -0.1, 0.0, -0.05)\n          &gt;&gt;&gt; print(cropped_dataset_2.read_array(band=0))# doctest: +SKIP\n          [[0.00921161 0.90841171]\n           [0.355636   0.18650262]]\n          &gt;&gt;&gt; print(arr[0, 2:4, 2:4])# doctest: +SKIP\n           [[0.00921161 0.90841171]\n           [0.355636   0.18650262]]\n\n          ```\n\n    \"\"\"\n    if isinstance(mask, GeoDataFrame):\n        dst = self._crop_with_polygon_warp(mask, touch=touch)\n    elif isinstance(mask, Dataset):\n        dst = self._crop_with_raster(mask)\n    else:\n        raise TypeError(\n            \"The second parameter: mask could be either GeoDataFrame or Dataset object\"\n        )\n\n    if inplace:\n        self.__init__(dst.raster)\n    else:\n        return dst\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.map_to_array_coordinates","title":"<code>map_to_array_coordinates(points)</code>","text":"<p>Convert coordinates of points to array indices.</p> <ul> <li>map_to_array_coordinates locates a point with real coordinates (x, y) or (lon, lat) on the array by finding     the cell indices (row, column) of the nearest cell in the raster.</li> <li>The point coordinate system of the raster has to be projected to be able to calculate the distance.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>GeoDataFrame | DataFrame | FeatureCollection</code> <ul> <li>GeoDataFrame: GeoDataFrame with POINT geometry.</li> <li>DataFrame: DataFrame with x, y columns.</li> </ul> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array with shape (N, 2) containing the row and column indices in the array.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 2 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> - DataFrame with x, y columns:</p> <ul> <li>We can give the function a DataFrame with x, y columns to array the coordinates of the points that are located within the dataset domain.</li> </ul> <p><pre><code>&gt;&gt;&gt; points = pd.DataFrame({\"x\": [0.025, 0.175, 0.375], \"y\": [0.025, 0.225, 0.125]})\n&gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n&gt;&gt;&gt; print(indices)\n[[0 0]\n [0 3]\n [0 7]]\n</code></pre> - GeoDataFrame with POINT geometry:</p> <ul> <li>We can give the function a GeoDataFrame with POINT geometry to array the coordinates of the points that locate within the dataset domain.</li> </ul> <pre><code>&gt;&gt;&gt; from shapely.geometry import Point\n&gt;&gt;&gt; from geopandas import GeoDataFrame\n&gt;&gt;&gt; points = GeoDataFrame({\"geometry\": [Point(0.025, 0.025), Point(0.175, 0.225), Point(0.375, 0.125)]})\n&gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n&gt;&gt;&gt; print(indices)\n[[0 0]\n [0 3]\n [0 7]]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def map_to_array_coordinates(\n    self,\n    points: Union[GeoDataFrame, FeatureCollection, DataFrame],\n) -&gt; np.ndarray:\n    \"\"\"Convert coordinates of points to array indices.\n\n    - map_to_array_coordinates locates a point with real coordinates (x, y) or (lon, lat) on the array by finding\n        the cell indices (row, column) of the nearest cell in the raster.\n    - The point coordinate system of the raster has to be projected to be able to calculate the distance.\n\n    Args:\n        points (GeoDataFrame | pandas.DataFrame | FeatureCollection):\n            - GeoDataFrame: GeoDataFrame with POINT geometry.\n            - DataFrame: DataFrame with x, y columns.\n\n    Returns:\n        np.ndarray:\n            Array with shape (N, 2) containing the row and column indices in the array.\n\n    Examples:\n        - Create `Dataset` consisting of 2 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; import pandas as pd\n          &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(2, 10, 10))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n        - DataFrame with x, y columns:\n\n          - We can give the function a DataFrame with x, y columns to array the coordinates of the points that are located within the dataset domain.\n\n          ```python\n          &gt;&gt;&gt; points = pd.DataFrame({\"x\": [0.025, 0.175, 0.375], \"y\": [0.025, 0.225, 0.125]})\n          &gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n          &gt;&gt;&gt; print(indices)\n          [[0 0]\n           [0 3]\n           [0 7]]\n\n          ```\n        - GeoDataFrame with POINT geometry:\n\n          - We can give the function a GeoDataFrame with POINT geometry to array the coordinates of the points that locate within the dataset domain.\n\n          ```python\n          &gt;&gt;&gt; from shapely.geometry import Point\n          &gt;&gt;&gt; from geopandas import GeoDataFrame\n          &gt;&gt;&gt; points = GeoDataFrame({\"geometry\": [Point(0.025, 0.025), Point(0.175, 0.225), Point(0.375, 0.125)]})\n          &gt;&gt;&gt; indices = dataset.map_to_array_coordinates(points)\n          &gt;&gt;&gt; print(indices)\n          [[0 0]\n           [0 3]\n           [0 7]]\n\n          ```\n    \"\"\"\n    if isinstance(points, GeoDataFrame):\n        points = FeatureCollection(points)\n    elif isinstance(points, DataFrame):\n        if all(elem not in points.columns for elem in [\"x\", \"y\"]):\n            raise ValueError(\n                \"If the input is a DataFrame, it should have two columns x, and y\"\n            )\n    else:\n        if not isinstance(points, FeatureCollection):\n            raise TypeError(\n                \"please check points input it should be GeoDataFrame/DataFrame/FeatureCollection - given\"\n                f\" {type(points)}\"\n            )\n    if not isinstance(points, DataFrame):\n        # get the x, y coordinates.\n        points.xy()\n        points = points.feature.loc[:, [\"x\", \"y\"]].values\n    else:\n        points = points.loc[:, [\"x\", \"y\"]].values\n\n    # since the first row is x-coords so the first column in the indices is the column index\n    indices = locate_values(points, self.x, self.y)\n    # rearrange the columns to make the row index first\n    indices = indices[:, [1, 0]]\n    return indices\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.array_to_map_coordinates","title":"<code>array_to_map_coordinates(rows_index, column_index, center=False)</code>","text":"<p>Convert array indices to map coordinates.</p> <p>array_to_map_coordinates converts the array indices (rows, cols) to real coordinates (x, y) or (lon, lat).</p> <p>Parameters:</p> Name Type Description Default <code>rows_index</code> <code>List[Number] | ndarray</code> <p>The row indices of the cells in the raster array.</p> required <code>column_index</code> <code>List[Number] | ndarray</code> <p>The column indices of the cells in the raster array.</p> required <code>center</code> <code>bool</code> <p>If True, the coordinates will be the center of the cell. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[Number], List[Number]]</code> <p>Tuple[List[Number], List[Number]]: A tuple of two lists: the x coordinates and the y coordinates of the cells.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 1 band, 10 rows, 10 columns, at the point lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Now call the function with two lists of row and column indices:</li> </ul> <pre><code>&gt;&gt;&gt; rows_index = [1, 3, 5]\n&gt;&gt;&gt; column_index = [2, 4, 6]\n&gt;&gt;&gt; coords = dataset.array_to_map_coordinates(rows_index, column_index)\n&gt;&gt;&gt; print(coords) # doctest: +SKIP\n([0.1, 0.2, 0.3], [-0.05, -0.15, -0.25])\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def array_to_map_coordinates(\n    self,\n    rows_index: Union[List[Number], np.ndarray],\n    column_index: Union[List[Number], np.ndarray],\n    center: bool = False,\n) -&gt; Tuple[List[Number], List[Number]]:\n    \"\"\"Convert array indices to map coordinates.\n\n    array_to_map_coordinates converts the array indices (rows, cols) to real coordinates (x, y) or (lon, lat).\n\n    Args:\n        rows_index (List[Number] | np.ndarray):\n            The row indices of the cells in the raster array.\n        column_index (List[Number] | np.ndarray):\n            The column indices of the cells in the raster array.\n        center (bool):\n            If True, the coordinates will be the center of the cell. Default is False.\n\n    Returns:\n        Tuple[List[Number], List[Number]]:\n            A tuple of two lists: the x coordinates and the y coordinates of the cells.\n\n    Examples:\n        - Create `Dataset` consisting of 1 band, 10 rows, 10 columns, at the point lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; import pandas as pd\n          &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(10, 10))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Now call the function with two lists of row and column indices:\n\n          ```python\n          &gt;&gt;&gt; rows_index = [1, 3, 5]\n          &gt;&gt;&gt; column_index = [2, 4, 6]\n          &gt;&gt;&gt; coords = dataset.array_to_map_coordinates(rows_index, column_index)\n          &gt;&gt;&gt; print(coords) # doctest: +SKIP\n          ([0.1, 0.2, 0.3], [-0.05, -0.15, -0.25])\n\n          ```\n    \"\"\"\n    top_left_x, top_left_y = self.top_left_corner\n    cell_size = self.cell_size\n    if center:\n        # for the top left corner of the cell\n        top_left_x += cell_size / 2\n        top_left_y -= cell_size / 2\n\n    x_coord_fn = lambda x: top_left_x + x * cell_size\n    y_coord_fn = lambda y: top_left_y - y * cell_size\n\n    x_coords = list(map(x_coord_fn, column_index))\n    y_coords = list(map(y_coord_fn, rows_index))\n\n    return x_coords, y_coords\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.extract","title":"<code>extract(band=None, exclude_value=None, feature=None)</code>","text":"<p>Extract.</p> <ul> <li>Extract method gets all the values in a raster, and excludes the values in the exclude_value parameter.</li> <li>If the feature parameter is given, the raster will be clipped to the extent of the given feature and the   values within the feature are extracted.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. Default is None.</p> <code>None</code> <code>exclude_value</code> <code>Numeric</code> <p>Values to exclude from extracted values. If the dataset is multi-band, the values in <code>exclude_value</code> will be filtered out from the first band only.</p> <code>None</code> <code>feature</code> <code>FeatureCollection | GeoDataFrame</code> <p>Vector data containing point geometries at which to extract the values. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The extracted values from each band in the dataset will be in one row in the returned array.</p> <p>Examples:</p> <ul> <li> <p>Extract all values from the dataset:</p> </li> <li> <p>First, create a dataset with 2 bands, 4 rows and 4 columns:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1, 5, size=(2, 4, 4))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 4 * 4\n            EPSG: 4326\n            Number of Bands: 2\n            Band names: ['Band_1', 'Band_2']\n            Mask: -9999.0\n            Data type: int32\n            File:...\n&lt;BLANKLINE&gt;\n&gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n[[[1 3 3 4]\n  [1 4 2 4]\n  [2 4 2 1]\n  [1 3 2 3]]\n [[3 2 1 3]\n  [4 3 2 2]\n  [2 2 3 4]\n  [1 4 1 4]]]\n</code></pre> </li> <li> <p>Now, extract the values in the dataset:</p> <pre><code>&gt;&gt;&gt; values = dataset.extract()\n&gt;&gt;&gt; print(values) # doctest: +SKIP\n[[1 3 3 4 1 4 2 4 2 4 2 1 1 3 2 3]\n [3 2 1 3 4 3 2 2 2 2 3 4 1 4 1 4]]\n</code></pre> </li> <li> <p>Extract all the values except 2:</p> <pre><code>&gt;&gt;&gt; values = dataset.extract(exclude_value=2)\n&gt;&gt;&gt; print(values) # doctest: +SKIP\n</code></pre> </li> <li> <p>Extract values at the location of the given point geometries:</p> </li> </ul> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; from shapely.geometry import Point\n</code></pre> <ul> <li> <p>Create the points using shapely and GeoPandas to cover the 4 cells with xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2, -0.1]:</p> <pre><code>&gt;&gt;&gt; points = gpd.GeoDataFrame(geometry=[Point(0.1, -0.1), Point(0.1, -0.2), Point(0.2, -0.2), Point(0.2, -0.1)],crs=4326)\n&gt;&gt;&gt; values = dataset.extract(feature=points)\n&gt;&gt;&gt; print(values) # doctest: +SKIP\n[[4 3 3 4]\n [3 4 4 2]]\n</code></pre> </li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def extract(\n    self,\n    band: int = None,\n    exclude_value: Any = None,\n    feature: Union[FeatureCollection, GeoDataFrame] = None,\n) -&gt; np.ndarray:\n    \"\"\"Extract.\n\n    - Extract method gets all the values in a raster, and excludes the values in the exclude_value parameter.\n    - If the feature parameter is given, the raster will be clipped to the extent of the given feature and the\n      values within the feature are extracted.\n\n    Args:\n        band (int, optional):\n            Band index. Default is None.\n        exclude_value (Numeric, optional):\n            Values to exclude from extracted values. If the dataset is multi-band, the values in `exclude_value`\n            will be filtered out from the first band only.\n        feature (FeatureCollection | GeoDataFrame, optional):\n            Vector data containing point geometries at which to extract the values. Default is None.\n\n    Returns:\n        np.ndarray:\n            The extracted values from each band in the dataset will be in one row in the returned array.\n\n    Examples:\n        - Extract all values from the dataset:\n\n          - First, create a dataset with 2 bands, 4 rows and 4 columns:\n\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(2, 4, 4))\n            &gt;&gt;&gt; top_left_corner = (0, 0)\n            &gt;&gt;&gt; cell_size = 0.05\n            &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n            &gt;&gt;&gt; print(dataset)\n            &lt;BLANKLINE&gt;\n                        Cell size: 0.05\n                        Dimension: 4 * 4\n                        EPSG: 4326\n                        Number of Bands: 2\n                        Band names: ['Band_1', 'Band_2']\n                        Mask: -9999.0\n                        Data type: int32\n                        File:...\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; print(dataset.read_array()) # doctest: +SKIP\n            [[[1 3 3 4]\n              [1 4 2 4]\n              [2 4 2 1]\n              [1 3 2 3]]\n             [[3 2 1 3]\n              [4 3 2 2]\n              [2 2 3 4]\n              [1 4 1 4]]]\n\n            ```\n\n          - Now, extract the values in the dataset:\n\n            ```python\n            &gt;&gt;&gt; values = dataset.extract()\n            &gt;&gt;&gt; print(values) # doctest: +SKIP\n            [[1 3 3 4 1 4 2 4 2 4 2 1 1 3 2 3]\n             [3 2 1 3 4 3 2 2 2 2 3 4 1 4 1 4]]\n\n            ```\n\n          - Extract all the values except 2:\n\n            ```python\n            &gt;&gt;&gt; values = dataset.extract(exclude_value=2)\n            &gt;&gt;&gt; print(values) # doctest: +SKIP\n\n            ```\n\n        - Extract values at the location of the given point geometries:\n\n          ```python\n          &gt;&gt;&gt; import geopandas as gpd\n          &gt;&gt;&gt; from shapely.geometry import Point\n          ```\n\n          - Create the points using shapely and GeoPandas to cover the 4 cells with xmin, ymin, xmax, ymax = [0.1, -0.2, 0.2, -0.1]:\n\n            ```python\n            &gt;&gt;&gt; points = gpd.GeoDataFrame(geometry=[Point(0.1, -0.1), Point(0.1, -0.2), Point(0.2, -0.2), Point(0.2, -0.1)],crs=4326)\n            &gt;&gt;&gt; values = dataset.extract(feature=points)\n            &gt;&gt;&gt; print(values) # doctest: +SKIP\n            [[4 3 3 4]\n             [3 4 4 2]]\n\n            ```\n    \"\"\"\n    # Optimize: make the read_array return only the array for inside the mask feature, and not to read the whole\n    #  raster\n    arr = self.read_array(band=band)\n    no_data_value = (\n        self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n    )\n    if feature is None:\n        mask = (\n            [no_data_value, exclude_value]\n            if exclude_value is not None\n            else [no_data_value]\n        )\n        values = get_pixels2(arr, mask)\n    else:\n        indices = self.map_to_array_coordinates(feature)\n        if arr.ndim &gt; 2:\n            values = arr[:, indices[:, 0], indices[:, 1]]\n        else:\n            values = arr[indices[:, 0], indices[:, 1]]\n\n    return values\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.overlay","title":"<code>overlay(classes_map, band=0, exclude_value=None)</code>","text":"<p>Overlay.</p> <p>Overlay method extracts all the values in the dataset for each class in the given class map.</p> <p>Parameters:</p> Name Type Description Default <code>classes_map</code> <code>Dataset</code> <p>Dataset object for the raster that has classes you want to overlay with the raster.</p> required <code>band</code> <code>int</code> <p>If the raster is multi-band, choose the band you want to overlay with the classes map. Default is 0.</p> <code>0</code> <code>exclude_value</code> <code>Numeric</code> <p>Values you want to exclude from extracted values. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[List[float], List[float]]</code> <p>Dictionary with class values as keys (from the class map), and for each key a list of all the intersected values in the base map.</p> <p>Examples:</p> <ul> <li>Read the dataset:</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/raster-folder/MSWEP_1979.01.01.tif\")\n&gt;&gt;&gt; dataset.plot(figsize=(6, 8)) # doctest: +SKIP\n</code></pre> <p></p> <ul> <li>Read the classes dataset:</li> </ul> <pre><code>&gt;&gt;&gt; classes = Dataset.read_file(\"examples/data/geotiff/rhine-classes.tif\")\n&gt;&gt;&gt; classes.plot(figsize=(6, 8), color_scale=4, bounds=[1,2,3,4,5,6]) # doctest: +SKIP\n</code></pre> <p></p> <ul> <li>Overlay the dataset with the classes dataset:</li> </ul> <pre><code>&gt;&gt;&gt; classes_dict = dataset.overlay(classes)\n&gt;&gt;&gt; print(classes_dict.keys()) # doctest: +SKIP\ndict_keys([1, 2, 3, 4, 5])\n</code></pre> <ul> <li>You can use the key <code>1</code> to get the values that overlay class 1.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def overlay(\n    self,\n    classes_map,\n    band: int = 0,\n    exclude_value: Union[float, int] = None,\n) -&gt; Dict[List[float], List[float]]:\n    \"\"\"Overlay.\n\n    Overlay method extracts all the values in the dataset for each class in the given class map.\n\n    Args:\n        classes_map (Dataset):\n            Dataset object for the raster that has classes you want to overlay with the raster.\n        band (int):\n            If the raster is multi-band, choose the band you want to overlay with the classes map. Default is 0.\n        exclude_value (Numeric, optional):\n            Values you want to exclude from extracted values. Default is None.\n\n    Returns:\n        Dict:\n            Dictionary with class values as keys (from the class map), and for each key a list of all the intersected\n            values in the base map.\n\n    Examples:\n        - Read the dataset:\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/raster-folder/MSWEP_1979.01.01.tif\")\n          &gt;&gt;&gt; dataset.plot(figsize=(6, 8)) # doctest: +SKIP\n\n          ```\n\n          ![rhine-rainfall](./../_images/dataset/rhine-rainfall.png)\n\n        - Read the classes dataset:\n\n          ```python\n          &gt;&gt;&gt; classes = Dataset.read_file(\"examples/data/geotiff/rhine-classes.tif\")\n          &gt;&gt;&gt; classes.plot(figsize=(6, 8), color_scale=4, bounds=[1,2,3,4,5,6]) # doctest: +SKIP\n\n          ```\n\n          ![rhine-classes](./../_images/dataset/rhine-classes.png)\n\n        - Overlay the dataset with the classes dataset:\n\n          ```python\n          &gt;&gt;&gt; classes_dict = dataset.overlay(classes)\n          &gt;&gt;&gt; print(classes_dict.keys()) # doctest: +SKIP\n          dict_keys([1, 2, 3, 4, 5])\n\n          ```\n\n        - You can use the key `1` to get the values that overlay class 1.\n    \"\"\"\n    if not self._check_alignment(classes_map):\n        raise AlignmentError(\n            \"The class Dataset is not aligned with the current raster, please use the method \"\n            \"'align' to align both rasters.\"\n        )\n    arr = self.read_array(band=band)\n    no_data_value = (\n        self.no_data_value[0] if self.no_data_value[0] is not None else np.nan\n    )\n    mask = (\n        [no_data_value, exclude_value]\n        if exclude_value is not None\n        else [no_data_value]\n    )\n    ind = get_indices2(arr, mask)\n    classes = classes_map.read_array()\n    values = dict()\n\n    # extract values\n    for i, ind_i in enumerate(ind):\n        # first check if the sub-basin has a list in the dict if not create a list\n        key = classes[ind_i[0], ind_i[1]]\n        if key not in list(values.keys()):\n            values[key] = list()\n\n        values[key].append(arr[ind_i[0], ind_i[1]])\n\n    return values\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_mask","title":"<code>get_mask(band=0)</code>","text":"<p>Get the mask array.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of the mask. 0 value for cells out of the domain, and 255 for cells in the domain.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_mask(self, band: int = 0) -&gt; np.ndarray:\n    \"\"\"Get the mask array.\n\n    Args:\n        band (int):\n            Band index. Default is 0.\n\n    Returns:\n        np.ndarray:\n            Array of the mask. 0 value for cells out of the domain, and 255 for cells in the domain.\n    \"\"\"\n    # TODO: there is a CreateMaskBand method in the gdal.Dataset class, it creates a mask band for the dataset\n    #   either internally or externally.\n    arr = self._iloc(band).GetMaskBand().ReadAsArray()\n    return arr\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.footprint","title":"<code>footprint(band=0, exclude_values=None)</code>","text":"<p>Extract the real coverage of the values in a certain band.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. Default is 0.</p> <code>0</code> <code>exclude_values</code> <code>Optional[List[Any]]</code> <p>If you want to exclude a certain value in the raster with another value inter the two values as a list of tuples a [(value_to_be_exclude_valuesd, new_value)].</p> <ul> <li>Example of exclude_values usage:</li> </ul> <pre><code>&gt;&gt;&gt; exclude_values = [0]\n</code></pre> <ul> <li>This parameter is introduced particularly in the case of rasters that has the no_data_value stored in   the <code>no_data_value</code> property does not match the value stored in the band, so this option can correct   this behavior.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <code>Union[GeoDataFrame, None]</code> <ul> <li>geodataframe containing the polygon representing the extent of the raster. the extent column should   contain a value of 2 only.</li> <li>if the dataset had separate polygons, each polygon will be in a separate row.</li> </ul> <p>Examples:</p> <ul> <li>The following raster dataset has flood depth stored in its values, and the non-flooded cells are filled with   zero, so to extract the flood extent, we need to exclude the zero flood depth cells.</li> </ul> <pre><code>&gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/rhine-flood.tif\")\n&gt;&gt;&gt; dataset.plot()\n(&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n</code></pre> <p></p> <ul> <li>Now, to extract the footprint of the dataset band, we need to specify the <code>exclude_values</code> parameter with the   value of the non-flooded cells.</li> </ul> <pre><code>&gt;&gt;&gt; extent = dataset.footprint(band=0, exclude_values=[0])\n&gt;&gt;&gt; print(extent)\n   Band_1                                           geometry\n0     2.0  POLYGON ((4070974.182 3181069.473, 4070974.182...\n1     2.0  POLYGON ((4077674.182 3181169.473, 4077674.182...\n2     2.0  POLYGON ((4091174.182 3169169.473, 4091174.182...\n3     2.0  POLYGON ((4088574.182 3176269.473, 4088574.182...\n4     2.0  POLYGON ((4082974.182 3167869.473, 4082974.182...\n5     2.0  POLYGON ((4092274.182 3168269.473, 4092274.182...\n6     2.0  POLYGON ((4072474.182 3181169.473, 4072474.182...\n\n&gt;&gt;&gt; extent.plot()\n&lt;Axes: &gt;\n</code></pre> <p></p> Source code in <code>pyramids/dataset.py</code> <pre><code>def footprint(\n    self,\n    band: int = 0,\n    exclude_values: Optional[List[Any]] = None,\n) -&gt; Union[GeoDataFrame, None]:\n    \"\"\"Extract the real coverage of the values in a certain band.\n\n    Args:\n        band (int):\n            Band index. Default is 0.\n        exclude_values (Optional[List[Any]]):\n            If you want to exclude a certain value in the raster with another value inter the two values as a\n            list of tuples a [(value_to_be_exclude_valuesd, new_value)].\n\n            - Example of exclude_values usage:\n\n              ```python\n              &gt;&gt;&gt; exclude_values = [0]\n\n              ```\n\n            - This parameter is introduced particularly in the case of rasters that has the no_data_value stored in\n              the `no_data_value` property does not match the value stored in the band, so this option can correct\n              this behavior.\n\n    Returns:\n        GeoDataFrame: \n            - geodataframe containing the polygon representing the extent of the raster. the extent column should\n              contain a value of 2 only.\n            - if the dataset had separate polygons, each polygon will be in a separate row.\n\n    Examples:\n        - The following raster dataset has flood depth stored in its values, and the non-flooded cells are filled with\n          zero, so to extract the flood extent, we need to exclude the zero flood depth cells.\n\n          ```python\n          &gt;&gt;&gt; dataset = Dataset.read_file(\"examples/data/geotiff/rhine-flood.tif\")\n          &gt;&gt;&gt; dataset.plot()\n          (&lt;Figure size 800x800 with 2 Axes&gt;, &lt;Axes: &gt;)\n\n          ```\n\n        ![dataset-footprint-rhine-flood](./../_images/dataset/dataset-footprint-rhine-flood.png)\n\n        - Now, to extract the footprint of the dataset band, we need to specify the `exclude_values` parameter with the\n          value of the non-flooded cells.\n\n          ```python\n          &gt;&gt;&gt; extent = dataset.footprint(band=0, exclude_values=[0])\n          &gt;&gt;&gt; print(extent)\n             Band_1                                           geometry\n          0     2.0  POLYGON ((4070974.182 3181069.473, 4070974.182...\n          1     2.0  POLYGON ((4077674.182 3181169.473, 4077674.182...\n          2     2.0  POLYGON ((4091174.182 3169169.473, 4091174.182...\n          3     2.0  POLYGON ((4088574.182 3176269.473, 4088574.182...\n          4     2.0  POLYGON ((4082974.182 3167869.473, 4082974.182...\n          5     2.0  POLYGON ((4092274.182 3168269.473, 4092274.182...\n          6     2.0  POLYGON ((4072474.182 3181169.473, 4072474.182...\n\n          &gt;&gt;&gt; extent.plot()\n          &lt;Axes: &gt;\n\n          ```\n\n        ![dataset-footprint-rhine-flood-extent](./../_images/dataset/dataset-footprint-rhine-flood-extent.png)\n\n    \"\"\"\n    arr = self.read_array(band=band)\n    no_data_val = self.no_data_value[band]\n\n    if no_data_val is None:\n        if not (np.isnan(arr)).any():\n            logger.warning(\n                \"The nodata value stored in the raster does not exist in the raster \"\n                \"so either the raster extent is all full of data, or the no_data_value stored in the raster is\"\n                \" not correct\"\n            )\n    else:\n        if not (np.isclose(arr, no_data_val, rtol=0.00001)).any():\n            logger.warning(\n                \"the nodata value stored in the raster does not exist in the raster \"\n                \"so either the raster extent is all full of data, or the no_data_value stored in the raster is\"\n                \" not correct\"\n            )\n    # if you want to exclude_values any value in the raster\n    if exclude_values:\n        for val in exclude_values:\n            try:\n                # in case the val2 is None, and the array is int type, the following line will give error as None\n                # is considered as float\n                arr[np.isclose(arr, val)] = no_data_val\n            except TypeError:\n                arr = arr.astype(np.float32)\n                arr[np.isclose(arr, val)] = no_data_val\n\n    # replace all the values with 2\n    if no_data_val is None:\n        # check if the whole raster is full of no_data_value\n        if (np.isnan(arr)).all():\n            logger.warning(\"the raster is full of no_data_value\")\n            return None\n\n        arr[~np.isnan(arr)] = 2\n    else:\n        # check if the whole raster is full of no_data_value\n        if (np.isclose(arr, no_data_val, rtol=0.00001)).all():\n            logger.warning(\"the raster is full of no_data_value\")\n            return None\n\n        arr[~np.isclose(arr, no_data_val, rtol=0.00001)] = 2\n    new_dataset = self.create_from_array(\n        arr, geo=self.geotransform, epsg=self.epsg, no_data_value=self.no_data_value\n    )\n    # then convert the raster into polygon\n    gdf = new_dataset.cluster2(band=band)\n    gdf.rename(columns={\"Band_1\": self.band_names[band]}, inplace=True)\n\n    return gdf\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.normalize","title":"<code>normalize(array)</code>  <code>staticmethod</code>","text":"<p>Normalize numpy arrays into scale 0.0\u20131.0.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numpy array to normalize.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized array.</p> Source code in <code>pyramids/dataset.py</code> <pre><code>@staticmethod\ndef normalize(array: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize numpy arrays into scale 0.0\u20131.0.\n\n    Args:\n        array (np.ndarray): Numpy array to normalize.\n\n    Returns:\n        np.ndarray: Normalized array.\n    \"\"\"\n    array_min = array.min()\n    array_max = array.max()\n    val = (array - array_min) / (array_max - array_min)\n    return val\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_tile","title":"<code>get_tile(size=256)</code>","text":"<p>Get tile.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the window in pixels. One value is required which is used for both the x and y size. e.g., 256 means a 256x256 window. Default is 256.</p> <code>256</code> <p>Yields:</p> Type Description <code>ndarray</code> <p>np.ndarray: Dataset array with a shape <code>[band, y, x]</code>.</p> <p>Examples:</p> <ul> <li>First, we will create a dataset with 3 rows and 5 columns.</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(3, 5)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; print(dataset)\n&lt;BLANKLINE&gt;\n            Cell size: 0.05\n            Dimension: 3 * 5\n            EPSG: 4326\n            Number of Bands: 1\n            Band names: ['Band_1']\n            Mask: -9999.0\n            Data type: float64\n            File:...\n&lt;BLANKLINE&gt;\n\n&gt;&gt;&gt; print(dataset.read_array())   # doctest: +SKIP\n[[0.55332314 0.48364841 0.67794589 0.6901816  0.70516817]\n [0.82518332 0.75657103 0.45693945 0.44331782 0.74677865]\n [0.22231314 0.96283065 0.15201337 0.03522544 0.44616888]]\n</code></pre> - The <code>get_tile</code> method splits the domain into tiles of the specified <code>size</code> using the <code>_window</code> function.</p> <p><pre><code>&gt;&gt;&gt; tile_dimensions = list(dataset._window(2))\n&gt;&gt;&gt; print(tile_dimensions)\n[(0, 0, 2, 2), (2, 0, 2, 2), (4, 0, 1, 2), (0, 2, 2, 1), (2, 2, 2, 1), (4, 2, 1, 1)]\n</code></pre> </p> <ul> <li>So the first two chunks are 22, 21 chunk, then two 12 chunks, and the last chunk is 11.</li> <li>The <code>get_tile</code> method returns a generator object that can be used to iterate over the smaller chunks of     the data.</li> </ul> <pre><code>&gt;&gt;&gt; tiles_generator = dataset.get_tile(size=2)\n&gt;&gt;&gt; print(tiles_generator)  # doctest: +SKIP\n&lt;generator object Dataset.get_tile at 0x00000145AA39E680&gt;\n&gt;&gt;&gt; print(list(tiles_generator))  # doctest: +SKIP\n[\n    array([[0.55332314, 0.48364841],\n           [0.82518332, 0.75657103]]),\n    array([[0.67794589, 0.6901816 ],\n           [0.45693945, 0.44331782]]),\n    array([[0.70516817], [0.74677865]]),\n    array([[0.22231314, 0.96283065]]),\n    array([[0.15201337, 0.03522544]]),\n    array([[0.44616888]])\n]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_tile(self, size=256) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"Get tile.\n\n    Args:\n        size (int):\n            Size of the window in pixels. One value is required which is used for both the x and y size. e.g., 256\n            means a 256x256 window. Default is 256.\n\n    Yields:\n        np.ndarray:\n            Dataset array with a shape `[band, y, x]`.\n\n    Examples:\n        - First, we will create a dataset with 3 rows and 5 columns.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(3, 5)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; print(dataset)\n          &lt;BLANKLINE&gt;\n                      Cell size: 0.05\n                      Dimension: 3 * 5\n                      EPSG: 4326\n                      Number of Bands: 1\n                      Band names: ['Band_1']\n                      Mask: -9999.0\n                      Data type: float64\n                      File:...\n          &lt;BLANKLINE&gt;\n\n          &gt;&gt;&gt; print(dataset.read_array())   # doctest: +SKIP\n          [[0.55332314 0.48364841 0.67794589 0.6901816  0.70516817]\n           [0.82518332 0.75657103 0.45693945 0.44331782 0.74677865]\n           [0.22231314 0.96283065 0.15201337 0.03522544 0.44616888]]\n\n          ```\n        - The `get_tile` method splits the domain into tiles of the specified `size` using the `_window` function.\n\n          ```python\n          &gt;&gt;&gt; tile_dimensions = list(dataset._window(2))\n          &gt;&gt;&gt; print(tile_dimensions)\n          [(0, 0, 2, 2), (2, 0, 2, 2), (4, 0, 1, 2), (0, 2, 2, 1), (2, 2, 2, 1), (4, 2, 1, 1)]\n\n          ```\n          ![get_tile](./../_images/dataset/get_tile.png)\n\n        - So the first two chunks are 2*2, 2*1 chunk, then two 1*2 chunks, and the last chunk is 1*1.\n        - The `get_tile` method returns a generator object that can be used to iterate over the smaller chunks of\n            the data.\n\n          ```python\n          &gt;&gt;&gt; tiles_generator = dataset.get_tile(size=2)\n          &gt;&gt;&gt; print(tiles_generator)  # doctest: +SKIP\n          &lt;generator object Dataset.get_tile at 0x00000145AA39E680&gt;\n          &gt;&gt;&gt; print(list(tiles_generator))  # doctest: +SKIP\n          [\n              array([[0.55332314, 0.48364841],\n                     [0.82518332, 0.75657103]]),\n              array([[0.67794589, 0.6901816 ],\n                     [0.45693945, 0.44331782]]),\n              array([[0.70516817], [0.74677865]]),\n              array([[0.22231314, 0.96283065]]),\n              array([[0.15201337, 0.03522544]]),\n              array([[0.44616888]])\n          ]\n\n          ```\n    \"\"\"\n    for xoff, yoff, xsize, ysize in self._window(size=size):\n        # read the array at certain indices\n        yield self.raster.ReadAsArray(\n            xoff=xoff, yoff=yoff, xsize=xsize, ysize=ysize\n        )\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.cluster","title":"<code>cluster(lower_bound, upper_bound)</code>","text":"<p>Group all the connected values between two bounds.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bound</code> <code>Number</code> <p>Lower bound of the cluster.</p> required <code>upper_bound</code> <code>Number</code> <p>Upper bound of the cluster.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, int, list, list]</code> <p>tuple[np.ndarray, int, list, list]: - cluster (np.ndarray):     Array with integers representing the cluster number per cell. - count (int):     Number of clusters in the array. - position (list[list[int, int]]):     List of [row, col] indices for the position of each value. - values (list[Number]):     Values stored in each cell in the cluster.</p> <p>Examples:</p> <ul> <li>First, we will create a dataset with 10 rows and 10 columns.</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(10)\n&gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n&gt;&gt;&gt; print(arr) # doctest: +SKIP\n[[2 3 3 2 3]\n [3 4 1 1 1]\n [1 3 3 2 2]\n [4 1 1 3 2]\n [2 4 2 3 2]]\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; dataset.plot(\n...     color_scale=4, bounds=[1, 1.9, 4.1, 5], display_cell_value=True, num_size=12,\n...     background_color_threshold=5\n... )  # doctest: +SKIP\n</code></pre> </p> <ul> <li>Now let's cluster the values in the dataset that are between 2 and 4.</li> </ul> <p><pre><code>&gt;&gt;&gt; lower_value = 2\n&gt;&gt;&gt; upper_value = 4\n&gt;&gt;&gt; cluster_array, count, position, values = dataset.cluster(lower_value, upper_value)\n</code></pre> - The first returned output is a binary array with 1 indicating that the cell value is inside the cluster, and 0 is outside.</p> <p><pre><code>&gt;&gt;&gt; print(cluster_array)  # doctest: +SKIP\n[[1. 1. 1. 1. 1.]\n [1. 1. 0. 0. 0.]\n [0. 1. 1. 1. 1.]\n [1. 0. 0. 1. 1.]\n [1. 1. 1. 1. 1.]]\n</code></pre> - The second returned value is the number of connected clusters.</p> <p><pre><code>&gt;&gt;&gt; print(count) # doctest: +SKIP\n2\n</code></pre> - The third returned value is the indices of the cells that belong to the cluster.</p> <p><pre><code>&gt;&gt;&gt; print(position) # doctest: +SKIP\n[[1, 0], [2, 1], [2, 2], [3, 3], [4, 3], [4, 4], [3, 4], [2, 4], [2, 3], [4, 2], [4, 1], [3, 0], [4, 0], [1, 1], [0, 2], [0, 3], [0, 4], [0, 1], [0, 0]]\n</code></pre> - The fourth returned value is a list of the values that are in the cluster (extracted from these cells).</p> <pre><code>&gt;&gt;&gt; print(values) # doctest: +SKIP\n[3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 4, 3, 2, 3, 3, 2]\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def cluster(\n    self, lower_bound: Any, upper_bound: Any\n) -&gt; Tuple[np.ndarray, int, list, list]:\n    \"\"\"Group all the connected values between two bounds.\n\n    Args:\n        lower_bound (Number):\n            Lower bound of the cluster.\n        upper_bound (Number):\n            Upper bound of the cluster.\n\n    Returns:\n        tuple[np.ndarray, int, list, list]:\n            - cluster (np.ndarray):\n                Array with integers representing the cluster number per cell.\n            - count (int):\n                Number of clusters in the array.\n            - position (list[list[int, int]]):\n                List of [row, col] indices for the position of each value.\n            - values (list[Number]):\n                Values stored in each cell in the cluster.\n\n    Examples:\n        - First, we will create a dataset with 10 rows and 10 columns.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; np.random.seed(10)\n          &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(5, 5))\n          &gt;&gt;&gt; print(arr) # doctest: +SKIP\n          [[2 3 3 2 3]\n           [3 4 1 1 1]\n           [1 3 3 2 2]\n           [4 1 1 3 2]\n           [2 4 2 3 2]]\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; dataset.plot(\n          ...     color_scale=4, bounds=[1, 1.9, 4.1, 5], display_cell_value=True, num_size=12,\n          ...     background_color_threshold=5\n          ... )  # doctest: +SKIP\n\n          ```\n          ![cluster](./../_images/dataset/cluster.png)\n\n        - Now let's cluster the values in the dataset that are between 2 and 4.\n\n          ```python\n          &gt;&gt;&gt; lower_value = 2\n          &gt;&gt;&gt; upper_value = 4\n          &gt;&gt;&gt; cluster_array, count, position, values = dataset.cluster(lower_value, upper_value)\n\n          ```\n        - The first returned output is a binary array with 1 indicating that the cell value is inside the cluster, and 0 is outside.\n\n          ```python\n          &gt;&gt;&gt; print(cluster_array)  # doctest: +SKIP\n          [[1. 1. 1. 1. 1.]\n           [1. 1. 0. 0. 0.]\n           [0. 1. 1. 1. 1.]\n           [1. 0. 0. 1. 1.]\n           [1. 1. 1. 1. 1.]]\n\n          ```\n        - The second returned value is the number of connected clusters.\n\n          ```python\n          &gt;&gt;&gt; print(count) # doctest: +SKIP\n          2\n\n          ```\n        - The third returned value is the indices of the cells that belong to the cluster.\n\n          ```python\n          &gt;&gt;&gt; print(position) # doctest: +SKIP\n          [[1, 0], [2, 1], [2, 2], [3, 3], [4, 3], [4, 4], [3, 4], [2, 4], [2, 3], [4, 2], [4, 1], [3, 0], [4, 0], [1, 1], [0, 2], [0, 3], [0, 4], [0, 1], [0, 0]]\n\n          ```\n        - The fourth returned value is a list of the values that are in the cluster (extracted from these cells).\n\n          ```python\n          &gt;&gt;&gt; print(values) # doctest: +SKIP\n          [3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 2, 4, 3, 2, 3, 3, 2]\n\n          ```\n\n    \"\"\"\n    data = self.read_array()\n    position = []\n    values = []\n    count = 1\n    cluster = np.zeros(shape=(data.shape[0], data.shape[1]))\n\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            if lower_bound &lt;= data[i, j] &lt;= upper_bound and cluster[i, j] == 0:\n                self._group_neighbours(\n                    data,\n                    i,\n                    j,\n                    lower_bound,\n                    upper_bound,\n                    position,\n                    values,\n                    count,\n                    cluster,\n                )\n                if cluster[i, j] == 0:\n                    position.append([i, j])\n                    values.append(data[i, j])\n                    cluster[i, j] = count\n                count += 1\n\n    return cluster, count, position, values\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.cluster2","title":"<code>cluster2(band=None)</code>","text":"<p>Cluster the connected equal cells into polygons.</p> <ul> <li>Creates vector polygons for all connected regions of pixels in the raster sharing a common     pixel value (group neighboring cells with the same value into one polygon).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int | List[int] | None</code> <p>Band index 0, 1, 2, 3, \u2026</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <code>GeoDataFrame</code> <p>GeodataFrame containing polygon geomtries for all connected regions.</p> <p>Examples:</p> <ul> <li>First, we will create a 10*10 dataset full of random integer between 1, and 5.</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(200)\n&gt;&gt;&gt; arr = np.random.randint(1, 5, size=(10, 10))\n&gt;&gt;&gt; print(arr)  # doctest: +SKIP\n[[3 2 1 1 3 4 1 4 2 3]\n [4 2 2 4 3 3 1 2 4 4]\n [4 2 4 2 3 4 2 1 4 3]\n [3 2 1 4 3 3 4 1 1 4]\n [1 2 4 2 2 1 3 2 3 1]\n [1 4 4 4 1 1 4 2 1 1]\n [1 3 2 3 3 4 1 3 1 3]\n [4 1 3 3 3 4 1 4 1 1]\n [2 1 3 3 4 2 2 1 3 4]\n [2 3 2 2 4 2 1 3 2 2]]\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Now, let's cluster the connected equal cells into polygons.</li> </ul> <pre><code>&gt;&gt;&gt; gdf = dataset.cluster2()\n&gt;&gt;&gt; print(gdf)  # doctest: +SKIP\n    Band_1                                           geometry\n0        3  POLYGON ((0 0, 0 -0.05, 0.05 -0.05, 0.05 0, 0 0))\n1        1  POLYGON ((0.1 0, 0.1 -0.05, 0.2 -0.05, 0.2 0, ...\n2        4  POLYGON ((0.25 0, 0.25 -0.05, 0.3 -0.05, 0.3 0...\n3        4  POLYGON ((0.35 0, 0.35 -0.05, 0.4 -0.05, 0.4 0...\n4        2  POLYGON ((0.4 0, 0.4 -0.05, 0.45 -0.05, 0.45 0...\n5        3  POLYGON ((0.45 0, 0.45 -0.05, 0.5 -0.05, 0.5 0...\n6        1  POLYGON ((0.3 0, 0.3 -0.1, 0.35 -0.1, 0.35 0, ...\n7        4  POLYGON ((0.15 -0.05, 0.15 -0.1, 0.2 -0.1, 0.2...\n8        2  POLYGON ((0.35 -0.05, 0.35 -0.1, 0.4 -0.1, 0.4...\n9        4  POLYGON ((0 -0.05, 0 -0.15, 0.05 -0.15, 0.05 -...\n10       4  POLYGON ((0.4 -0.05, 0.4 -0.15, 0.45 -0.15, 0....\n11       4  POLYGON ((0.1 -0.1, 0.1 -0.15, 0.15 -0.15, 0.1...\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def cluster2(\n    self,\n    band: Union[int, List[int]] = None,\n) -&gt; GeoDataFrame:\n    \"\"\"Cluster the connected equal cells into polygons.\n\n    - Creates vector polygons for all connected regions of pixels in the raster sharing a common\n        pixel value (group neighboring cells with the same value into one polygon).\n\n    Args:\n        band (int | List[int] | None):\n            Band index 0, 1, 2, 3, \u2026\n\n    Returns:\n        GeoDataFrame:\n            GeodataFrame containing polygon geomtries for all connected regions.\n\n    Examples:\n        - First, we will create a 10*10 dataset full of random integer between 1, and 5.\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; np.random.seed(200)\n          &gt;&gt;&gt; arr = np.random.randint(1, 5, size=(10, 10))\n          &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n          [[3 2 1 1 3 4 1 4 2 3]\n           [4 2 2 4 3 3 1 2 4 4]\n           [4 2 4 2 3 4 2 1 4 3]\n           [3 2 1 4 3 3 4 1 1 4]\n           [1 2 4 2 2 1 3 2 3 1]\n           [1 4 4 4 1 1 4 2 1 1]\n           [1 3 2 3 3 4 1 3 1 3]\n           [4 1 3 3 3 4 1 4 1 1]\n           [2 1 3 3 4 2 2 1 3 4]\n           [2 3 2 2 4 2 1 3 2 2]]\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Now, let's cluster the connected equal cells into polygons.\n\n          ```python\n          &gt;&gt;&gt; gdf = dataset.cluster2()\n          &gt;&gt;&gt; print(gdf)  # doctest: +SKIP\n              Band_1                                           geometry\n          0        3  POLYGON ((0 0, 0 -0.05, 0.05 -0.05, 0.05 0, 0 0))\n          1        1  POLYGON ((0.1 0, 0.1 -0.05, 0.2 -0.05, 0.2 0, ...\n          2        4  POLYGON ((0.25 0, 0.25 -0.05, 0.3 -0.05, 0.3 0...\n          3        4  POLYGON ((0.35 0, 0.35 -0.05, 0.4 -0.05, 0.4 0...\n          4        2  POLYGON ((0.4 0, 0.4 -0.05, 0.45 -0.05, 0.45 0...\n          5        3  POLYGON ((0.45 0, 0.45 -0.05, 0.5 -0.05, 0.5 0...\n          6        1  POLYGON ((0.3 0, 0.3 -0.1, 0.35 -0.1, 0.35 0, ...\n          7        4  POLYGON ((0.15 -0.05, 0.15 -0.1, 0.2 -0.1, 0.2...\n          8        2  POLYGON ((0.35 -0.05, 0.35 -0.1, 0.4 -0.1, 0.4...\n          9        4  POLYGON ((0 -0.05, 0 -0.15, 0.05 -0.15, 0.05 -...\n          10       4  POLYGON ((0.4 -0.05, 0.4 -0.15, 0.45 -0.15, 0....\n          11       4  POLYGON ((0.1 -0.1, 0.1 -0.15, 0.15 -0.15, 0.1...\n\n          ```\n\n    \"\"\"\n    if band is None:\n        band = 0\n\n    name = self.band_names[band]\n    gdf = self._band_to_polygon(band, name)\n\n    return gdf\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.create_overviews","title":"<code>create_overviews(resampling_method='nearest', overview_levels=None)</code>","text":"<p>Create overviews for the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>resampling_method</code> <code>str</code> <p>The resampling method used to create the overviews. Possible values are \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\", \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".</p> <code>'nearest'</code> <code>overview_levels</code> <code>list</code> <p>The overview levels. Restricted to typical power-of-two reduction factors. Defaults to [2, 4, 8, 16, 32].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Creates internal or external overviews depending on the dataset access mode. See Notes.</p> Notes <ul> <li>External (.ovr file): If the dataset is read with <code>read_only=True</code> then the overviews file will be created   as an external .ovr file in the same directory of the dataset.</li> <li>Internal: If the dataset is read with <code>read_only=False</code> then the overviews will be created internally in   the dataset, and the dataset needs to be saved/flushed to persist the changes to disk.</li> <li>You can check the count per band via the <code>overview_count</code> property.</li> </ul> <p>Examples:</p> <ul> <li>Create a Dataset with 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0):</li> </ul> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> - Now, create overviews using the default parameters:</p> <p><pre><code>&gt;&gt;&gt; dataset.create_overviews()\n&gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n[4, 4, 4, 4]\n</code></pre> - For each band, there are 4 overview levels you can use to plot the bands:</p> <p><pre><code>&gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=0) # doctest: +SKIP\n</code></pre> </p> <ul> <li>However, the dataset originally is 10*10, but the first overview level (2) displays half of the cells by   aggregating all the cells using the nearest neighbor. The second level displays only 3 cells in each:</li> </ul> <p><pre><code>&gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=1)   # doctest: +SKIP\n</code></pre> </p> <ul> <li>For the third overview level:</li> </ul> <p><pre><code>&gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=2)       # doctest: +SKIP\n</code></pre> </p> See Also <ul> <li>Dataset.recreate_overviews: Recreate the dataset overviews if they exist</li> <li>Dataset.get_overview: Get an overview of a band</li> <li>Dataset.overview_count: Number of overviews</li> <li>Dataset.read_overview_array: Read overview values</li> <li>Dataset.plot: Plot a band</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def create_overviews(\n    self, resampling_method: str = \"nearest\", overview_levels: list = None\n) -&gt; None:\n    \"\"\"Create overviews for the dataset.\n\n    Args:\n        resampling_method (str):\n            The resampling method used to create the overviews. Possible values are\n            \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\",\n            \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".\n        overview_levels (list, optional):\n            The overview levels. Restricted to typical power-of-two reduction factors. Defaults to [2, 4, 8, 16,\n            32].\n\n    Returns:\n        None:\n            Creates internal or external overviews depending on the dataset access mode. See Notes.\n\n    Notes:\n        - External (.ovr file): If the dataset is read with `read_only=True` then the overviews file will be created\n          as an external .ovr file in the same directory of the dataset.\n        - Internal: If the dataset is read with `read_only=False` then the overviews will be created internally in\n          the dataset, and the dataset needs to be saved/flushed to persist the changes to disk.\n        - You can check the count per band via the `overview_count` property.\n\n    Examples:\n        - Create a Dataset with 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.rand(4, 10, 10)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n        - Now, create overviews using the default parameters:\n\n          ```python\n          &gt;&gt;&gt; dataset.create_overviews()\n          &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n          [4, 4, 4, 4]\n\n          ```\n        - For each band, there are 4 overview levels you can use to plot the bands:\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=0) # doctest: +SKIP\n\n          ```\n          ![overviews-level-0](./../_images/dataset/overviews-level-0.png)\n\n        - However, the dataset originally is 10*10, but the first overview level (2) displays half of the cells by\n          aggregating all the cells using the nearest neighbor. The second level displays only 3 cells in each:\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=1)   # doctest: +SKIP\n\n          ```\n          ![overviews-level-1](./../_images/dataset/overviews-level-1.png)\n\n        - For the third overview level:\n\n          ```python\n          &gt;&gt;&gt; dataset.plot(band=0, overview=True, overview_index=2)       # doctest: +SKIP\n\n          ```\n          ![overviews-level-2](./../_images/dataset/overviews-level-2.png)\n\n    See Also:\n        - Dataset.recreate_overviews: Recreate the dataset overviews if they exist\n        - Dataset.get_overview: Get an overview of a band\n        - Dataset.overview_count: Number of overviews\n        - Dataset.read_overview_array: Read overview values\n        - Dataset.plot: Plot a band\n    \"\"\"\n    if overview_levels is None:\n        overview_levels = OVERVIEW_LEVELS\n    else:\n        if not isinstance(overview_levels, list):\n            raise TypeError(\"overview_levels should be a list\")\n\n        # if self.raster.HasArbitraryOverviews():\n        if not all(elem in OVERVIEW_LEVELS for elem in overview_levels):\n            raise ValueError(\n                \"overview_levels are restricted to the typical power-of-two reduction factors \"\n                \"(like 2, 4, 8, 16, etc.)\"\n            )\n\n    if resampling_method.upper() not in RESAMPLING_METHODS:\n        raise ValueError(f\"resampling_method should be one of {RESAMPLING_METHODS}\")\n    # Define the overview levels (the reduction factor).\n    # e.g., 2 means the overview will be half the resolution of the original dataset.\n\n    # Build overviews using nearest neighbor resampling\n    # NEAREST is the resampling method used. Other methods include AVERAGE, GAUSS, etc.\n    self.raster.BuildOverviews(resampling_method, overview_levels)\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.recreate_overviews","title":"<code>recreate_overviews(resampling_method='nearest')</code>","text":"<p>Recreate overviews for the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>resampling_method</code> <code>str</code> <p>Resampling method used to recreate overviews. Possible values are \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\", \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".</p> <code>'nearest'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If resampling_method is not one of the allowed values above.</p> <code>ReadOnlyError</code> <p>If overviews are internal and the dataset is opened read-only. Read with read_only=False.</p> See Also <ul> <li>Dataset.create_overviews: Recreate the dataset overviews if they exist.</li> <li>Dataset.get_overview: Get an overview of a band.</li> <li>Dataset.overview_count: Number of overviews.</li> <li>Dataset.read_overview_array: Read overview values.</li> <li>Dataset.plot: Plot a band.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def recreate_overviews(self, resampling_method: str = \"nearest\"):\n    \"\"\"Recreate overviews for the dataset.\n\n    Args:\n        resampling_method (str): Resampling method used to recreate overviews. Possible values are\n            \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\",\n            \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\". Defaults to \"nearest\".\n\n    Raises:\n        ValueError:\n            If resampling_method is not one of the allowed values above.\n        ReadOnlyError:\n            If overviews are internal and the dataset is opened read-only. Read with read_only=False.\n\n    See Also:\n        - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n        - Dataset.get_overview: Get an overview of a band.\n        - Dataset.overview_count: Number of overviews.\n        - Dataset.read_overview_array: Read overview values.\n        - Dataset.plot: Plot a band.\n    \"\"\"\n    if resampling_method.upper() not in RESAMPLING_METHODS:\n        raise ValueError(f\"resampling_method should be one of {RESAMPLING_METHODS}\")\n    # Build overviews using nearest neighbor resampling\n    # nearest is the resampling method used. Other methods include AVERAGE, GAUSS, etc.\n    try:\n        for i in range(self.band_count):\n            band = self._iloc(i)\n            for j in range(self.overview_count[i]):\n                ovr = self.get_overview(i, j)\n                # TODO: if this method takes a long time, we can use the gdal.RegenerateOverviews() method\n                #  which is faster but it does not give the option to choose the resampling method. and the\n                #  overviews has to be given to the function as a list.\n                #  overviews = [band.GetOverview(i) for i in range(band.GetOverviewCount())]\n                #  band.RegenerateOverviews(overviews) or gdal.RegenerateOverviews(overviews)\n                gdal.RegenerateOverview(band, ovr, resampling_method)\n    except RuntimeError:\n        raise ReadOnlyError(\n            \"The Dataset is opened with a read only. Please read the dataset using read_only=False\"\n        )\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_overview","title":"<code>get_overview(band=0, overview_index=0)</code>","text":"<p>Get an overview of a band.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>The band index. Defaults to 0.</p> <code>0</code> <code>overview_index</code> <code>int</code> <p>Index of the overview. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Band</code> <p>gdal.Band: GDAL band object.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n&gt;&gt;&gt; print(arr[0, :, :]) # doctest: +SKIP\narray([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n       [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n       [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n       [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n       [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n       [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n       [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n       [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n       [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n       [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Now, create overviews using the default parameters and inspect them:</li> </ul> <pre><code>&gt;&gt;&gt; dataset.create_overviews()\n&gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n[4, 4, 4, 4]\n\n&gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=0)\n&gt;&gt;&gt; print(ovr)  # doctest: +SKIP\n&lt;osgeo.gdal.Band; proxy of &lt;Swig Object of type 'GDALRasterBandShadow *' at 0x0000017E2B5AF1B0&gt; &gt;\n&gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\narray([[6, 3, 4, 4, 8],\n       [5, 9, 7, 5, 6],\n       [5, 3, 1, 7, 5],\n       [1, 4, 2, 6, 9],\n       [4, 3, 6, 5, 8]], dtype=int32)\n&gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=1)\n&gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\narray([[6, 7, 3],\n       [2, 5, 6],\n       [6, 9, 9]], dtype=int32)\n&gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=2)\n&gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\narray([[6, 8],\n       [8, 5]], dtype=int32)\n&gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=3)\n&gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\narray([[6]], dtype=int32)\n</code></pre> See Also <ul> <li>Dataset.create_overviews: Create the dataset overviews if they exist.</li> <li>Dataset.create_overviews: Recreate the dataset overviews if they exist.</li> <li>Dataset.overview_count: Number of overviews.</li> <li>Dataset.read_overview_array: Read overview values.</li> <li>Dataset.plot: Plot a band.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_overview(self, band: int = 0, overview_index: int = 0) -&gt; gdal.Band:\n    \"\"\"Get an overview of a band.\n\n    Args:\n        band (int):\n            The band index. Defaults to 0.\n        overview_index (int):\n            Index of the overview. Defaults to 0.\n\n    Returns:\n        gdal.Band:\n            GDAL band object.\n\n    Examples:\n        - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n          &gt;&gt;&gt; print(arr[0, :, :]) # doctest: +SKIP\n          array([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n                 [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n                 [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n                 [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n                 [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n                 [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n                 [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n                 [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n                 [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n                 [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Now, create overviews using the default parameters and inspect them:\n\n          ```python\n          &gt;&gt;&gt; dataset.create_overviews()\n          &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n          [4, 4, 4, 4]\n\n          &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=0)\n          &gt;&gt;&gt; print(ovr)  # doctest: +SKIP\n          &lt;osgeo.gdal.Band; proxy of &lt;Swig Object of type 'GDALRasterBandShadow *' at 0x0000017E2B5AF1B0&gt; &gt;\n          &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n          array([[6, 3, 4, 4, 8],\n                 [5, 9, 7, 5, 6],\n                 [5, 3, 1, 7, 5],\n                 [1, 4, 2, 6, 9],\n                 [4, 3, 6, 5, 8]], dtype=int32)\n          &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=1)\n          &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n          array([[6, 7, 3],\n                 [2, 5, 6],\n                 [6, 9, 9]], dtype=int32)\n          &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=2)\n          &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n          array([[6, 8],\n                 [8, 5]], dtype=int32)\n          &gt;&gt;&gt; ovr = dataset.get_overview(band=0, overview_index=3)\n          &gt;&gt;&gt; ovr.ReadAsArray()  # doctest: +SKIP\n          array([[6]], dtype=int32)\n\n          ```\n\n    See Also:\n        - Dataset.create_overviews: Create the dataset overviews if they exist.\n        - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n        - Dataset.overview_count: Number of overviews.\n        - Dataset.read_overview_array: Read overview values.\n        - Dataset.plot: Plot a band.\n    \"\"\"\n    band = self._iloc(band)\n    n_views = band.GetOverviewCount()\n    if n_views == 0:\n        raise ValueError(\n            \"The band has no overviews, please use the `create_overviews` method to build the overviews\"\n        )\n\n    if overview_index &gt;= n_views:\n        raise ValueError(f\"overview_level should be less than {n_views}\")\n\n    # TODO:find away to create a Dataset object from the overview band and to return the Dataset object instead\n    #  of the gdal band.\n    return band.GetOverview(overview_index)\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.read_overview_array","title":"<code>read_overview_array(band=None, overview_index=0)</code>","text":"<p>Read overview values.</p> <pre><code>- Read the values stored in a given band or overview.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int | None</code> <p>The band to read. If None and multiple bands exist, reads all bands at the given overview.</p> <code>None</code> <code>overview_index</code> <code>int</code> <p>Index of the overview. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array with the values in the raster.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):</li> </ul> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n&gt;&gt;&gt; print(arr[0, :, :])     # doctest: +SKIP\narray([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n       [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n       [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n       [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n       [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n       [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n       [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n       [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n       [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n       [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n</code></pre> <ul> <li>Create overviews using the default parameters and read overview arrays:</li> </ul> <pre><code>&gt;&gt;&gt; dataset.create_overviews()\n&gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n[4, 4, 4, 4]\n\n&gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=0)\n&gt;&gt;&gt; print(arr)  # doctest: +SKIP\narray([[6, 3, 4, 4, 8],\n       [5, 9, 7, 5, 6],\n       [5, 3, 1, 7, 5],\n       [1, 4, 2, 6, 9],\n       [4, 3, 6, 5, 8]], dtype=int32)\n&gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=1)\n&gt;&gt;&gt; print(arr)  # doctest: +SKIP\narray([[6, 7, 3],\n       [2, 5, 6],\n       [6, 9, 9]], dtype=int32)\n&gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=2)\n&gt;&gt;&gt; print(arr)  # doctest: +SKIP\narray([[6, 8],\n       [8, 5]], dtype=int32)\n&gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=3)\n&gt;&gt;&gt; print(arr)  # doctest: +SKIP\narray([[6]], dtype=int32)\n</code></pre> See Also <ul> <li>Dataset.create_overviews: Create the dataset overviews.</li> <li>Dataset.create_overviews: Recreate the dataset overviews if they exist.</li> <li>Dataset.get_overview: Get an overview of a band.</li> <li>Dataset.overview_count: Number of overviews.</li> <li>Dataset.plot: Plot a band.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def read_overview_array(\n    self, band: int = None, overview_index: int = 0\n) -&gt; np.ndarray:\n    \"\"\"Read overview values.\n\n        - Read the values stored in a given band or overview.\n\n    Args:\n        band (int | None):\n            The band to read. If None and multiple bands exist, reads all bands at the given overview.\n        overview_index (int):\n            Index of the overview. Defaults to 0.\n\n    Returns:\n        np.ndarray:\n            Array with the values in the raster.\n\n    Examples:\n        - Create `Dataset` consisting of 4 bands, 10 rows, 10 columns, at lon/lat (0, 0):\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1, 10, size=(4, 10, 10))\n          &gt;&gt;&gt; print(arr[0, :, :])     # doctest: +SKIP\n          array([[6, 3, 3, 7, 4, 8, 4, 3, 8, 7],\n                 [6, 7, 3, 7, 8, 6, 3, 4, 3, 8],\n                 [5, 8, 9, 6, 7, 7, 5, 4, 6, 4],\n                 [2, 9, 9, 5, 8, 4, 9, 6, 8, 7],\n                 [5, 8, 3, 9, 1, 5, 7, 9, 5, 9],\n                 [8, 3, 7, 2, 2, 5, 2, 8, 7, 7],\n                 [1, 1, 4, 2, 2, 2, 6, 5, 9, 2],\n                 [6, 3, 2, 9, 8, 8, 1, 9, 7, 7],\n                 [4, 1, 3, 1, 6, 7, 5, 4, 8, 7],\n                 [9, 7, 2, 1, 4, 6, 1, 2, 3, 3]], dtype=int32)\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n          ```\n\n        - Create overviews using the default parameters and read overview arrays:\n\n          ```python\n          &gt;&gt;&gt; dataset.create_overviews()\n          &gt;&gt;&gt; print(dataset.overview_count)  # doctest: +SKIP\n          [4, 4, 4, 4]\n\n          &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=0)\n          &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n          array([[6, 3, 4, 4, 8],\n                 [5, 9, 7, 5, 6],\n                 [5, 3, 1, 7, 5],\n                 [1, 4, 2, 6, 9],\n                 [4, 3, 6, 5, 8]], dtype=int32)\n          &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=1)\n          &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n          array([[6, 7, 3],\n                 [2, 5, 6],\n                 [6, 9, 9]], dtype=int32)\n          &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=2)\n          &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n          array([[6, 8],\n                 [8, 5]], dtype=int32)\n          &gt;&gt;&gt; arr = dataset.read_overview_array(band=0, overview_index=3)\n          &gt;&gt;&gt; print(arr)  # doctest: +SKIP\n          array([[6]], dtype=int32)\n\n          ```\n\n    See Also:\n        - Dataset.create_overviews: Create the dataset overviews.\n        - Dataset.create_overviews: Recreate the dataset overviews if they exist.\n        - Dataset.get_overview: Get an overview of a band.\n        - Dataset.overview_count: Number of overviews.\n        - Dataset.plot: Plot a band.\n    \"\"\"\n    if band is None and self.band_count &gt; 1:\n        if any(elem == 0 for elem in self.overview_count):\n            raise ValueError(\n                \"Some bands do not have overviews, please create overviews first\"\n            )\n        # read the array from the first overview to get the size of the array.\n        arr = self.get_overview(0, 0).ReadAsArray()\n        arr = np.ones(\n            (\n                self.band_count,\n                arr.shape[0],\n                arr.shape[1],\n            ),\n            dtype=self.numpy_dtype[0],\n        )\n        for i in range(self.band_count):\n            arr[i, :, :] = self.get_overview(i, overview_index).ReadAsArray()\n    else:\n        if band is None:\n            band = 0\n        else:\n            if band &gt; self.band_count - 1:\n                raise ValueError(\n                    f\"band index should be between 0 and {self.band_count - 1}\"\n                )\n            if self.overview_count[band] == 0:\n                raise ValueError(\n                    f\"band {band} has no overviews, please create overviews first\"\n                )\n        arr = self.get_overview(band, overview_index).ReadAsArray()\n\n    return arr\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_band_by_color","title":"<code>get_band_by_color(color_name)</code>","text":"<p>Get the band associated with a given color.</p> <p>Parameters:</p> Name Type Description Default <code>color_name</code> <code>str</code> <p>One of ['undefined', 'gray_index', 'palette_index', 'red', 'green', 'blue', 'alpha', 'hue', 'saturation', 'lightness', 'cyan', 'magenta', 'yellow', 'black', 'YCbCr_YBand', 'YCbCr_CbBand', 'YCbCr_CrBand'].</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Band index.</p> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consisting of 3 bands and assign RGB colors:</li> </ul> <pre><code>&gt;&gt;&gt; arr = np.random.randint(1, 3, size=(3, 10, 10))\n&gt;&gt;&gt; top_left_corner = (0, 0)\n&gt;&gt;&gt; cell_size = 0.05\n&gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n&gt;&gt;&gt; dataset.band_color = {0: 'red', 1: 'green', 2: 'blue'}\n</code></pre> <ul> <li>Now use <code>get_band_by_color</code> to know which band is the red band, for example:</li> </ul> <pre><code>&gt;&gt;&gt; band_index = dataset.get_band_by_color('red')\n&gt;&gt;&gt; print(band_index)\n0\n</code></pre> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_band_by_color(self, color_name: str) -&gt; int:\n    \"\"\"Get the band associated with a given color.\n\n    Args:\n        color_name (str):\n            One of ['undefined', 'gray_index', 'palette_index', 'red', 'green', 'blue', 'alpha', 'hue',\n            'saturation', 'lightness', 'cyan', 'magenta', 'yellow', 'black', 'YCbCr_YBand', 'YCbCr_CbBand',\n            'YCbCr_CrBand'].\n\n    Returns:\n        int:\n            Band index.\n\n    Examples:\n        - Create `Dataset` consisting of 3 bands and assign RGB colors:\n\n          ```python\n          &gt;&gt;&gt; arr = np.random.randint(1, 3, size=(3, 10, 10))\n          &gt;&gt;&gt; top_left_corner = (0, 0)\n          &gt;&gt;&gt; cell_size = 0.05\n          &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n          &gt;&gt;&gt; dataset.band_color = {0: 'red', 1: 'green', 2: 'blue'}\n\n          ```\n\n        - Now use `get_band_by_color` to know which band is the red band, for example:\n\n          ```python\n          &gt;&gt;&gt; band_index = dataset.get_band_by_color('red')\n          &gt;&gt;&gt; print(band_index)\n          0\n\n          ```\n    \"\"\"\n    colors = list(self.band_color.values())\n    if color_name not in colors:\n        band_index = None\n    else:\n        band_index = colors.index(color_name)\n    return band_index\n</code></pre>"},{"location":"reference/dataset/#pyramids.dataset.Dataset.get_histogram","title":"<code>get_histogram(band=0, bins=6, min_value=None, max_value=None, include_out_of_range=False, approx_ok=False)</code>","text":"<p>Get histogram.</p> <p>Parameters:</p> Name Type Description Default <code>band</code> <code>int</code> <p>Band index. Default is 1.</p> <code>0</code> <code>bins</code> <code>int</code> <p>Number of bins. Default is 6.</p> <code>6</code> <code>min_value</code> <code>float</code> <p>Minimum value. Default is None.</p> <code>None</code> <code>max_value</code> <code>float</code> <p>Maximum value. Default is None.</p> <code>None</code> <code>include_out_of_range</code> <code>bool</code> <p>If True, add out-of-range values into the first and last buckets. Default is False.</p> <code>False</code> <code>approx_ok</code> <code>bool</code> <p>If True, compute an approximate histogram by using subsampling or overviews. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list, list[tuple[Any, Any]]]</code> <p>tuple[list, list[tuple[Any, Any]]]: Histogram values and bin edges.</p> Hint <ul> <li> <p>The value of the histogram will be stored in an xml file by the name of the raster file with the extension     of .aux.xml.</p> </li> <li> <p>The content of the file will be like the following:   <pre><code>    &lt;PAMDataset&gt;\n      &lt;PAMRasterBand band=\"1\"&gt;\n        &lt;Description&gt;Band_1&lt;/Description&gt;\n        &lt;Histograms&gt;\n          &lt;HistItem&gt;\n            &lt;HistMin&gt;0&lt;/HistMin&gt;\n            &lt;HistMax&gt;88&lt;/HistMax&gt;\n            &lt;BucketCount&gt;6&lt;/BucketCount&gt;\n            &lt;IncludeOutOfRange&gt;0&lt;/IncludeOutOfRange&gt;\n            &lt;Approximate&gt;0&lt;/Approximate&gt;\n            &lt;HistCounts&gt;75|6|0|4|2|1&lt;/HistCounts&gt;\n          &lt;/HistItem&gt;\n        &lt;/Histograms&gt;\n      &lt;/PAMRasterBand&gt;\n    &lt;/PAMDataset&gt;\n</code></pre></p> </li> </ul> <p>Examples:</p> <ul> <li>Create <code>Dataset</code> consists of 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).</li> </ul> <p>```python</p> <p>import numpy as np arr = np.random.randint(1, 12, size=(10, 10)) print(arr)    # doctest: +SKIP   [[ 4  1  1  2  6  9  2  5  1  8]    [ 1 11  5  6  2  5  4  6  6  7]    [ 5  2 10  4  8 11  4 11 11  1]    [ 2  3  6  3  1  5 11 10 10  7]    [ 8  2 11  3  1  3  5  4 10 10]    [ 1  2  1  6 10  3  6  4  2  8]    [ 9  5  7  9  7  8  1 11  4  4]    [ 7  7  2  2  5  3  7  2  9  9]    [ 2 10  3  2  1 11  5  9  8 11]    [ 1  5  6 11  3  3  8  1  2  1]] top_left_corner = (0, 0) cell_size = 0.05 dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)</p> <p>```</p> <ul> <li>Now, let's get the histogram of the first band using the <code>get_histogram</code> method with the default     parameters:     <pre><code>&gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0)\n&gt;&gt;&gt; print(hist)  # doctest: +SKIP\n[28, 17, 10, 15, 13, 7]\n&gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n[(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n</code></pre></li> <li>we can also exclude values from the histogram by using the <code>min_value</code> and <code>max_value</code>:     <pre><code>&gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, min_value=5, max_value=10)\n&gt;&gt;&gt; print(hist)  # doctest: +SKIP\n[10, 8, 7, 7, 6, 0]\n&gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n[(1.0, 1.835), (1.835, 2.67), (2.67, 3.5), (3.5, 4.34), (4.34, 5.167), (5.167, 6.0)]\n</code></pre></li> <li>For datasets with big dimensions, computing the histogram can take some time; approximating the computation     of the histogram can save a lot of computation time. When using the parameter <code>approx_ok</code> with a <code>True</code>     value the histogram will be calculated from resampling the band or from the overviews if they exist.     <pre><code>&gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, approx_ok=True)\n&gt;&gt;&gt; print(hist)  # doctest: +SKIP\n[28, 17, 10, 15, 13, 7]\n&gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n[(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n</code></pre></li> <li>As you see for small datasets, the approximation of the histogram will be the same as without approximation.</li> </ul> Source code in <code>pyramids/dataset.py</code> <pre><code>def get_histogram(\n    self,\n    band: int = 0,\n    bins: int = 6,\n    min_value: float = None,\n    max_value: float = None,\n    include_out_of_range: bool = False,\n    approx_ok: bool = False,\n) -&gt; tuple[list, list[tuple[Any, Any]]]:\n    \"\"\"Get histogram.\n\n    Args:\n        band (int, optional):\n            Band index. Default is 1.\n        bins (int, optional):\n            Number of bins. Default is 6.\n        min_value (float, optional):\n            Minimum value. Default is None.\n        max_value (float, optional):\n            Maximum value. Default is None.\n        include_out_of_range (bool, optional):\n            If True, add out-of-range values into the first and last buckets. Default is False.\n        approx_ok (bool, optional):\n            If True, compute an approximate histogram by using subsampling or overviews. Default is False.\n\n    Returns:\n        tuple[list, list[tuple[Any, Any]]]:\n            Histogram values and bin edges.\n\n    Hint:\n        - The value of the histogram will be stored in an xml file by the name of the raster file with the extension\n            of .aux.xml.\n\n        - The content of the file will be like the following:\n          ```xml\n\n              &lt;PAMDataset&gt;\n                &lt;PAMRasterBand band=\"1\"&gt;\n                  &lt;Description&gt;Band_1&lt;/Description&gt;\n                  &lt;Histograms&gt;\n                    &lt;HistItem&gt;\n                      &lt;HistMin&gt;0&lt;/HistMin&gt;\n                      &lt;HistMax&gt;88&lt;/HistMax&gt;\n                      &lt;BucketCount&gt;6&lt;/BucketCount&gt;\n                      &lt;IncludeOutOfRange&gt;0&lt;/IncludeOutOfRange&gt;\n                      &lt;Approximate&gt;0&lt;/Approximate&gt;\n                      &lt;HistCounts&gt;75|6|0|4|2|1&lt;/HistCounts&gt;\n                    &lt;/HistItem&gt;\n                  &lt;/Histograms&gt;\n                &lt;/PAMRasterBand&gt;\n              &lt;/PAMDataset&gt;\n\n          ```\n\n    Examples:\n        - Create `Dataset` consists of 4 bands, 10 rows, 10 columns, at the point lon/lat (0, 0).\n\n          ```python\n          &gt;&gt;&gt; import numpy as np\n          &gt;&gt;&gt; arr = np.random.randint(1, 12, size=(10, 10))\n          &gt;&gt;&gt; print(arr)    # doctest: +SKIP\n          [[ 4  1  1  2  6  9  2  5  1  8]\n           [ 1 11  5  6  2  5  4  6  6  7]\n           [ 5  2 10  4  8 11  4 11 11  1]\n           [ 2  3  6  3  1  5 11 10 10  7]\n           [ 8  2 11  3  1  3  5  4 10 10]\n           [ 1  2  1  6 10  3  6  4  2  8]\n           [ 9  5  7  9  7  8  1 11  4  4]\n           [ 7  7  2  2  5  3  7  2  9  9]\n           [ 2 10  3  2  1 11  5  9  8 11]\n           [ 1  5  6 11  3  3  8  1  2  1]]\n           &gt;&gt;&gt; top_left_corner = (0, 0)\n           &gt;&gt;&gt; cell_size = 0.05\n           &gt;&gt;&gt; dataset = Dataset.create_from_array(arr, top_left_corner=top_left_corner, cell_size=cell_size, epsg=4326)\n\n           ```\n\n        - Now, let's get the histogram of the first band using the `get_histogram` method with the default\n            parameters:\n            ```python\n            &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0)\n            &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n            [28, 17, 10, 15, 13, 7]\n            &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n            [(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n\n            ```\n        - we can also exclude values from the histogram by using the `min_value` and `max_value`:\n            ```python\n            &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, min_value=5, max_value=10)\n            &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n            [10, 8, 7, 7, 6, 0]\n            &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n            [(1.0, 1.835), (1.835, 2.67), (2.67, 3.5), (3.5, 4.34), (4.34, 5.167), (5.167, 6.0)]\n\n            ```\n        - For datasets with big dimensions, computing the histogram can take some time; approximating the computation\n            of the histogram can save a lot of computation time. When using the parameter `approx_ok` with a `True`\n            value the histogram will be calculated from resampling the band or from the overviews if they exist.\n            ```python\n            &gt;&gt;&gt; hist, ranges = dataset.get_histogram(band=0, approx_ok=True)\n            &gt;&gt;&gt; print(hist)  # doctest: +SKIP\n            [28, 17, 10, 15, 13, 7]\n            &gt;&gt;&gt; print(ranges)   # doctest: +SKIP\n            [(1.0, 2.67), (2.67, 4.34), (4.34, 6.0), (6.0, 7.67), (7.67, 9.34), (9.34, 11.0)]\n\n            ```\n        - As you see for small datasets, the approximation of the histogram will be the same as without approximation.\n\n    \"\"\"\n    band = self._iloc(band)\n    min_val, max_val = band.ComputeRasterMinMax()\n    if min_value is None:\n        min_value = min_val\n    if max_value is None:\n        max_value = max_val\n\n    bin_width = (max_value - min_value) / bins\n    ranges = [\n        (min_val + i * bin_width, min_val + (i + 1) * bin_width)\n        for i in range(bins)\n    ]\n\n    hist = band.GetHistogram(\n        min=min_value,\n        max=max_value,\n        buckets=bins,\n        include_out_of_range=include_out_of_range,\n        approx_ok=approx_ok,\n    )\n    return hist, ranges\n</code></pre>"},{"location":"reference/featurecollection/","title":"FeatureCollection Class","text":""},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection","title":"<code>pyramids.featurecollection.FeatureCollection</code>","text":"<p>FeatureCollection.</p> <p>Utilities for working with vector datasets (GeoDataFrames/OGR DataSources), such as: - Reading/writing files - Converting between GeoDataFrame and OGR DataSource - Creating simple geometries (points, polygons) - Exploding multi-geometries, extracting coordinates - Rasterization to a Dataset - Reprojecting point coordinates - Computing center points</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>class FeatureCollection:\n    \"\"\"FeatureCollection.\n\n    Utilities for working with vector datasets (GeoDataFrames/OGR DataSources), such as:\n    - Reading/writing files\n    - Converting between GeoDataFrame and OGR DataSource\n    - Creating simple geometries (points, polygons)\n    - Exploding multi-geometries, extracting coordinates\n    - Rasterization to a Dataset\n    - Reprojecting point coordinates\n    - Computing center points\n    \"\"\"\n\n    def __init__(self, gdf: Union[GeoDataFrame, DataSource]):\n        \"\"\"Create a FeatureCollection object.\"\"\"\n        # read the drivers catalog\n        self._feature = gdf\n\n    def __str__(self):\n        \"\"\"__str__.\"\"\"\n        message = f\"\"\"\n            Feature: {self.feature}\n        \"\"\"\n        # EPSG: {self.epsg}\n        # Variables: {self.variables}\n        # Number of Bands: {self.band_count}\n        # Band names: {self.band_names}\n        # Dimension: {self.rows * self.columns}\n        # Mask: {self._no_data_value[0]}\n        # Data type: {self.dtype[0]}\n        return message\n\n    @property\n    def feature(self) -&gt; Union[GeoDataFrame, DataSource]:\n        \"\"\"Geodataframe or DataSource.\"\"\"\n        return self._feature\n\n    @property\n    def epsg(self) -&gt; int:\n        \"\"\"EPSG number.\"\"\"\n        return self._get_epsg()\n\n    @property\n    def total_bounds(self) -&gt; List[Number]:\n        \"\"\"Bounding coordinates `min-x`, `min-y`, `max-x`, `maxy`.\"\"\"\n        if isinstance(self.feature, GeoDataFrame):\n            bounds = self.feature.total_bounds.tolist()\n        else:\n            bounds = self.feature.GetLayer().GetExtent()\n            bounds = [bounds[0], bounds[2], bounds[1], bounds[3]]\n        return bounds\n\n    @property\n    def top_left_corner(self) -&gt; List[Number]:\n        \"\"\"Top left corner coordinates.\"\"\"\n        if isinstance(self.feature, GeoDataFrame):\n            bounds = self.feature.total_bounds.tolist()\n        else:\n            bounds = self.feature.GetLayer().GetExtent()\n\n        bounds = [bounds[0], bounds[3]]\n        return bounds\n\n    @property\n    def layers_count(self) -&gt; Union[int, None]:\n        \"\"\"layers_count.\n\n        Number of layers in a datasource.\n        \"\"\"\n        if isinstance(self.feature, DataSource) or isinstance(\n            self.feature, gdal.Dataset\n        ):\n            return self.feature.GetLayerCount()\n        else:\n            return None\n\n    @property\n    def layer_names(self) -&gt; List[str]:\n        \"\"\"OGR object layers names'.\"\"\"\n        names = []\n        if isinstance(self.feature, DataSource) or isinstance(\n            self.feature, gdal.Dataset\n        ):\n            for i in range(self.layers_count):\n                names.append(self.feature.GetLayer(i).GetLayerDefn().GetName())\n\n        return names\n\n    @property\n    def column(self) -&gt; List:\n        \"\"\"Column Names.\"\"\"\n        names = []\n        if isinstance(self.feature, DataSource) or isinstance(\n            self.feature, gdal.Dataset\n        ):\n            for i in range(self.layers_count):\n                layer_dfn = self.feature.GetLayer(i).GetLayerDefn()\n                cols = layer_dfn.GetFieldCount()\n                names = names + [\n                    layer_dfn.GetFieldDefn(j).GetName() for j in range(cols)\n                ]\n            names = names + [\"geometry\"]\n        else:\n            names = self.feature.columns.tolist()\n        return names\n\n    @property\n    def file_name(self) -&gt; str:\n        \"\"\"Get file name in case of the base object is an ogr.Datasource or gdal.Dataset.\"\"\"\n        if isinstance(self.feature, gdal.Dataset) or isinstance(\n            self.feature, DataSource\n        ):\n            file_name = self.feature.GetFileList()[0]\n        else:\n            file_name = \"\"\n\n        return file_name\n\n    @property\n    def dtypes(self) -&gt; Dict[str, str]:\n        \"\"\"Data Type.\n\n            - Get the data types of the columns in the vector file.\n\n        Returns:\n            list of the data types (strings/numpy datatypes) of the columns in the vector file, except the geometry\n            column.\n        \"\"\"\n        if isinstance(self.feature, GeoDataFrame):\n            dtypes = self.feature.dtypes.to_dict()\n            # convert the dtype to string as it returns a dtype object in linux instead.\n            dtypes = {key: str(value) for key, value in dtypes.items()}\n        else:\n            dtypes = []\n            for i in range(self.layers_count):\n                layer_dfn = self.feature.GetLayer(i).GetLayerDefn()\n                cols = layer_dfn.GetFieldCount()\n                dtypes = dtypes + [\n                    ogr_to_numpy_dtype(layer_dfn.GetFieldDefn(j).GetType()).__name__\n                    for j in range(cols)\n                ]\n            # the geometry column is not in the returned dictionary if the vector is DataSource\n            dtypes = {col_i: type_i for col_i, type_i in zip(self.column, dtypes)}\n\n        return dtypes\n\n    @classmethod\n    def read_file(cls, path: str) -&gt; \"FeatureCollection\":\n        \"\"\"Open a vector dataset using OGR or GeoPandas.\n\n        Args:\n            path (str): Path to vector file.\n\n        Returns:\n            FeatureCollection: A FeatureCollection wrapping the GeoDataFrame.\n        \"\"\"\n        gdf = gpd.read_file(path)\n\n        # update = False if read_only else True\n        # ds = ogr.OpenShared(path, update=update)\n        # # ds = gdal.OpenEx(path)\n        return cls(gdf)\n\n    @staticmethod\n    def create_ds(driver: str = \"geojson\", path: str = None) -&gt; Union[DataSource, None]:\n        \"\"\"Create OGR DataSource.\n\n        Args:\n            driver (str): Driver type [\"GeoJSON\", \"memory\"].\n            path (str): Path to save the vector data.\n\n        Returns:\n            DataSource | None:\n                Created OGR DataSource or None if inplace behavior applies elsewhere.\n        \"\"\"\n        driver = driver.lower()\n        gdal_name = CATALOG.get_gdal_name(driver)\n\n        if gdal_name is None:\n            raise DriverNotExistError(f\"The given driver:{driver} is not suported.\")\n\n        if driver == \"memory\":\n            path = \"memData\"\n\n        ds = FeatureCollection._create_driver(gdal_name, path)\n        return ds\n\n    @staticmethod\n    def _create_driver(driver: str, path: str):\n        \"\"\"Create Driver.\"\"\"\n        return ogr.GetDriverByName(driver).CreateDataSource(path)\n\n    @staticmethod\n    def _copy_driver_to_memory(ds: DataSource, name: str = \"memory\") -&gt; DataSource:\n        \"\"\"Copy driver to a memory driver.\n\n        Args:\n            ds (DataSource): OGR datasource.\n            name (str): Datasource name.\n\n        Returns:\n            DataSource: The copied in-memory OGR DataSource.\n        \"\"\"\n        return ogr.GetDriverByName(\"Memory\").CopyDataSource(ds, name)\n\n    def to_file(self, path: str, driver: str = \"geojson\") -&gt; None:\n        \"\"\"Save FeatureCollection to disk.\n\n            Currently, saves OGR DataSource to disk.\n\n        Args:\n            path (str):\n                Path to save the vector.\n            driver (str):\n                Driver type.\n\n        Returns:\n            None\n        \"\"\"\n        driver_gdal_name = CATALOG.get_gdal_name(driver)\n        if isinstance(self.feature, DataSource):\n            ogr.GetDriverByName(driver_gdal_name).CopyDataSource(self.feature, path)\n        else:\n            self.feature.to_file(path, driver=driver_gdal_name)\n\n    def _gdf_to_ds(\n        self, inplace: bool = False, gdal_dataset=False\n    ) -&gt; Union[DataSource, \"FeatureCollection\", None]:\n        \"\"\"Convert a GeoPandas GeoDataFrame into an OGR DataSource.\n\n        Args:\n            inplace (bool):\n                Convert the GeoDataFrame to DataSource in place. Default is False.\n            gdal_dataset (bool):\n                True to convert the GeoDataFrame into a GDAL Dataset (the object created by reading the vector with gdal.OpenEx). Default is False.\n\n        Returns:\n            DataSource | FeatureCollection | None:\n                OGR DataSource, or a FeatureCollection wrapper if not inplace.\n        \"\"\"\n        if isinstance(self.feature, GeoDataFrame):\n            gdf_json = json.loads(self.feature.to_json())\n            geojson_str = json.dumps(gdf_json)\n            # Use the /vsimem/ (Virtual File Systems) to write the GeoJSON string to memory\n            gdal.FileFromMemBuffer(MEMORY_FILE, geojson_str)\n            # Use OGR to open the GeoJSON from memory\n            if not gdal_dataset:\n                drv = ogr.GetDriverByName(\"GeoJSON\")\n                ds = drv.Open(MEMORY_FILE)\n            else:\n                ds = gdal.OpenEx(MEMORY_FILE)\n        else:\n            ds = self.feature\n\n        if inplace:\n            self.__init__(ds)\n            ds = None\n        else:\n            ds = FeatureCollection(ds)\n\n        return ds\n\n    # def _gdf_to_ds_copy(self, inplace=False) -&gt; DataSource:\n    #     \"\"\"Convert ogr DataSource object to a GeoDataFrame.\n    #\n    #     Returns:\n    #         ogr.DataSource\n    #     \"\"\"\n    #     # Create a temporary directory for files.\n    #     temp_dir = tempfile.mkdtemp()\n    #     new_vector_path = os.path.join(temp_dir, f\"{uuid.uuid1()}.geojson\")\n    #     if isinstance(self.feature, GeoDataFrame):\n    #         self.feature.to_file(new_vector_path)\n    #         ds = ogr.Open(new_vector_path)\n    #         # ds = FeatureCollection(ds)\n    #         ds = FeatureCollection._copy_driver_to_memory(ds)\n    #     else:\n    #         ds = FeatureCollection._copy_driver_to_memory(self.feature)\n    #\n    #     if inplace:\n    #         self.__init__(ds)\n    #         ds = None\n    #\n    #     return ds\n\n    def _ds_to_gdf_with_io(self, inplace: bool = False) -&gt; GeoDataFrame:\n        \"\"\"Convert ogr DataSource object to a GeoDataFrame.\n\n        Returns:\n            GeoDataFrame\n        \"\"\"\n        # Create a temporary directory for files.\n        temp_dir = tempfile.mkdtemp()\n        new_vector_path = os.path.join(temp_dir, f\"{uuid.uuid1()}.geojson\")\n        self.to_file(new_vector_path, driver=\"geojson\")\n        gdf = gpd.read_file(new_vector_path)\n\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        if inplace:\n            self.__init__(gdf)\n            gdf = None\n\n        return gdf\n\n    def _ds_to_gdf_in_memory(self, inplace: bool = False) -&gt; GeoDataFrame:\n        \"\"\"Convert ogr DataSource object to a GeoDataFrame.\n\n        Returns:\n            GeoDataFrame\n        \"\"\"\n        gdal_ds = ogr_ds_togdal_dataset(self.feature)\n        layer_name = gdal_ds.GetLayer().GetName()  # self.layer_names[0]\n        gdal.VectorTranslate(\n            MEMORY_FILE,\n            gdal_ds,\n            SQLStatement=f\"SELECT * FROM {layer_name}\",\n            layerName=layer_name,\n        )\n        # import fiona\n        # from fiona import MemoryFile\n        # f = MemoryFile(MEMORY_FILE)\n        # f = fiona.Collection(MEMORY_FILE)\n\n        # f = fiona.open(MEMORY_FILE, driver='geojson')\n        # gdf = gpd.GeoDataFrame.from_features(f, crs=f.crs)\n        gdf = gpd.read_file(MEMORY_FILE, layer=layer_name, driver=\"geojson\")\n\n        if inplace:\n            self.__init__(gdf)\n            gdf = None\n\n        return gdf\n\n    def _ds_to_gdf(self, inplace: bool = False) -&gt; GeoDataFrame:\n        \"\"\"Convert ogr DataSource object to a GeoDataFrame.\n\n        Returns:\n            GeoDataFrame\n        \"\"\"\n        try:\n            gdf = self._ds_to_gdf_in_memory(inplace=inplace)\n        except:  # pragma: no cover\n            # keep the exception unspecified and we want to catch fiona.errors.DriverError but we do not want to\n            # explicitly import fiona here\n            gdf = self._ds_to_gdf_with_io(inplace=inplace)\n\n        return gdf\n\n    def to_dataset(\n        self,\n        cell_size: Any = None,\n        dataset=None,\n        column_name: Union[str, List[str]] = None,\n    ) -&gt; \"Dataset\":\n        \"\"\"Covert a vector into raster.\n\n            - The raster cell values will be taken from the column name given in the vector_filed in the vector file.\n            - all the new raster geotransform data will be copied from the given raster.\n            - raster and vector should have the same projection\n\n        Args:\n            cell_size (int | None):\n                Cell size for the new raster. Optional if dataset is provided. Default is None.\n            dataset (Dataset | None):\n                Raster object to copy geotransform (projection, rows, columns, location) from. Optional if cell_size is\n                provided. Default is None.\n            column_name (str | List[str] | None):\n                Column name(s) in the vector to burn values from. If None, all columns are considered as bands.\n                Default is None.\n\n        Returns:\n            Dataset:\n                Single-band raster with vector geometries burned.\n        \"\"\"\n        from pyramids.dataset import Dataset\n\n        if cell_size is None and dataset is None:\n            raise ValueError(\"You have to enter either cell size of Dataset object\")\n\n        # Check EPSG are same, if not reproject vector.\n        ds_epsg = self.epsg\n        if dataset is not None:\n            if dataset.epsg != ds_epsg:\n                raise ValueError(\n                    f\"Dataset and vector are not the same EPSG. {dataset.epsg} != {ds_epsg}\"\n                )\n\n        # TODO: this case\n        if dataset is not None:\n            if not isinstance(dataset, Dataset):\n                raise TypeError(\n                    \"The second parameter should be a Dataset object (check how to read a raster using the \"\n                    \"Dataset module)\"\n                )\n            # if the raster is given, the top left corner of the raster will be taken as the top left corner for\n            # the rasterized polygon\n            xmin, ymax = dataset.top_left_corner\n            no_data_value = (\n                dataset.no_data_value[0]\n                if dataset.no_data_value[0] is not None\n                else np.nan\n            )\n            rows = dataset.rows\n            columns = dataset.columns\n            cell_size = dataset.cell_size\n        else:\n            # if a raster is not given, the xmin and ymax will be taken as the top left corner for the rasterized\n            # polygon.\n            xmin, ymin, xmax, ymax = self.feature.total_bounds\n            no_data_value = Dataset.default_no_data_value\n            columns = int(np.ceil((xmax - xmin) / cell_size))\n            rows = int(np.ceil((ymax - ymin) / cell_size))\n\n        burn_values = None\n        if column_name is None:\n            column_name = self.column\n            column_name.remove(\"geometry\")\n\n        if isinstance(column_name, list):\n            numpy_dtype = self.dtypes[column_name[0]]\n        else:\n            numpy_dtype = self.dtypes[column_name]\n\n        dtype = str(numpy_dtype)\n        attribute = column_name\n\n        # convert the vector to a gdal Dataset (vector but read by gdal.EX)\n        vector_gdal_ex = self._gdf_to_ds(gdal_dataset=True)\n        top_left_corner = (xmin, ymax)\n\n        bands_count = 1 if not isinstance(attribute, list) else len(attribute)\n        dataset_n = Dataset.create(\n            cell_size,\n            rows,\n            columns,\n            dtype,\n            bands_count,\n            top_left_corner,\n            ds_epsg,\n            no_data_value,\n        )\n\n        bands = list(range(1, bands_count + 1))\n        # loop over bands\n        for ind, band in enumerate(bands):\n            rasterize_opts = gdal.RasterizeOptions(\n                bands=[band],\n                burnValues=burn_values,\n                attribute=attribute[ind] if isinstance(attribute, list) else attribute,\n                allTouched=True,\n            )\n            # if the second parameter to the Rasterize function is str, it will be read using gdal.OpenEX inside the\n            # function, so if the second parameter is not str, it should be a dataset, if you try to use ogr.DataSource\n            # it will give an error.\n            # the second parameter can be given as a path, or read the vector using gdal.OpenEX and use it as a\n            # second parameter.\n            _ = gdal.Rasterize(\n                dataset_n.raster, vector_gdal_ex.feature, options=rasterize_opts\n            )\n\n        return dataset_n\n\n    @staticmethod\n    def _get_ds_epsg(ds: DataSource):\n        \"\"\"Get EPSG for a given OGR DataSource.\n\n        Args:\n            ds (DataSource):\n                OGR datasource (vector file read by OGR).\n\n        Returns:\n            int:\n                EPSG number.\n        \"\"\"\n        layer = ds.GetLayer(0)\n        spatial_ref = layer.GetSpatialRef()\n        spatial_ref.AutoIdentifyEPSG()\n        epsg = int(spatial_ref.GetAuthorityCode(None))\n        return epsg\n\n    @staticmethod\n    def _create_sr_from_proj(prj: str, string_type: str = None):\n        r\"\"\"Create a spatial reference object from projection.\n\n        Args:\n            prj (str):\n                Projection string, e.g.,\n                ```python\n                \"GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\n                \\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],\n                UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AXIS[\\\"Latitude\\\",NORTH],\n                AXIS[\\\"Longitude\\\",EAST],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]]\"\n\n                ```\n            string_type (str):\n                Type of the string [\"ESRI wkt\", \"WKT\", \"PROj4\"].\n        \"\"\"\n        srs = osr.SpatialReference()\n\n        if string_type is None:\n            srs.ImportFromWkt(prj)\n        elif prj.startswith(\"PROJCS\") or prj.startswith(\"GEOGCS\"):\n            # ESRI well know text strings,\n            srs.ImportFromESRI([prj])\n        else:\n            # Proj4 strings.\n            srs.ImportFromProj4(prj)\n\n        return srs\n\n    @staticmethod\n    def get_epsg_from_prj(prj: str) -&gt; int:\n        \"\"\"Create spatial reference from the projection then auto identify the epsg using the osr object.\n\n        Args:\n            prj (str): Projection string.\n\n        Returns:\n            int: epsg number\n\n        Examples:\n            - Get EPSG from a dataset projection:\n\n              ```python\n              &gt;&gt;&gt; from pyramids.dataset import Dataset\n              &gt;&gt;&gt; src = Dataset.read_file(\"path/to/raster.tif\")\n              &gt;&gt;&gt; prj = src.GetProjection()\n              &gt;&gt;&gt; epsg = FeatureCollection.get_epsg_from_prj(prj)\n\n              ```\n        \"\"\"\n        if prj != \"\":\n            srs = FeatureCollection._create_sr_from_proj(prj)\n            try:\n                # if could not identify epsg use the authority code.\n                response = srs.AutoIdentifyEPSG()\n            except RuntimeError:\n                response = 6\n\n            if response == 0:\n                epsg = int(srs.GetAuthorityCode(None))\n            else:\n                # the GetAuthorityCode failed to identify the epsg number https://gdal.org/doxygen/classOGRSpatialReference.html\n                # srs.FindMatches()\n                epsg = int(srs.GetAttrValue(\"AUTHORITY\", 1))\n        else:\n            epsg = 4326\n        return epsg\n\n    @staticmethod\n    def _get_gdf_epsg(gdf: GeoDataFrame):\n        \"\"\"Get epsg for a given geodataframe.\n\n        Args:\n            gdf (GeoDataFrame):\n                Vector file read by geopandas.\n\n        Returns:\n            int: epsg number\n        \"\"\"\n        return gdf.crs.to_epsg()\n\n    def _get_epsg(self) -&gt; int:\n        \"\"\"getEPSG.\n\n        Returns:\n            int: epsg number\n        \"\"\"\n        vector_obj = self.feature\n        if isinstance(vector_obj, ogr.DataSource):\n            epsg = FeatureCollection._get_ds_epsg(vector_obj)\n        elif isinstance(vector_obj, gpd.GeoDataFrame):\n            epsg = FeatureCollection._get_gdf_epsg(vector_obj)\n        else:\n            raise ValueError(\n                f\"Unable to get EPSG from: {type(vector_obj)}, only ogr.Datasource and \"\n                \"geopandas.GeoDataFrame are supported\"\n            )\n        return epsg\n\n    @staticmethod\n    def _get_xy_coords(geometry, coord_type: str) -&gt; List:\n        \"\"\"getXYCoords.\n\n           Returns either x or y coordinates from geometry coordinate sequence.\n           Used with LineString and Polygon geometries.\n\n        Args:\n            geometry (LineString):\n                The geometry of a shapefile.\n            coord_type (str):\n                Either \"x\" or \"y\".\n\n        Returns:\n            array: Contains x coordinates or y coordinates of all edges of the shapefile\n        \"\"\"\n        if coord_type == \"x\":\n            coords = geometry.coords.xy[0].tolist()\n        elif coord_type == \"y\":\n            coords = geometry.coords.xy[1].tolist()\n        else:\n            raise ValueError(\"coord_type can only have a value of 'x' or 'y' \")\n\n        return coords\n\n    @staticmethod\n    def _get_point_coords(geometry: Point, coord_type: str) -&gt; Union[float, int]:\n        \"\"\"Get point coordinates for a Point geometry.\n\n        Returns coordinates of a Point object.\n\n        Args:\n            geometry (Point):\n                The geometry of a shapefile.\n            coord_type (str):\n                Either \"x\" or \"y\".\n\n        Returns:\n            float | int:\n                The x or y coordinate of the Point according to coord_type.\n        \"\"\"\n        if coord_type == \"x\":\n            coord = geometry.x\n        elif coord_type == \"y\":\n            coord = geometry.y\n        else:\n            raise ValueError(\"coord_type can only have a value of 'x' or 'y' \")\n\n        return coord\n\n    @staticmethod\n    def _get_line_coords(geometry: LineString, coord_type: str):\n        \"\"\"Get coordinates of a LineString object.\n\n        Args:\n            geometry (LineString):\n                The geometry of a shapefile.\n            coord_type (str):\n                Either \"x\" or \"y\".\n\n        Returns:\n            list: Contains x or y coordinates of all edges of the shapefile.\n        \"\"\"\n        return FeatureCollection._get_xy_coords(geometry, coord_type)\n\n    @staticmethod\n    def _get_poly_coords(geometry: Polygon, coord_type: str) -&gt; List:\n        \"\"\"Get coordinates of a Polygon's exterior.\n\n        Args:\n            geometry (Polygon):\n                The geometry of a shapefile.\n            coord_type (str):\n                Either \"x\" or \"y\".\n\n        Returns:\n            list:\n                Contains x or y coordinates of all edges of the shapefile.\n        \"\"\"\n        # convert the polygon into lines\n        ext = geometry.exterior  # type = LinearRing\n\n        return FeatureCollection._get_xy_coords(ext, coord_type)\n\n    @staticmethod\n    def _explode_multi_geometry(multi_polygon: MultiPolygon):\n        \"\"\"Explode a MultiPolygon into its Polygon parts.\n\n        Args:\n            multi_polygon (MultiPolygon):\n                A MultiPolygon geometry.\n\n        Returns:\n            list:\n                List of Polygon geometries.\n        \"\"\"\n        # outdf = gpd.GeoDataFrame()\n        # multdf = gpd.GeoDataFrame()\n        # multdf[\"geometry\"] = list(multi_polygon)\n        # n_rows = len(multi_polygon)\n        # multdf = multdf.append([multi_polygon] * n_rows, ignore_index=True)\n        # for i, poly in enumerate(multi_polygon):\n        #     multdf.loc[i, \"geometry\"] = poly\n        return list(multi_polygon)\n\n    @staticmethod\n    def _explode_gdf(gdf: GeoDataFrame, geometry: str = \"multipolygon\"):\n        \"\"\"Explode Multi geometries into single geometries.\n\n        Explodes MultiPolygon (or specified multi-geometry) into separate geometries per row.\n\n        Args:\n            gdf (GeoDataFrame):\n                GeoDataFrame to explode.\n            geometry (str):\n                The multi-geometry type to explode. Default is \"multipolygon\".\n\n        Returns:\n            GeoDataFrame:\n                A new GeoDataFrame with exploded geometries.\n        \"\"\"\n        # explode the multi_polygon into polygon\n        new_gdf = gpd.GeoDataFrame()\n        to_drop = []\n        for idx, row in gdf.iterrows():\n            geom_type = row.geometry.geom_type.lower()\n            if geom_type == geometry:\n                # get number of the polygons inside the multipolygon class\n                n_rows = len(row.geometry.geoms)\n                new_gdf = gpd.GeoDataFrame(pd.concat([new_gdf] + [row] * n_rows))\n                new_gdf.reset_index(drop=True, inplace=True)\n                new_gdf.columns = row.index.values\n                # for each rows assign each polygon\n                for geom in range(n_rows):\n                    new_gdf.loc[geom, \"geometry\"] = row.geometry.geoms[geom]\n                to_drop.append(idx)\n\n        # drop the exploded rows\n        gdf.drop(labels=to_drop, axis=0, inplace=True)\n        # concatinate the exploded rows\n        new_gdf = gpd.GeoDataFrame(pd.concat([gdf] + [new_gdf]))\n        new_gdf.reset_index(drop=True, inplace=True)\n        new_gdf.columns = gdf.columns\n        return new_gdf\n\n    @staticmethod\n    def _multi_geom_handler(\n        multi_geometry: Union[MultiPolygon, MultiPoint, MultiLineString],\n        coord_type: str,\n        geom_type: str,\n    ):\n        \"\"\"Handle multi-geometries by merging coordinates.\n\n        Function for handling multi-geometries (MultiPoint, MultiLineString, MultiPolygon).\n        Returns a list of coordinates where all parts are merged into a single list; individual geometries are\n        separated with np.nan.\n\n        Args:\n            multi_geometry (MultiPolygon | MultiPoint | MultiLineString):\n                The geometry of a shapefile.\n            coord_type (str):\n                Either \"x\" or \"y\".\n            geom_type (str):\n                \"MultiPoint\" or \"MultiLineString\" or \"MultiPolygon\".\n\n        Returns:\n            list: Contains x or y coordinates of all edges of the shapefile.\n        \"\"\"\n        coord_arrays = []\n        geom_type = geom_type.lower()\n        if geom_type == \"multipoint\" or geom_type == \"multilinestring\":\n            for i, part in enumerate(multi_geometry.geoms):\n                if geom_type == \"multipoint\":\n                    vals = FeatureCollection._get_point_coords(part, coord_type)\n                    coord_arrays.append(vals)\n                elif geom_type == \"multilinestring\":\n                    vals = FeatureCollection._get_line_coords(part, coord_type)\n                    coord_arrays.append(vals)\n        elif geom_type == \"multipolygon\":\n            for i, part in enumerate(multi_geometry.geoms):\n                # multi_2_single = FeatureCollection._explode(part) if part.type.startswith(\"MULTI\") else part\n                vals = FeatureCollection._get_poly_coords(part, coord_type)\n                coord_arrays.append(vals)\n\n        return coord_arrays\n\n    @staticmethod\n    def _get_coords(row, geom_col: str, coord_type: str):\n        \"\"\"Get coordinates ('x' or 'y') of a geometry row.\n\n        Returns coordinates for Point, LineString, or Polygon as a list. Can also handle Multi geometries\n        (not MultiPolygon) appropriately.\n\n        Args:\n            row (pd.Series):\n                A whole row of the GeoDataFrame.\n            geom_col (str):\n                Name of the column where the geometry is stored in the dataframe.\n            coord_type (str):\n                \"x\" or \"y\" to choose which coordinate to get.\n\n        Returns:\n            list | int:\n                Coordinates or -9999 for multipolygon to mark for removal.\n        \"\"\"\n        # get geometry object\n        geom = row[geom_col]\n        # check the geometry type\n        gtype = geom.geom_type.lower()\n        # \"Normal\" geometries\n        if gtype == \"point\":\n            return FeatureCollection._get_point_coords(geom, coord_type)\n        elif gtype == \"linestring\":\n            return list(FeatureCollection._get_line_coords(geom, coord_type))\n        elif gtype == \"polygon\":\n            return list(FeatureCollection._get_poly_coords(geom, coord_type))\n        elif gtype == \"multipolygon\":\n            # the multipolygon geometry row will be deleted in the xy method\n            return -9999\n        elif gtype == \"geometrycollection\":\n            return FeatureCollection._geometry_collection(geom, coord_type)\n        # Multi geometries\n        else:\n            return FeatureCollection._multi_geom_handler(geom, coord_type, gtype)\n\n    def xy(self) -&gt; None:\n        \"\"\"Compute x and y coordinates of all vertices.\n\n        Processes the geometry column of the GeoDataFrame and returns the x and y coordinates of all the vertices.\n\n        Returns:\n            None\n        \"\"\"\n        # explode the gdf if the Geometry of type MultiPolygon\n        gdf = self._explode_gdf(self._feature, geometry=\"multipolygon\")\n        gdf = self._explode_gdf(gdf, geometry=\"geometrycollection\")\n        self._feature = gdf\n\n        # get the x &amp; y coordinates of the exploded multi_polygons\n        self._feature[\"x\"] = self._feature.apply(\n            self._get_coords, geom_col=\"geometry\", coord_type=\"x\", axis=1\n        )\n        self._feature[\"y\"] = self._feature.apply(\n            self._get_coords, geom_col=\"geometry\", coord_type=\"y\", axis=1\n        )\n\n        to_delete = np.where(self._feature[\"x\"] == -9999)[0]\n        self._feature.drop(to_delete, inplace=True)\n        self._feature.reset_index(drop=True, inplace=True)\n\n    @staticmethod\n    def create_polygon(\n        coords: List[Tuple[float, float]], wkt: bool = False\n    ) -&gt; Union[str, Polygon]:\n        \"\"\"Create a polygon geometry from coordinates.\n\n        Args:\n            coords (List[Tuple[float, float]]):\n                List of (x, y) tuples.\n            wkt (bool):\n                True to return Well-Known Text (WKT) string; False to return a Shapely Polygon object.\n\n        Returns:\n            str | Polygon:\n                WKT string if wkt is True; otherwise a Shapely Polygon object.\n\n        Examples:\n            - Create a WKT polygon from coordinates and print it:\n\n              ```python\n              &gt;&gt;&gt; coordinates = [(-106.64, 24), (-106.49, 24.05), (-106.49, 24.01), (-106.49, 23.98)]\n              &gt;&gt;&gt; feature_collection = FeatureCollection.create_polygon(coordinates, wkt=True)\n              &gt;&gt;&gt; print(feature_collection)\n              'POLYGON ((-106.64 24, -106.49 24.05, -106.49 24.01, -106.49 23.98, -106.64 24))'\n\n              ```\n\n            - Create a Shapely Polygon and assign it to a GeoDataFrame:\n\n              ```python\n              &gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n              &gt;&gt;&gt; new_geometry.loc[0,'geometry'] = FeatureCollection.create_polygon(coordinates, wkt=False)\n\n              ```\n        \"\"\"\n        poly = Polygon(coords)\n        if wkt:\n            return poly.wkt\n        else:\n            return poly\n\n    @staticmethod\n    def create_point(\n        coords: Iterable[Tuple[float]], epsg: int = None\n    ) -&gt; Union[List[Point], GeoDataFrame]:\n        \"\"\"Create Shapely Point objects from coordinate tuples.\n\n        Args:\n            coords (Iterable[Tuple[float]]):\n                List of tuples [(x1, y1), (x2, y2)] or [(lon1, lat1), (lon2, lat1)].\n            epsg (int):\n                EPSG number for coordinates. If provided, returns a GeoDataFrame wrapped as FeatureCollection.\n\n        Returns:\n            list | FeatureCollection:\n                List of Shapely Point objects, or FeatureCollection if epsg is provided.\n\n        Examples:\n            - Create points and assign to a GeoDataFrame:\n\n              ```python\n              &gt;&gt;&gt; coordinates = [(24.95, 60.16), (24.95, 60.16), (24.95, 60.17), (24.95, 60.16)]\n              &gt;&gt;&gt; point_list = FeatureCollection.create_point(coordinates)\n              &gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n              &gt;&gt;&gt; new_geometry.loc[:, 'geometry'] = point_list\n\n              ```\n        \"\"\"\n        points = list(map(Point, coords))\n\n        if epsg is not None:\n            points = gpd.GeoDataFrame(columns=[\"geometry\"], data=points, crs=epsg)\n            points = FeatureCollection(points)\n\n        return points\n\n    def concate(self, gdf: GeoDataFrame, inplace: bool = False) -&gt; Union[GeoDataFrame, None]:\n        \"\"\"Concatenate two shapefiles into one object.\n\n        Args:\n            gdf (GeoDataFrame):\n                GeoDataFrame containing the geometries to combine.\n            inplace (bool):\n                If True, modifies the current object in place. Default is False.\n\n        Returns:\n            GeoDataFrame | None:\n                New combined GeoDataFrame, or None if inplace is True.\n\n        Examples:\n            - Concatenate two GeoDataFrames:\n\n              ```python\n              &gt;&gt;&gt; subbasins = FeatureCollection.read_file(\"sub-basins.shp\")\n              &gt;&gt;&gt; new_sub = gpd.read_file(\"new-sub-basins.shp\")\n              &gt;&gt;&gt; all_subs = subbasins.concate(new_sub, new_sub, inplace=False)\n\n              ```\n        \"\"\"\n        # concatenate the second shapefile into the first shapefile\n        new_gdf = gpd.GeoDataFrame(pd.concat([self.feature, gdf]))\n        # re-index the data frame\n        new_gdf.index = [i for i in range(len(new_gdf))]\n        # take the spatial reference of the first geodataframe\n        new_gdf.crs = self.feature.crs\n        if inplace:\n            self.__init__(new_gdf)\n            new_gdf = None\n\n        return new_gdf\n\n    # @staticmethod\n    # def gcs_distance(coords_1: tuple, coords_2: tuple):\n    #     \"\"\"GCS_distance.\n    #\n    #     this function calculates the distance between two points that have\n    #     geographic coordinate system\n    #\n    #     parameters:\n    #         coords_1: [Tuple]\n    #             tuple of (long, lat) of the first point\n    #         coords_2: [Tuple]\n    #             tuple of (long, lat) of the second point\n    #\n    #     Returns:\n    #         distance between the two points\n    #\n    #     Examples:\n    #     ```python\n    #     &gt;&gt;&gt; point_1 = (52.22, 21.01)\n    #     &gt;&gt;&gt; point_2 = (52.40, 16.92)\n    #     &gt;&gt;&gt; distance = FeatureCollection.gcs_distance(point_1, point_2)\n    #\n    #     ```\n    #     \"\"\"\n    #     import_error_msg = f\"The triggered function requires geopy package to be install, please install is manually\"\n    #     import_geopy(import_error_msg)\n    #     import geopy.distance as distance\n    #     dist = distance.vincenty(coords_1, coords_2).m\n    #\n    #     return dist\n\n    @staticmethod\n    def reproject_points(\n        lat: list,\n        lon: list,\n        from_epsg: int = 4326,\n        to_epsg: int = 3857,\n        precision: int = 6,\n    ) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"reproject_points.\n\n        This function changes the projection of coordinates from one coordinate system to another (default: from GCS to Web Mercator as used by Google Maps).\n\n        Args:\n            lat (list):\n                List of latitudes of the points.\n            lon (list):\n                List of longitudes of the points.\n            from_epsg (int):\n                Reference number of the source projection (https://epsg.io/).\n            to_epsg (int):\n                Reference number of the target projection (https://epsg.io/).\n            precision (int):\n                Number of decimal places.\n\n        Returns:\n            tuple[list, list]:\n                y coordinates list, x coordinates list of the points.\n\n        Examples:\n            - From Web Mercator to GCS WGS84:\n\n              ```python\n              &gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n              &gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n              &gt;&gt;&gt;  longs, lats = FeatureCollection.reproject_points(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n\n              ```\n        \"\"\"\n        # Proj gives a future warning however the from_epsg argument to the functiuon\n        # is correct the following couple of code lines are to disable the warning\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n            from_epsg = \"epsg:\" + str(from_epsg)\n            inproj = Proj(init=from_epsg)  # GCS geographic coordinate system\n            to_epsg = \"epsg:\" + str(to_epsg)\n            outproj = Proj(init=to_epsg)  # WGS84 web mercator\n\n        x = np.ones(len(lat)) * np.nan\n        y = np.ones(len(lat)) * np.nan\n\n        for i in range(len(lat)):\n            x[i], y[i] = np.round(\n                transform(inproj, outproj, lon[i], lat[i], always_xy=True), precision\n            )\n\n        return y, x\n\n    @staticmethod\n    def reproject_points2(\n        lat: list, lng: list, from_epsg: int = 4326, to_epsg: int = 3857\n    ) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"reproject_points.\n\n        This function changes the projection of the coordinates from one coordinate system to another\n        (default: from GCS to Web Mercator used by Google Maps).\n\n        Args:\n            lat (list):\n                List of latitudes of the points.\n            lng (list):\n                List of longitudes of the points.\n            from_epsg (int):\n                EPSG code of the source projection (https://epsg.io/).\n            to_epsg (int):\n                EPSG code of the target projection (https://epsg.io/).\n\n        Returns:\n            tuple[list, list]:\n                x coordinates list, y coordinates list of the points.\n\n        Examples:\n            - From Web Mercator to GCS WGS84:\n\n              ```python\n              &gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n              &gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n              &gt;&gt;&gt; longs, lats = FeatureCollection.reproject_points2(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n\n              ```\n        \"\"\"\n        source = osr.SpatialReference()\n        source.ImportFromEPSG(from_epsg)\n\n        target = osr.SpatialReference()\n        target.ImportFromEPSG(to_epsg)\n\n        transform = osr.CoordinateTransformation(source, target)\n        x = []\n        y = []\n        for i in range(len(lat)):\n            point = ogr.CreateGeometryFromWkt(\n                \"POINT (\" + str(lng[i]) + \" \" + str(lat[i]) + \")\"\n            )\n            point.Transform(transform)\n            x.append(point.GetPoints()[0][0])\n            y.append(point.GetPoints()[0][1])\n        return x, y\n\n    def center_point(\n        self,\n    ) -&gt; GeoDataFrame:\n        \"\"\"Center Point.\n\n        Center Point function takes a geodata frame of polygons and returns the center of each polygon\n\n        Returns:\n            saveIng the shapefile or CenterPointDataFrame :\n                If you choose True in the \"save\" input the function will save the shapefile in the given \"savePath\"\n                If you choose False in the \"save\" input the function will return a [geodataframe] dataframe\n                containing CenterPoint DataFrame you can save it as a shapefile using\n                CenterPointDataFrame.to_file(\"Anyname.shp\")\n\n\n        Examples:\n            - Return a geodata frame\n            ```python\n            &gt;&gt;&gt; sub_basins = gpd.read_file(\"inputs/sub_basins.shp\")\n            &gt;&gt;&gt; CenterPointDataFrame = FeatureCollection.polygon_center_point(sub_basins, save=False)\n\n            ```\n            - save a shapefile\n            ```python\n            &gt;&gt;&gt; sub_basins = gpd.read_file(\"Inputs/sub_basins.shp\")\n            &gt;&gt;&gt; FeatureCollection.center_point(sub_basins, save=True, save_path=\"centerpoint.shp\")\n\n            ```\n        \"\"\"\n        # get the X, Y coordinates of the points of the polygons and the multipolygons\n        self.xy()\n        poly = self.feature\n        # calculate the average X &amp; Y coordinate for each geometry in the shapefile\n        for i, row_i in poly.iterrows():\n            poly.loc[i, \"avg_x\"] = np.mean(row_i[\"x\"])\n            poly.loc[i, \"avg_y\"] = np.mean(row_i[\"y\"])\n\n        coords_list = zip(poly[\"avg_x\"].tolist(), poly[\"avg_y\"].tolist())\n        poly[\"center_point\"] = FeatureCollection.create_point(coords_list)\n\n        return poly\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.feature","title":"<code>feature</code>  <code>property</code>","text":"<p>Geodataframe or DataSource.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.epsg","title":"<code>epsg</code>  <code>property</code>","text":"<p>EPSG number.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.total_bounds","title":"<code>total_bounds</code>  <code>property</code>","text":"<p>Bounding coordinates <code>min-x</code>, <code>min-y</code>, <code>max-x</code>, <code>maxy</code>.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.top_left_corner","title":"<code>top_left_corner</code>  <code>property</code>","text":"<p>Top left corner coordinates.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.layers_count","title":"<code>layers_count</code>  <code>property</code>","text":"<p>layers_count.</p> <p>Number of layers in a datasource.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.layer_names","title":"<code>layer_names</code>  <code>property</code>","text":"<p>OGR object layers names'.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.column","title":"<code>column</code>  <code>property</code>","text":"<p>Column Names.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.file_name","title":"<code>file_name</code>  <code>property</code>","text":"<p>Get file name in case of the base object is an ogr.Datasource or gdal.Dataset.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.dtypes","title":"<code>dtypes</code>  <code>property</code>","text":"<p>Data Type.</p> <pre><code>- Get the data types of the columns in the vector file.\n</code></pre> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>list of the data types (strings/numpy datatypes) of the columns in the vector file, except the geometry</p> <code>Dict[str, str]</code> <p>column.</p>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.__init__","title":"<code>__init__(gdf)</code>","text":"<p>Create a FeatureCollection object.</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def __init__(self, gdf: Union[GeoDataFrame, DataSource]):\n    \"\"\"Create a FeatureCollection object.\"\"\"\n    # read the drivers catalog\n    self._feature = gdf\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.__str__","title":"<code>__str__()</code>","text":"<p>str.</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def __str__(self):\n    \"\"\"__str__.\"\"\"\n    message = f\"\"\"\n        Feature: {self.feature}\n    \"\"\"\n    # EPSG: {self.epsg}\n    # Variables: {self.variables}\n    # Number of Bands: {self.band_count}\n    # Band names: {self.band_names}\n    # Dimension: {self.rows * self.columns}\n    # Mask: {self._no_data_value[0]}\n    # Data type: {self.dtype[0]}\n    return message\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.read_file","title":"<code>read_file(path)</code>  <code>classmethod</code>","text":"<p>Open a vector dataset using OGR or GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to vector file.</p> required <p>Returns:</p> Name Type Description <code>FeatureCollection</code> <code>FeatureCollection</code> <p>A FeatureCollection wrapping the GeoDataFrame.</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@classmethod\ndef read_file(cls, path: str) -&gt; \"FeatureCollection\":\n    \"\"\"Open a vector dataset using OGR or GeoPandas.\n\n    Args:\n        path (str): Path to vector file.\n\n    Returns:\n        FeatureCollection: A FeatureCollection wrapping the GeoDataFrame.\n    \"\"\"\n    gdf = gpd.read_file(path)\n\n    # update = False if read_only else True\n    # ds = ogr.OpenShared(path, update=update)\n    # # ds = gdal.OpenEx(path)\n    return cls(gdf)\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.create_ds","title":"<code>create_ds(driver='geojson', path=None)</code>  <code>staticmethod</code>","text":"<p>Create OGR DataSource.</p> <p>Parameters:</p> Name Type Description Default <code>driver</code> <code>str</code> <p>Driver type [\"GeoJSON\", \"memory\"].</p> <code>'geojson'</code> <code>path</code> <code>str</code> <p>Path to save the vector data.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataSource, None]</code> <p>DataSource | None: Created OGR DataSource or None if inplace behavior applies elsewhere.</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef create_ds(driver: str = \"geojson\", path: str = None) -&gt; Union[DataSource, None]:\n    \"\"\"Create OGR DataSource.\n\n    Args:\n        driver (str): Driver type [\"GeoJSON\", \"memory\"].\n        path (str): Path to save the vector data.\n\n    Returns:\n        DataSource | None:\n            Created OGR DataSource or None if inplace behavior applies elsewhere.\n    \"\"\"\n    driver = driver.lower()\n    gdal_name = CATALOG.get_gdal_name(driver)\n\n    if gdal_name is None:\n        raise DriverNotExistError(f\"The given driver:{driver} is not suported.\")\n\n    if driver == \"memory\":\n        path = \"memData\"\n\n    ds = FeatureCollection._create_driver(gdal_name, path)\n    return ds\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.to_file","title":"<code>to_file(path, driver='geojson')</code>","text":"<p>Save FeatureCollection to disk.</p> <pre><code>Currently, saves OGR DataSource to disk.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the vector.</p> required <code>driver</code> <code>str</code> <p>Driver type.</p> <code>'geojson'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def to_file(self, path: str, driver: str = \"geojson\") -&gt; None:\n    \"\"\"Save FeatureCollection to disk.\n\n        Currently, saves OGR DataSource to disk.\n\n    Args:\n        path (str):\n            Path to save the vector.\n        driver (str):\n            Driver type.\n\n    Returns:\n        None\n    \"\"\"\n    driver_gdal_name = CATALOG.get_gdal_name(driver)\n    if isinstance(self.feature, DataSource):\n        ogr.GetDriverByName(driver_gdal_name).CopyDataSource(self.feature, path)\n    else:\n        self.feature.to_file(path, driver=driver_gdal_name)\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.to_dataset","title":"<code>to_dataset(cell_size=None, dataset=None, column_name=None)</code>","text":"<p>Covert a vector into raster.</p> <pre><code>- The raster cell values will be taken from the column name given in the vector_filed in the vector file.\n- all the new raster geotransform data will be copied from the given raster.\n- raster and vector should have the same projection\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cell_size</code> <code>int | None</code> <p>Cell size for the new raster. Optional if dataset is provided. Default is None.</p> <code>None</code> <code>dataset</code> <code>Dataset | None</code> <p>Raster object to copy geotransform (projection, rows, columns, location) from. Optional if cell_size is provided. Default is None.</p> <code>None</code> <code>column_name</code> <code>str | List[str] | None</code> <p>Column name(s) in the vector to burn values from. If None, all columns are considered as bands. Default is None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <code>Dataset</code> <p>Single-band raster with vector geometries burned.</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def to_dataset(\n    self,\n    cell_size: Any = None,\n    dataset=None,\n    column_name: Union[str, List[str]] = None,\n) -&gt; \"Dataset\":\n    \"\"\"Covert a vector into raster.\n\n        - The raster cell values will be taken from the column name given in the vector_filed in the vector file.\n        - all the new raster geotransform data will be copied from the given raster.\n        - raster and vector should have the same projection\n\n    Args:\n        cell_size (int | None):\n            Cell size for the new raster. Optional if dataset is provided. Default is None.\n        dataset (Dataset | None):\n            Raster object to copy geotransform (projection, rows, columns, location) from. Optional if cell_size is\n            provided. Default is None.\n        column_name (str | List[str] | None):\n            Column name(s) in the vector to burn values from. If None, all columns are considered as bands.\n            Default is None.\n\n    Returns:\n        Dataset:\n            Single-band raster with vector geometries burned.\n    \"\"\"\n    from pyramids.dataset import Dataset\n\n    if cell_size is None and dataset is None:\n        raise ValueError(\"You have to enter either cell size of Dataset object\")\n\n    # Check EPSG are same, if not reproject vector.\n    ds_epsg = self.epsg\n    if dataset is not None:\n        if dataset.epsg != ds_epsg:\n            raise ValueError(\n                f\"Dataset and vector are not the same EPSG. {dataset.epsg} != {ds_epsg}\"\n            )\n\n    # TODO: this case\n    if dataset is not None:\n        if not isinstance(dataset, Dataset):\n            raise TypeError(\n                \"The second parameter should be a Dataset object (check how to read a raster using the \"\n                \"Dataset module)\"\n            )\n        # if the raster is given, the top left corner of the raster will be taken as the top left corner for\n        # the rasterized polygon\n        xmin, ymax = dataset.top_left_corner\n        no_data_value = (\n            dataset.no_data_value[0]\n            if dataset.no_data_value[0] is not None\n            else np.nan\n        )\n        rows = dataset.rows\n        columns = dataset.columns\n        cell_size = dataset.cell_size\n    else:\n        # if a raster is not given, the xmin and ymax will be taken as the top left corner for the rasterized\n        # polygon.\n        xmin, ymin, xmax, ymax = self.feature.total_bounds\n        no_data_value = Dataset.default_no_data_value\n        columns = int(np.ceil((xmax - xmin) / cell_size))\n        rows = int(np.ceil((ymax - ymin) / cell_size))\n\n    burn_values = None\n    if column_name is None:\n        column_name = self.column\n        column_name.remove(\"geometry\")\n\n    if isinstance(column_name, list):\n        numpy_dtype = self.dtypes[column_name[0]]\n    else:\n        numpy_dtype = self.dtypes[column_name]\n\n    dtype = str(numpy_dtype)\n    attribute = column_name\n\n    # convert the vector to a gdal Dataset (vector but read by gdal.EX)\n    vector_gdal_ex = self._gdf_to_ds(gdal_dataset=True)\n    top_left_corner = (xmin, ymax)\n\n    bands_count = 1 if not isinstance(attribute, list) else len(attribute)\n    dataset_n = Dataset.create(\n        cell_size,\n        rows,\n        columns,\n        dtype,\n        bands_count,\n        top_left_corner,\n        ds_epsg,\n        no_data_value,\n    )\n\n    bands = list(range(1, bands_count + 1))\n    # loop over bands\n    for ind, band in enumerate(bands):\n        rasterize_opts = gdal.RasterizeOptions(\n            bands=[band],\n            burnValues=burn_values,\n            attribute=attribute[ind] if isinstance(attribute, list) else attribute,\n            allTouched=True,\n        )\n        # if the second parameter to the Rasterize function is str, it will be read using gdal.OpenEX inside the\n        # function, so if the second parameter is not str, it should be a dataset, if you try to use ogr.DataSource\n        # it will give an error.\n        # the second parameter can be given as a path, or read the vector using gdal.OpenEX and use it as a\n        # second parameter.\n        _ = gdal.Rasterize(\n            dataset_n.raster, vector_gdal_ex.feature, options=rasterize_opts\n        )\n\n    return dataset_n\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.get_epsg_from_prj","title":"<code>get_epsg_from_prj(prj)</code>  <code>staticmethod</code>","text":"<p>Create spatial reference from the projection then auto identify the epsg using the osr object.</p> <p>Parameters:</p> Name Type Description Default <code>prj</code> <code>str</code> <p>Projection string.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>epsg number</p> <p>Examples:</p> <ul> <li>Get EPSG from a dataset projection:</li> </ul> <pre><code>&gt;&gt;&gt; from pyramids.dataset import Dataset\n&gt;&gt;&gt; src = Dataset.read_file(\"path/to/raster.tif\")\n&gt;&gt;&gt; prj = src.GetProjection()\n&gt;&gt;&gt; epsg = FeatureCollection.get_epsg_from_prj(prj)\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef get_epsg_from_prj(prj: str) -&gt; int:\n    \"\"\"Create spatial reference from the projection then auto identify the epsg using the osr object.\n\n    Args:\n        prj (str): Projection string.\n\n    Returns:\n        int: epsg number\n\n    Examples:\n        - Get EPSG from a dataset projection:\n\n          ```python\n          &gt;&gt;&gt; from pyramids.dataset import Dataset\n          &gt;&gt;&gt; src = Dataset.read_file(\"path/to/raster.tif\")\n          &gt;&gt;&gt; prj = src.GetProjection()\n          &gt;&gt;&gt; epsg = FeatureCollection.get_epsg_from_prj(prj)\n\n          ```\n    \"\"\"\n    if prj != \"\":\n        srs = FeatureCollection._create_sr_from_proj(prj)\n        try:\n            # if could not identify epsg use the authority code.\n            response = srs.AutoIdentifyEPSG()\n        except RuntimeError:\n            response = 6\n\n        if response == 0:\n            epsg = int(srs.GetAuthorityCode(None))\n        else:\n            # the GetAuthorityCode failed to identify the epsg number https://gdal.org/doxygen/classOGRSpatialReference.html\n            # srs.FindMatches()\n            epsg = int(srs.GetAttrValue(\"AUTHORITY\", 1))\n    else:\n        epsg = 4326\n    return epsg\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.xy","title":"<code>xy()</code>","text":"<p>Compute x and y coordinates of all vertices.</p> <p>Processes the geometry column of the GeoDataFrame and returns the x and y coordinates of all the vertices.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def xy(self) -&gt; None:\n    \"\"\"Compute x and y coordinates of all vertices.\n\n    Processes the geometry column of the GeoDataFrame and returns the x and y coordinates of all the vertices.\n\n    Returns:\n        None\n    \"\"\"\n    # explode the gdf if the Geometry of type MultiPolygon\n    gdf = self._explode_gdf(self._feature, geometry=\"multipolygon\")\n    gdf = self._explode_gdf(gdf, geometry=\"geometrycollection\")\n    self._feature = gdf\n\n    # get the x &amp; y coordinates of the exploded multi_polygons\n    self._feature[\"x\"] = self._feature.apply(\n        self._get_coords, geom_col=\"geometry\", coord_type=\"x\", axis=1\n    )\n    self._feature[\"y\"] = self._feature.apply(\n        self._get_coords, geom_col=\"geometry\", coord_type=\"y\", axis=1\n    )\n\n    to_delete = np.where(self._feature[\"x\"] == -9999)[0]\n    self._feature.drop(to_delete, inplace=True)\n    self._feature.reset_index(drop=True, inplace=True)\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.create_polygon","title":"<code>create_polygon(coords, wkt=False)</code>  <code>staticmethod</code>","text":"<p>Create a polygon geometry from coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>List[Tuple[float, float]]</code> <p>List of (x, y) tuples.</p> required <code>wkt</code> <code>bool</code> <p>True to return Well-Known Text (WKT) string; False to return a Shapely Polygon object.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[str, Polygon]</code> <p>str | Polygon: WKT string if wkt is True; otherwise a Shapely Polygon object.</p> <p>Examples:</p> <ul> <li>Create a WKT polygon from coordinates and print it:</li> </ul> <pre><code>&gt;&gt;&gt; coordinates = [(-106.64, 24), (-106.49, 24.05), (-106.49, 24.01), (-106.49, 23.98)]\n&gt;&gt;&gt; feature_collection = FeatureCollection.create_polygon(coordinates, wkt=True)\n&gt;&gt;&gt; print(feature_collection)\n'POLYGON ((-106.64 24, -106.49 24.05, -106.49 24.01, -106.49 23.98, -106.64 24))'\n</code></pre> <ul> <li>Create a Shapely Polygon and assign it to a GeoDataFrame:</li> </ul> <pre><code>&gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n&gt;&gt;&gt; new_geometry.loc[0,'geometry'] = FeatureCollection.create_polygon(coordinates, wkt=False)\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef create_polygon(\n    coords: List[Tuple[float, float]], wkt: bool = False\n) -&gt; Union[str, Polygon]:\n    \"\"\"Create a polygon geometry from coordinates.\n\n    Args:\n        coords (List[Tuple[float, float]]):\n            List of (x, y) tuples.\n        wkt (bool):\n            True to return Well-Known Text (WKT) string; False to return a Shapely Polygon object.\n\n    Returns:\n        str | Polygon:\n            WKT string if wkt is True; otherwise a Shapely Polygon object.\n\n    Examples:\n        - Create a WKT polygon from coordinates and print it:\n\n          ```python\n          &gt;&gt;&gt; coordinates = [(-106.64, 24), (-106.49, 24.05), (-106.49, 24.01), (-106.49, 23.98)]\n          &gt;&gt;&gt; feature_collection = FeatureCollection.create_polygon(coordinates, wkt=True)\n          &gt;&gt;&gt; print(feature_collection)\n          'POLYGON ((-106.64 24, -106.49 24.05, -106.49 24.01, -106.49 23.98, -106.64 24))'\n\n          ```\n\n        - Create a Shapely Polygon and assign it to a GeoDataFrame:\n\n          ```python\n          &gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n          &gt;&gt;&gt; new_geometry.loc[0,'geometry'] = FeatureCollection.create_polygon(coordinates, wkt=False)\n\n          ```\n    \"\"\"\n    poly = Polygon(coords)\n    if wkt:\n        return poly.wkt\n    else:\n        return poly\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.create_point","title":"<code>create_point(coords, epsg=None)</code>  <code>staticmethod</code>","text":"<p>Create Shapely Point objects from coordinate tuples.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Iterable[Tuple[float]]</code> <p>List of tuples [(x1, y1), (x2, y2)] or [(lon1, lat1), (lon2, lat1)].</p> required <code>epsg</code> <code>int</code> <p>EPSG number for coordinates. If provided, returns a GeoDataFrame wrapped as FeatureCollection.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[Point], GeoDataFrame]</code> <p>list | FeatureCollection: List of Shapely Point objects, or FeatureCollection if epsg is provided.</p> <p>Examples:</p> <ul> <li>Create points and assign to a GeoDataFrame:</li> </ul> <pre><code>&gt;&gt;&gt; coordinates = [(24.95, 60.16), (24.95, 60.16), (24.95, 60.17), (24.95, 60.16)]\n&gt;&gt;&gt; point_list = FeatureCollection.create_point(coordinates)\n&gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n&gt;&gt;&gt; new_geometry.loc[:, 'geometry'] = point_list\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef create_point(\n    coords: Iterable[Tuple[float]], epsg: int = None\n) -&gt; Union[List[Point], GeoDataFrame]:\n    \"\"\"Create Shapely Point objects from coordinate tuples.\n\n    Args:\n        coords (Iterable[Tuple[float]]):\n            List of tuples [(x1, y1), (x2, y2)] or [(lon1, lat1), (lon2, lat1)].\n        epsg (int):\n            EPSG number for coordinates. If provided, returns a GeoDataFrame wrapped as FeatureCollection.\n\n    Returns:\n        list | FeatureCollection:\n            List of Shapely Point objects, or FeatureCollection if epsg is provided.\n\n    Examples:\n        - Create points and assign to a GeoDataFrame:\n\n          ```python\n          &gt;&gt;&gt; coordinates = [(24.95, 60.16), (24.95, 60.16), (24.95, 60.17), (24.95, 60.16)]\n          &gt;&gt;&gt; point_list = FeatureCollection.create_point(coordinates)\n          &gt;&gt;&gt; new_geometry = gpd.GeoDataFrame()\n          &gt;&gt;&gt; new_geometry.loc[:, 'geometry'] = point_list\n\n          ```\n    \"\"\"\n    points = list(map(Point, coords))\n\n    if epsg is not None:\n        points = gpd.GeoDataFrame(columns=[\"geometry\"], data=points, crs=epsg)\n        points = FeatureCollection(points)\n\n    return points\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.concate","title":"<code>concate(gdf, inplace=False)</code>","text":"<p>Concatenate two shapefiles into one object.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the geometries to combine.</p> required <code>inplace</code> <code>bool</code> <p>If True, modifies the current object in place. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, None]</code> <p>GeoDataFrame | None: New combined GeoDataFrame, or None if inplace is True.</p> <p>Examples:</p> <ul> <li>Concatenate two GeoDataFrames:</li> </ul> <pre><code>&gt;&gt;&gt; subbasins = FeatureCollection.read_file(\"sub-basins.shp\")\n&gt;&gt;&gt; new_sub = gpd.read_file(\"new-sub-basins.shp\")\n&gt;&gt;&gt; all_subs = subbasins.concate(new_sub, new_sub, inplace=False)\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def concate(self, gdf: GeoDataFrame, inplace: bool = False) -&gt; Union[GeoDataFrame, None]:\n    \"\"\"Concatenate two shapefiles into one object.\n\n    Args:\n        gdf (GeoDataFrame):\n            GeoDataFrame containing the geometries to combine.\n        inplace (bool):\n            If True, modifies the current object in place. Default is False.\n\n    Returns:\n        GeoDataFrame | None:\n            New combined GeoDataFrame, or None if inplace is True.\n\n    Examples:\n        - Concatenate two GeoDataFrames:\n\n          ```python\n          &gt;&gt;&gt; subbasins = FeatureCollection.read_file(\"sub-basins.shp\")\n          &gt;&gt;&gt; new_sub = gpd.read_file(\"new-sub-basins.shp\")\n          &gt;&gt;&gt; all_subs = subbasins.concate(new_sub, new_sub, inplace=False)\n\n          ```\n    \"\"\"\n    # concatenate the second shapefile into the first shapefile\n    new_gdf = gpd.GeoDataFrame(pd.concat([self.feature, gdf]))\n    # re-index the data frame\n    new_gdf.index = [i for i in range(len(new_gdf))]\n    # take the spatial reference of the first geodataframe\n    new_gdf.crs = self.feature.crs\n    if inplace:\n        self.__init__(new_gdf)\n        new_gdf = None\n\n    return new_gdf\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.reproject_points","title":"<code>reproject_points(lat, lon, from_epsg=4326, to_epsg=3857, precision=6)</code>  <code>staticmethod</code>","text":"<p>reproject_points.</p> <p>This function changes the projection of coordinates from one coordinate system to another (default: from GCS to Web Mercator as used by Google Maps).</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>list</code> <p>List of latitudes of the points.</p> required <code>lon</code> <code>list</code> <p>List of longitudes of the points.</p> required <code>from_epsg</code> <code>int</code> <p>Reference number of the source projection (https://epsg.io/).</p> <code>4326</code> <code>to_epsg</code> <code>int</code> <p>Reference number of the target projection (https://epsg.io/).</p> <code>3857</code> <code>precision</code> <code>int</code> <p>Number of decimal places.</p> <code>6</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>tuple[list, list]: y coordinates list, x coordinates list of the points.</p> <p>Examples:</p> <ul> <li>From Web Mercator to GCS WGS84:</li> </ul> <pre><code>&gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n&gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n&gt;&gt;&gt;  longs, lats = FeatureCollection.reproject_points(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef reproject_points(\n    lat: list,\n    lon: list,\n    from_epsg: int = 4326,\n    to_epsg: int = 3857,\n    precision: int = 6,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"reproject_points.\n\n    This function changes the projection of coordinates from one coordinate system to another (default: from GCS to Web Mercator as used by Google Maps).\n\n    Args:\n        lat (list):\n            List of latitudes of the points.\n        lon (list):\n            List of longitudes of the points.\n        from_epsg (int):\n            Reference number of the source projection (https://epsg.io/).\n        to_epsg (int):\n            Reference number of the target projection (https://epsg.io/).\n        precision (int):\n            Number of decimal places.\n\n    Returns:\n        tuple[list, list]:\n            y coordinates list, x coordinates list of the points.\n\n    Examples:\n        - From Web Mercator to GCS WGS84:\n\n          ```python\n          &gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n          &gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n          &gt;&gt;&gt;  longs, lats = FeatureCollection.reproject_points(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n\n          ```\n    \"\"\"\n    # Proj gives a future warning however the from_epsg argument to the functiuon\n    # is correct the following couple of code lines are to disable the warning\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n        from_epsg = \"epsg:\" + str(from_epsg)\n        inproj = Proj(init=from_epsg)  # GCS geographic coordinate system\n        to_epsg = \"epsg:\" + str(to_epsg)\n        outproj = Proj(init=to_epsg)  # WGS84 web mercator\n\n    x = np.ones(len(lat)) * np.nan\n    y = np.ones(len(lat)) * np.nan\n\n    for i in range(len(lat)):\n        x[i], y[i] = np.round(\n            transform(inproj, outproj, lon[i], lat[i], always_xy=True), precision\n        )\n\n    return y, x\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.reproject_points2","title":"<code>reproject_points2(lat, lng, from_epsg=4326, to_epsg=3857)</code>  <code>staticmethod</code>","text":"<p>reproject_points.</p> <p>This function changes the projection of the coordinates from one coordinate system to another (default: from GCS to Web Mercator used by Google Maps).</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>list</code> <p>List of latitudes of the points.</p> required <code>lng</code> <code>list</code> <p>List of longitudes of the points.</p> required <code>from_epsg</code> <code>int</code> <p>EPSG code of the source projection (https://epsg.io/).</p> <code>4326</code> <code>to_epsg</code> <code>int</code> <p>EPSG code of the target projection (https://epsg.io/).</p> <code>3857</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>tuple[list, list]: x coordinates list, y coordinates list of the points.</p> <p>Examples:</p> <ul> <li>From Web Mercator to GCS WGS84:</li> </ul> <pre><code>&gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n&gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n&gt;&gt;&gt; longs, lats = FeatureCollection.reproject_points2(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n</code></pre> Source code in <code>pyramids/featurecollection.py</code> <pre><code>@staticmethod\ndef reproject_points2(\n    lat: list, lng: list, from_epsg: int = 4326, to_epsg: int = 3857\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"reproject_points.\n\n    This function changes the projection of the coordinates from one coordinate system to another\n    (default: from GCS to Web Mercator used by Google Maps).\n\n    Args:\n        lat (list):\n            List of latitudes of the points.\n        lng (list):\n            List of longitudes of the points.\n        from_epsg (int):\n            EPSG code of the source projection (https://epsg.io/).\n        to_epsg (int):\n            EPSG code of the target projection (https://epsg.io/).\n\n    Returns:\n        tuple[list, list]:\n            x coordinates list, y coordinates list of the points.\n\n    Examples:\n        - From Web Mercator to GCS WGS84:\n\n          ```python\n          &gt;&gt;&gt; x_coords = [-8418583.96378159, -8404716.499972705]\n          &gt;&gt;&gt; y_coords = [529374.3212213353, 529374.3212213353]\n          &gt;&gt;&gt; longs, lats = FeatureCollection.reproject_points2(y_coords, x_coords, from_epsg=3857, to_epsg=4326)\n\n          ```\n    \"\"\"\n    source = osr.SpatialReference()\n    source.ImportFromEPSG(from_epsg)\n\n    target = osr.SpatialReference()\n    target.ImportFromEPSG(to_epsg)\n\n    transform = osr.CoordinateTransformation(source, target)\n    x = []\n    y = []\n    for i in range(len(lat)):\n        point = ogr.CreateGeometryFromWkt(\n            \"POINT (\" + str(lng[i]) + \" \" + str(lat[i]) + \")\"\n        )\n        point.Transform(transform)\n        x.append(point.GetPoints()[0][0])\n        y.append(point.GetPoints()[0][1])\n    return x, y\n</code></pre>"},{"location":"reference/featurecollection/#pyramids.featurecollection.FeatureCollection.center_point","title":"<code>center_point()</code>","text":"<p>Center Point.</p> <p>Center Point function takes a geodata frame of polygons and returns the center of each polygon</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>saveIng the shapefile or CenterPointDataFrame : If you choose True in the \"save\" input the function will save the shapefile in the given \"savePath\" If you choose False in the \"save\" input the function will return a [geodataframe] dataframe containing CenterPoint DataFrame you can save it as a shapefile using CenterPointDataFrame.to_file(\"Anyname.shp\")</p> <p>Examples:</p> <ul> <li>Return a geodata frame <pre><code>&gt;&gt;&gt; sub_basins = gpd.read_file(\"inputs/sub_basins.shp\")\n&gt;&gt;&gt; CenterPointDataFrame = FeatureCollection.polygon_center_point(sub_basins, save=False)\n</code></pre></li> <li>save a shapefile <pre><code>&gt;&gt;&gt; sub_basins = gpd.read_file(\"Inputs/sub_basins.shp\")\n&gt;&gt;&gt; FeatureCollection.center_point(sub_basins, save=True, save_path=\"centerpoint.shp\")\n</code></pre></li> </ul> Source code in <code>pyramids/featurecollection.py</code> <pre><code>def center_point(\n    self,\n) -&gt; GeoDataFrame:\n    \"\"\"Center Point.\n\n    Center Point function takes a geodata frame of polygons and returns the center of each polygon\n\n    Returns:\n        saveIng the shapefile or CenterPointDataFrame :\n            If you choose True in the \"save\" input the function will save the shapefile in the given \"savePath\"\n            If you choose False in the \"save\" input the function will return a [geodataframe] dataframe\n            containing CenterPoint DataFrame you can save it as a shapefile using\n            CenterPointDataFrame.to_file(\"Anyname.shp\")\n\n\n    Examples:\n        - Return a geodata frame\n        ```python\n        &gt;&gt;&gt; sub_basins = gpd.read_file(\"inputs/sub_basins.shp\")\n        &gt;&gt;&gt; CenterPointDataFrame = FeatureCollection.polygon_center_point(sub_basins, save=False)\n\n        ```\n        - save a shapefile\n        ```python\n        &gt;&gt;&gt; sub_basins = gpd.read_file(\"Inputs/sub_basins.shp\")\n        &gt;&gt;&gt; FeatureCollection.center_point(sub_basins, save=True, save_path=\"centerpoint.shp\")\n\n        ```\n    \"\"\"\n    # get the X, Y coordinates of the points of the polygons and the multipolygons\n    self.xy()\n    poly = self.feature\n    # calculate the average X &amp; Y coordinate for each geometry in the shapefile\n    for i, row_i in poly.iterrows():\n        poly.loc[i, \"avg_x\"] = np.mean(row_i[\"x\"])\n        poly.loc[i, \"avg_y\"] = np.mean(row_i[\"y\"])\n\n    coords_list = zip(poly[\"avg_x\"].tolist(), poly[\"avg_y\"].tolist())\n    poly[\"center_point\"] = FeatureCollection.create_point(coords_list)\n\n    return poly\n</code></pre>"},{"location":"tutorials/datacube-basics/","title":"Tutorial: Datacube basics","text":"<p>This tutorial shows how to construct a simple datacube from a folder of rasters and compute a basic aggregation.</p> <pre><code>from pyramids.datacube import Datacube\n\n# Prepare a folder with a few demo rasters (use repo tests or your own)\nfolder = \"tests\\\\data\\\\geotiff\\\\rhine\"  # adjust as needed\n\n# Parse files and build cube (order: numeric part in filenames)\ndc = Datacube.read_multiple_files(folder, with_order=True, regex_string=r\"\\d+\", date=False)\nprint(dc)\n\n# Open bands into the cube's memory structure (if required by API)\n# dc.open_datacube()\n\n# Example: compute per-pixel mean over time (pseudo-code; see your API methods)\n# mean_arr = dc.mean(axis=0)\n# print(mean_arr.shape)\n</code></pre> <p>Notes: - Ensure the rasters share the same shape and georeferencing. - Use regex and format options to parse timestamps when needed.</p>"},{"location":"tutorials/datacube/","title":"DataCube","text":"<ul> <li>DataCube class is made to operate on multiple single files.</li> <li>DataCube represents a stack of rasters that have the same dimensions (rows &amp; columns).</li> </ul> <p>The datacube object has attributes and methods to help working with multiple raster files, or to repeat the same operation on multiple rasters.</p> <ul> <li>To import the Datacube class:</li> </ul> <pre><code>from pyramids.dataset import Datacube\n</code></pre> <ul> <li>The detailed module attributes and methods are summarized in the following figure.</li> </ul> <p></p>"},{"location":"tutorials/datacube/#attributes","title":"Attributes","text":"<p>The DataCube object has the following attributes:</p> <ol> <li>base: Dataset object</li> <li>columns: number of columns in the dataset</li> <li>rows: number of rows in the dataset</li> <li>time_length: number of files (each file represents a timestamp)</li> <li>shape: (time_length, rows, columns)</li> <li>files: files that have been read</li> </ol> <p></p>"},{"location":"tutorials/datacube/#methods","title":"Methods","text":""},{"location":"tutorials/datacube/#read_multiple_files","title":"read_multiple_files","text":"<ul> <li><code>read_multiple_files</code> parses files in a directory and constructs a 3D array with the 2D dimensions of the first raster and length equal to the number of files.</li> <li>All rasters should have the same dimensions.</li> <li>If you want to read the rasters with a certain order, then all raster file names should have a date that follows the same format (YYYY.MM.DD / YYYY-MM-DD or YYYY_MM_DD), e.g. \"MSWEP_1979.01.01.tif\".</li> </ul> <p>Note:     \u2014 read_multiple_files only parses file names; to open each raster, read a specific band, and add it to the          DataCube you have to do one step further using the open_datacube method.</p>"},{"location":"tutorials/datacube/#parameters","title":"Parameters","text":"<ul> <li>path: str | list \u2014 path of the folder that contains all the rasters, or a list of raster paths</li> <li>with_order: bool \u2014 True if the raster names follow a certain order</li> <li>regex_string: str \u2014 regex string to locate the date in file names; default r\"\\d{4}.\\d{2}.\\d{2}\"</li> <li>date: bool \u2014 True if the number in the file name is a date; default True</li> <li>file_name_data_fmt: str \u2014 date format if files have a date; e.g. \"%Y.%m.%d\"</li> <li>start: str \u2014 start date filter</li> <li>end: str \u2014 end date filter</li> <li>fmt: str \u2014 format of the given date in the start/end parameter</li> <li>extension: str \u2014 file extension to read; default \".tif\"</li> </ul>"},{"location":"tutorials/datacube/#case-with_order-false","title":"Case: with_order = False","text":"<p>If you want to make some mathematical operation on all the rasters, the order of the rasters does not matter.</p> <pre><code>&gt;&gt;&gt; rasters_folder_path = \"examples/data/geotiff/raster-folder\"\n&gt;&gt;&gt; datacube = Datacube.read_multiple_files(rasters_folder_path)\n&gt;&gt;&gt; print(datacube)\n    Files: 6\n    Cell size: 5000.0\n    EPSG: 4647\n    Dimension: 125 * 93\n    Mask: 2147483648.0\n</code></pre>"},{"location":"tutorials/datacube/#case-with_order-true","title":"Case: with_order = True","text":"<p>If the order is important (each raster represents a timestamp): - Each raster must have a date in its file name.</p> <p>Directory contents example:</p> <pre><code>MSWEP_1979.01.01.tif\nMSWEP_1979.01.02.tif\nMSWEP_1979.01.03.tif\nMSWEP_1979.01.04.tif\nMSWEP_1979.01.05.tif\nMSWEP_1979.01.06.tif\n</code></pre> <pre><code>&gt;&gt;&gt; rasters_folder_path = \"examples/data/geotiff/raster-folder\"\n&gt;&gt;&gt; datacube = Datacube.read_multiple_files(\n...     rasters_folder_path,\n...     regex_string=r\"\\d{4}.\\d{2}.\\d{2}\",\n...     date=True,\n...     file_name_data_fmt=\"%Y.%m.%d\",\n... )\n&gt;&gt;&gt; print(datacube)\n     Files: 6\n     Cell size: 5000.0\n     EPSG: 4647\n     Dimension: 125 * 93\n     Mask: 2147483648.0\n</code></pre> <p>If the directory contains files with a number in each file name:</p> <pre><code>0_MSWEP.tif\n1_MSWEP.tif\n2_MSWEP.tif\n3_MSWEP.tif\n4_MSWEP.tif\n</code></pre> <pre><code>rasters_folder_path = \"tests/data/geotiff/rhine\"\ndatacube = Datacube.read_multiple_files(\n    rasters_folder_path, with_order=True, regex_string=r\"\\d+\", date=False,\n)\nprint(datacube)\n# &gt;&gt;&gt;     Files: 3\n# &gt;&gt;&gt;     Cell size: 5000.0\n# &gt;&gt;&gt;     EPSG: 4647\n# &gt;&gt;&gt;     Dimension: 125 * 93\n# &gt;&gt;&gt;     Mask: 2147483648.0\n</code></pre>"},{"location":"tutorials/datacube/#open_datacube","title":"open_datacube","text":"<p>After using read_multiple_files to parse the files in the directory, you can read the values of a specific band from each raster using open_datacube.</p> <pre><code>rasters_folder_path = \"examples/data/geotiff/raster-folder\"\ndatacube = Datacube.read_multiple_files(rasters_folder_path, file_name_data_fmt=\"%Y.%m.%d\", separator=\".\")\ndataset.open_datacube()\nprint(dataset.values.shape)\n# &gt;&gt;&gt;     (6, 125, 93)\n</code></pre>"},{"location":"tutorials/dataset/","title":"dataset","text":"<ul> <li>dataset module contains Two classes <code>Dataset</code> and <code>DataCube</code>.</li> </ul> <p>.. digraph:: Linking</p> <pre><code>dataset -&gt; Dataset;\ndataset -&gt; DataCube;\ndpi=200;\n</code></pre> <ul> <li>Dataset represent a raster object which could be created from reading a geotiff, netcdf, ascii or any file     format/driver supported by gdal.</li> <li>The raster could have single or multi bands.</li> <li> <p>The raster could have different variables (like netcdf file) and these variable can have similar or different     dimensions.</p> </li> <li> <p>DataCube represent a stack of raster's which have the same dimensions, contains data that have same dimensions (rows     &amp; columns).</p> </li> </ul>"},{"location":"tutorials/dataset/#dataset_1","title":"Dataset","text":"<ul> <li>The main purpose of the <code>Dataset</code> object is to deal with raster objects, single or multi-bands, has variables/subsets     like netcdf file or has one variable like most GeoTIFF files.</li> </ul> <ul> <li>The <code>Dataset</code> object data model is as following</li> </ul> <ul> <li>To import the Dataset object</li> </ul> <p><pre><code>from pyramids.dataset import Dataset\n</code></pre> to start exploring the functionality of the dataset module <code>read_file</code> method to read any raster file.</p>"},{"location":"tutorials/dataset/#read_file","title":"read_file","text":"<ul> <li>to read any files using the Dataset object you can use the <code>read_file</code> method.</li> </ul> <p><pre><code>path = \"examples/data/dem/DEM5km_Rhine_burned_fill.tif\"\ndataset = Dataset.read_file(path)\ndataset.plot(title=\"Rhine river basin\", ticks_spacing=500,cmap=\"terrain\", color_scale=1, vmin=0,\n         cbar_label=\"Elevation (m)\")\n</code></pre> </p> <ul> <li>The <code>read_file</code> method detects the type of the input file from the extension at the end of the path.</li> <li> <p>Similarly, you can read an ascii file using the same way.</p> </li> <li> <p>For ASCII files</p> </li> </ul> <p><pre><code>path = \"examples/data/dem/dem5km_rhine.asc\"\ndataset = Dataset.read_file(path)\ndataset.plot(title=\"Rhine river basin\", ticks_spacing=500,cmap=\"terrain\", color_scale=1, vmin=0,\n            cbar_label=\"Elevation (m)\")\n</code></pre> - For netcdf file</p> <pre><code>path = \"examples/data/dem/dem5km_rhine.nc\"\ndataset = Dataset.read_file(path)\ndataset.plot(title=\"Rhine river basin\", ticks_spacing=500,cmap=\"terrain\", color_scale=1, vmin=0,\n            cbar_label=\"Elevation (m)\")\n</code></pre>"},{"location":"tutorials/dataset/#dataset-object-attributes","title":"Dataset object attributes","text":"<ul> <li>The Dataset object has  the following attributes, which enables you to access all the stored data in you raster     file (GeoTIFF/NetCDF/ASCII)</li> </ul>"},{"location":"tutorials/dataset/#raster","title":"raster","text":"<pre><code>print(dataset.raster)\n&lt;osgeo.gdal.Dataset; proxy of &lt;Swig Object of type 'GDALDatasetShadow *' at 0x0000026C8DD51FE0&gt; &gt;\n</code></pre>"},{"location":"tutorials/dataset/#cell_size","title":"cell_size","text":"<pre><code>print(dataset.cell_size)\n&gt;&gt;&gt; 5000.0\n</code></pre>"},{"location":"tutorials/dataset/#values","title":"values","text":"<pre><code>print(dataset.values)\n&gt;&gt;&gt; array([[-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n         ...,\n         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n         [-3.4028235e+38, -3.4028235e+38, -3.4028235e+38, ...,\n         -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]], dtype=float32)\n</code></pre>"},{"location":"tutorials/dataset/#shape","title":"shape","text":"<pre><code>print(dataset.shape)\n(1, 125, 93)\n</code></pre>"},{"location":"tutorials/dataset/#rows","title":"rows","text":"<pre><code>print(dataset.rows)\n&gt;&gt;&gt; 125\n</code></pre>"},{"location":"tutorials/dataset/#columns","title":"columns","text":"<pre><code>print(dataset.columns)\n&gt;&gt;&gt; 93\n</code></pre>"},{"location":"tutorials/dataset/#pivot_point","title":"pivot_point","text":"<ul> <li>The upper left corner of the raster     (minimum lon/x, maximum lat/y).</li> </ul> <pre><code>print(dataset.pivot_point)\n&gt;&gt;&gt; (32239263.70388, 5756081.42235)\n</code></pre>"},{"location":"tutorials/dataset/#geotransform","title":"geotransform","text":"<ul> <li>geotransform data of the upper left corner of the raster     (minimum lon/x, pixel-size, rotation, maximum lat/y, rotation, pixel-size).</li> </ul> <pre><code>print(dataset.geotransform)\n&gt;&gt;&gt; (32239263.70388, 5000.0, 0.0, 5756081.42235, 0.0, -5000.0)\n</code></pre>"},{"location":"tutorials/dataset/#bounds","title":"bounds","text":"<pre><code>print(dataset.bounds)\n&gt;&gt;&gt; 0  POLYGON ((32239263.704 5756081.422, 32239263.7...\n</code></pre> <p><pre><code>dataset.bounds.plot()\n</code></pre> </p>"},{"location":"tutorials/dataset/#bbox","title":"bbox","text":"<pre><code>print(dataset.bbox)\n&gt;&gt;&gt; [32239263.70388, 5131081.42235, 32704263.70388, 5756081.42235]\n</code></pre>"},{"location":"tutorials/dataset/#epsg","title":"epsg","text":"<ul> <li>integer reference number that defines the projection (https://epsg.io/)</li> </ul> <pre><code>print(dataset.epsg)\n&gt;&gt;&gt; 4647\n</code></pre>"},{"location":"tutorials/dataset/#crs","title":"crs","text":"<pre><code>print(dataset.crs)\n&gt;&gt;&gt; 'PROJCS[\"ETRS89 / UTM zone 32N (zE-N)\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",32500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"4647\"]]'\n</code></pre>"},{"location":"tutorials/dataset/#laty","title":"lat/y","text":"<pre><code>dataset.lat or dataset.y\n&gt;&gt;&gt; array([5753581.42235, 5748581.42235, 5743581.42235, 5738581.42235,\n        5733581.42235, 5728581.42235, 5723581.42235, 5718581.42235,\n        5713581.42235, 5708581.42235, 5703581.42235, 5698581.42235,\n        5693581.42235, 5688581.42235, 5683581.42235, 5678581.42235,\n        5673581.42235, 5668581.42235, 5663581.42235, 5658581.42235,\n        5653581.42235, 5648581.42235, 5643581.42235, 5638581.42235,\n        5633581.42235, 5628581.42235, 5623581.42235, 5618581.42235,\n        5613581.42235, 5608581.42235, 5603581.42235, 5598581.42235,\n        5593581.42235, 5588581.42235, 5583581.42235, 5578581.42235,\n        5573581.42235, 5568581.42235, 5563581.42235, 5558581.42235,\n        5553581.42235, 5548581.42235, 5543581.42235, 5538581.42235,\n        5533581.42235, 5528581.42235, 5523581.42235, 5518581.42235,\n        5513581.42235, 5508581.42235, 5503581.42235, 5498581.42235,\n        5493581.42235, 5488581.42235, 5483581.42235, 5478581.42235,\n        5473581.42235, 5468581.42235, 5463581.42235, 5458581.42235,\n        5453581.42235, 5448581.42235, 5443581.42235, 5438581.42235,\n        5433581.42235, 5428581.42235, 5423581.42235, 5418581.42235,\n        5413581.42235, 5408581.42235, 5403581.42235, 5398581.42235,\n        5393581.42235, 5388581.42235, 5383581.42235, 5378581.42235,\n        5373581.42235, 5368581.42235, 5363581.42235, 5358581.42235,\n        5353581.42235, 5348581.42235, 5343581.42235, 5338581.42235,\n        5333581.42235, 5328581.42235, 5323581.42235, 5318581.42235,\n        5313581.42235, 5308581.42235, 5303581.42235, 5298581.42235,\n        5293581.42235, 5288581.42235, 5283581.42235, 5278581.42235,\n        5273581.42235, 5268581.42235, 5263581.42235, 5258581.42235,\n        5253581.42235, 5248581.42235, 5243581.42235, 5238581.42235,\n        5233581.42235, 5228581.42235, 5223581.42235, 5218581.42235,\n        5213581.42235, 5208581.42235, 5203581.42235, 5198581.42235,\n        5193581.42235, 5188581.42235, 5183581.42235, 5178581.42235,\n        5173581.42235, 5168581.42235, 5163581.42235, 5158581.42235,\n        5153581.42235, 5148581.42235, 5143581.42235, 5138581.42235,\n        5133581.42235])\n</code></pre>"},{"location":"tutorials/dataset/#lonx","title":"lon/x","text":"<pre><code>dataset.lon/dataset.x\narray([32241763.70388, 32246763.70388, 32251763.70388, 32256763.70388,\n    32261763.70388, 32266763.70388, 32271763.70388, 32276763.70388,\n    32281763.70388, 32286763.70388, 32291763.70388, 32296763.70388,\n    32301763.70388, 32306763.70388, 32311763.70388, 32316763.70388,\n    32321763.70388, 32326763.70388, 32331763.70388, 32336763.70388,\n    32341763.70388, 32346763.70388, 32351763.70388, 32356763.70388,\n    32361763.70388, 32366763.70388, 32371763.70388, 32376763.70388,\n    32381763.70388, 32386763.70388, 32391763.70388, 32396763.70388,\n    32401763.70388, 32406763.70388, 32411763.70388, 32416763.70388,\n    32421763.70388, 32426763.70388, 32431763.70388, 32436763.70388,\n    32441763.70388, 32446763.70388, 32451763.70388, 32456763.70388,\n    32461763.70388, 32466763.70388, 32471763.70388, 32476763.70388,\n    32481763.70388, 32486763.70388, 32491763.70388, 32496763.70388,\n    32501763.70388, 32506763.70388, 32511763.70388, 32516763.70388,\n    32521763.70388, 32526763.70388, 32531763.70388, 32536763.70388,\n    32541763.70388, 32546763.70388, 32551763.70388, 32556763.70388,\n    32561763.70388, 32566763.70388, 32571763.70388, 32576763.70388,\n    32581763.70388, 32586763.70388, 32591763.70388, 32596763.70388,\n    32601763.70388, 32606763.70388, 32611763.70388, 32616763.70388,\n    32621763.70388, 32626763.70388, 32631763.70388, 32636763.70388,\n    32641763.70388, 32646763.70388, 32651763.70388, 32656763.70388,\n    32661763.70388, 32666763.70388, 32671763.70388, 32676763.70388,\n    32681763.70388, 32686763.70388, 32691763.70388, 32696763.70388,\n    32701763.70388])\n</code></pre>"},{"location":"tutorials/dataset/#band_count","title":"band_count","text":"<pre><code>print(dataset.band_count)\n1\n</code></pre>"},{"location":"tutorials/dataset/#band_names","title":"band_names","text":"<pre><code>print(dataset.band_names)\n['Band_1']\n</code></pre>"},{"location":"tutorials/dataset/#variables","title":"variables","text":"<pre><code>print(dataset.variables)\n{}\n</code></pre>"},{"location":"tutorials/dataset/#no_data_value","title":"no_data_value","text":"<pre><code>print(dataset.no_data_value)\n[-3.4028234663852886e+38]\n</code></pre>"},{"location":"tutorials/dataset/#meta_data","title":"meta_data","text":"<pre><code>print(dataset.meta_data)\n{'Band1#grid_mapping': 'transverse_mercator', 'Band1#long_name': 'GDAL Band Number 1', 'Band1#_FillValue': '-3.4028235e+38', 'NC_GLOBAL#Conventions': 'CF-1.5', 'NC_GLOBAL#GDAL': 'GDAL 3.6.3, released 2023/03/07', 'NC_GLOBAL#GDAL_AREA_OR_POINT': 'Area', 'NC_GLOBAL#history': 'Sun Apr 16 22:17:20 2023: GDAL CreateCopy( examples/data/dem/dem5km_rhine.nc, ... )', 'transverse_mercator#crs_wkt': 'PROJCS[\"ETRS89 / UTM zone 32N (zE-N)\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",32500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"4647\"]]', 'transverse_mercator#false_easting': '32500000', 'transverse_mercator#false_northing': '0', 'transverse_mercator#GeoTransform': '32239263.70388 5000 0 5756081.42235 0 -5000 ', 'transverse_mercator#grid_mapping_name': 'transverse_mercator', 'transverse_mercator#inverse_flattening': '298.257222101004', 'transverse_mercator#latitude_of_projection_origin': '0', 'transverse_mercator#longitude_of_central_meridian': '9', 'transverse_mercator#longitude_of_prime_meridian': '0', 'transverse_mercator#long_name': 'CRS definition', 'transverse_mercator#scale_factor_at_central_meridian': '0.9996', 'transverse_mercator#semi_major_axis': '6378137', 'transverse_mercator#spatial_ref': 'PROJCS[\"ETRS89 / UTM zone 32N (zE-N)\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",32500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"4647\"]]', 'x#long_name': 'x coordinate of projection', 'x#standard_name': 'projection_x_coordinate', 'x#units': 'm', 'y#long_name': 'y coordinate of projection', 'y#standard_name': 'projection_y_coordinate', 'y#units': 'm'}\n</code></pre>"},{"location":"tutorials/dataset/#dtype","title":"dtype","text":"<pre><code>print(dataset.dtype)\n[6]\n</code></pre>"},{"location":"tutorials/dataset/#file_name","title":"file_name","text":"<pre><code>print(dataset.file_name)\n'examples/data/dem/dem5km_rhine.nc'\n</code></pre>"},{"location":"tutorials/dataset/#time_stamp","title":"time_stamp","text":"<pre><code>print(dataset.time_stamp)\n</code></pre>"},{"location":"tutorials/dataset/#driver_type","title":"driver_type","text":"<pre><code>print(dataset.driver_type)\n'geotiff'\n</code></pre>"},{"location":"tutorials/dataset/#color_table","title":"color_table","text":"<pre><code>print(dataset.color_table)\nband values  red green blue alpha\n 0    1      0    0     0    0     0\n 1    1      1  112   153   89   255\n 2    1      2    0     0    0     0\n 3    1      3  242   238  162   255\n 4    1      4    0     0    0     0\n 5    1      5  242   206  133   255\n 6    1      6    0     0    0     0\n 7    1      7  194   140  124   255\n 8    1      8    0     0    0     0\n 9    1      9  214   193  156   255\n</code></pre>"},{"location":"tutorials/dataset/#create-dataset-object","title":"Create Dataset object","text":""},{"location":"tutorials/dataset/#create_from_array","title":"create_from_array","text":"<ul> <li><code>create_from_array</code> method creates a <code>Dataset</code> from a given array and geotransform data     and save the tif file if a Path is given or it will return the gdal.Dataset</li> </ul>"},{"location":"tutorials/dataset/#parameters","title":"Parameters","text":"<pre><code>path : [str], optional\n    Path to save the Raster, if '' is given a memory raster will be returned. The default is ''.\narr : [array], optional\n    numpy array. The default is ''.\ngeo : [list], optional\n    geotransform list [minimum lon, pixel-size, rotation, maximum lat, rotation,\n        pixel-size]. The default is ''.\nnodatavalue : TYPE, optional\n    DESCRIPTION. The default is -9999.\nepsg: [integer]\n    integer reference number to the new projection (https://epsg.io/)\n        (default 3857 the reference no of WGS84 web mercator )\n</code></pre>"},{"location":"tutorials/dataset/#returns","title":"Returns","text":"<pre><code>dst : [gdal.Dataset/save raster to drive].\n    if a path is given the created raster will be saved to drive, if not\n    a gdal.Dataset will be returned.\n</code></pre> <ul> <li>If we take the array we obtained from the <code>read_array</code>, do some arithmetic operation in it, then we created a     <code>gdal.DataSet</code> out of it</li> </ul> <pre><code>src = Raster.createRaster(arr=arr, geo=geo, epsg=str(epsg), nodatavalue=no_data_val)\nMap.plot(src, title=\"Flow Accumulation\")\n</code></pre>"},{"location":"tutorials/dataset/#dataset_like","title":"dataset_like","text":"<ul> <li><code>dataset_like</code> method creates a Geotiff raster like another input raster, new raster will have the same projection,     coordinates or the top left corner of the original raster, cell size, nodata value, and number of rows and columns     the raster and the dem should have the same number of columns and rows</li> </ul>"},{"location":"tutorials/dataset/#parameters_1","title":"Parameters","text":"<pre><code>src : [gdal.dataset]\n    source raster to get the spatial information\narray : [numpy array]\n    to store in the new raster\npath : [String]\n    path to save the new raster including new raster name and extension (.tif)\npixel_type : [integer]\n    type of the data to be stored in the pixels,default is 1 (float32)\n    for example pixel type of flow direction raster is unsigned integer\n    1 for float32\n    2 for float64\n    3 for Unsigned integer 16\n    4 for Unsigned integer 32\n    5 for integer 16\n    6 for integer 32\n</code></pre>"},{"location":"tutorials/dataset/#returns_1","title":"Returns","text":"<pre><code>save the new raster to the given path\n</code></pre> <ul> <li>If we have made some calculation on raster array and we want to save the array back in the raster</li> </ul> <p><pre><code>arr2 = np.ones(shape=arr.shape, dtype=np.float64) * nodataval\narr2[~np.isclose(arr, nodataval, rtol=0.001)] = 5\n\npath = \"examples/data/rasterlike.tif\"\nRaster.rasterLike(src, arr2, path)\n</code></pre> - Now to check the raster that has been saved we can read it again with <code>gda.Open</code></p> <pre><code>dst = gdal.Open(path)\nMap.plot(dst, title=\"Flow Accumulation\", color_scale=1)\n</code></pre> <p></p>"},{"location":"tutorials/dataset/#access-data-methods","title":"Access data methods","text":""},{"location":"tutorials/dataset/#read_array","title":"read_array","text":"<pre><code>arr = Dataset.read_array(src)\nprint(arr)\narray([[-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38,  0.000000e+00,  0.000000e+00,  0.000000e+00,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38,  1.000000e+00,  0.000000e+00,  2.000000e+00,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38, -3.402823e+38,  0.000000e+00,\n     0.000000e+00,  2.000000e+00,  0.000000e+00,  4.000000e+00,\n     0.000000e+00,  0.000000e+00, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38, -3.402823e+38,  0.000000e+00,\n     4.000000e+00,  4.000000e+00,  0.000000e+00,  5.000000e+00,\n     2.000000e+00,  0.000000e+00, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38, -3.402823e+38,  0.000000e+00,\n     0.000000e+00,  1.100000e+01,  0.000000e+00,  0.000000e+00,\n     1.000000e+01,  1.000000e+00, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38, -3.402823e+38,  0.000000e+00,  0.000000e+00,\n     0.000000e+00,  1.500000e+01,  0.000000e+00,  0.000000e+00,\n     0.000000e+00,  1.300000e+01, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38,  0.000000e+00,  1.000000e+00,  1.000000e+00,\n     1.500000e+01,  2.300000e+01,  4.500000e+01,  1.000000e+00,\n     0.000000e+00,  1.500000e+01, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38,  0.000000e+00,  1.000000e+00,  1.100000e+01,\n     6.000000e+00,  0.000000e+00,  2.000000e+00,  4.900000e+01,\n     5.400000e+01,  0.000000e+00,  1.600000e+01,  1.700000e+01,\n     0.000000e+00, -3.402823e+38],\n   [-3.402823e+38,  0.000000e+00,  6.000000e+00,  4.000000e+00,\n     0.000000e+00,  1.000000e+00,  1.000000e+00,  0.000000e+00,\n     0.000000e+00,  5.500000e+01,  1.000000e+00,  2.000000e+00,\n     8.600000e+01, -3.402823e+38],\n   [ 0.000000e+00,  4.000000e+00,  2.000000e+00,  0.000000e+00,\n     0.000000e+00,  0.000000e+00, -3.402823e+38,  0.000000e+00,\n     1.000000e+00,  2.000000e+00,  5.900000e+01,  6.300000e+01,\n     0.000000e+00,  8.800000e+01],\n   [ 0.000000e+00,  1.000000e+00,  1.000000e+00, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38,  0.000000e+00,  1.000000e+00,  0.000000e+00,\n    -3.402823e+38, -3.402823e+38],\n   [-3.402823e+38,  0.000000e+00,  0.000000e+00, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38, -3.402823e+38, -3.402823e+38,\n    -3.402823e+38, -3.402823e+38]], dtype=float32)\n</code></pre>"},{"location":"tutorials/dataset/#blocksize-and-readasarray","title":"Blocksize and ReadAsArray","text":"<ul> <li>The <code>read_array</code> method reads the raster as a numpy array, the method can take a block size to read the raster     in blocks, this is useful when the raster is too large to be read at once.</li> </ul> <p>When you know the block size, you can more effectively plan and execute data processing tasks: - Data Reading/Writing: When reading or writing data, doing so in multiples of the block size can reduce the number of disk accesses required, as each access operation will align with the blocks on disk. - Optimizations: Some formats are optimized for specific block sizes, or for being accessed in certain ways. For example, tiled TIFFs might perform better with square block sizes.</p> <p><pre><code>dataset = Dataset.read_file(\"tests/data/geotiff/era5_land_monthly_averaged.tif\")\narr = dataset.read_array(window=[0, 0, 5, 5])\nprint(arr.shape)\n(5, 5)\n</code></pre> - to get the block size of the dataset</p> <pre><code>print(dataset.blocksize)\n(128, 128)\n</code></pre>"},{"location":"tutorials/dataset/#band-statistics-stats","title":"Band statistics (stats)","text":"<ul> <li>To get a summary statistics (min, max, mean, std) of a band/all bands.</li> <li>The method returns a <code>DataFrame</code> of statistics values of each band, the dataframe has the following columns:     [min, max, mean, std], the index of the dataframe is the band names.</li> </ul> <p><pre><code>era5_image = \"tests/data/geotiff/era5_land_monthly_averaged.tif\"\ndataset = Dataset.read_file(era5_image)\nstats = dataset.stats()\nprint(stats)\n             min         max        mean       std\n  Band_1  270.369720  270.762299  270.551361  0.154270\n  Band_2  269.611938  269.744751  269.673645  0.043788\n  Band_3  273.641479  274.168823  273.953979  0.198447\n  Band_4  273.991516  274.540344  274.310669  0.205754\n</code></pre> - The method can also take a mask (polygon/dataset) to calculate the statistics of the masked area only.</p> <pre><code>era5_image = \"tests/data/geotiff/era5_land_monthly_averaged.tif\"\ndataset = Dataset.read_file(era5_image)\nmask = gpd.read_file(\"tests/data/geotiff/era5-mask.geojson\")\nstats = dataset.stats(mask=mask)\nprint(stats)\n             min         max        mean       std\n  Band_1  270.369720  270.399017  270.384369  0.014648\n  Band_2  269.651001  269.744751  269.697876  0.046875\n  Band_3  273.889526  273.901245  273.895386  0.005859\n  Band_4  274.235657  274.255188  274.245422  0.009766\n</code></pre>"},{"location":"tutorials/dataset/#write-raster-to-disk","title":"Write raster to disk","text":"<p>to write the dataset object to disk using any of the raster formats (GeoTIFF/NetCDF/ASCII), you can use the <code>to_file</code> method.</p> <p></p>"},{"location":"tutorials/dataset/#to_file","title":"to_file","text":"<ul> <li><code>to_file</code> writes the Dataset object to disk.</li> </ul>"},{"location":"tutorials/dataset/#parameters_2","title":"Parameters","text":"<pre><code>path: [str]\n    a path including the name of the raster and extension.\n    &gt;&gt;&gt; path = \"data/cropped.tif\"\ndriver: [str]\n        driver = \"geotiff\"/\"ascii\"/\"netcdf\".\nband: [int]\n    band index, needed only in case of ascii drivers. Default is 0.\n</code></pre>"},{"location":"tutorials/dataset/#geotiff","title":"GeoTIFF","text":"<pre><code>dataset.to_file(\"examples/data/dem/dem5km_rhine.tif\", driver=\"geotiff\")\n</code></pre>"},{"location":"tutorials/dataset/#ascii","title":"ASCII","text":"<p><pre><code>dataset.to_file(\"examples/data/dem/dem5km_rhine.asc\", driver=\"ascii\")\n</code></pre> - The ASCII file will look like</p> <pre><code>ncols         93\nnrows         125\nxllcorner     32239263.70388\nyllcorner     5131081.42235\ncellsize      5000.0\nNODATA_value  -3.4028230607370965e+38\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03  0.03\n</code></pre>"},{"location":"tutorials/dataset/#netcdf","title":"NetCDF","text":"<pre><code>dataset.to_file(\"examples/data/dem/dem5km_rhine.nc\", driver=\"netcdf\")\n</code></pre>"},{"location":"tutorials/dataset/#spatial-properties","title":"Spatial properties","text":""},{"location":"tutorials/dataset/#convert_longitude","title":"convert_longitude","text":"<ul> <li>some files (especially netcdf files) uses longitude values from 0 degrees to 360 degrees, instead of the usual,     GIS-standard, arrangement of -180 degrees to 180 degrees for longitude centered on the Prime Meridian, and -90 degrees     to 90 degrees for latitude centered on the Equator. the <code>convert_longitude</code> method corrects such behavior.</li> </ul> <ul> <li>read the raster files using the <code>read_file</code> and plot it with the <code>plot</code> method.</li> </ul> <p><pre><code>dataset = Dataset.read_file(path)\nfig, ax = dataset.plot(\n    band=0, figsize=(10, 5), title=\"Noah daily Precipitation 1979-01-01\", cbar_label=\"Raindall mm/day\", vmax=30,\n    cbar_length=0.85\n)\n</code></pre> - You cas see how the most left of the map lies the african continent instead of north and south america.</p> <p></p> <ul> <li>To correct the values of the longitude you can use the <code>convert_longitude</code> as follows.</li> </ul> <p><pre><code>new_dataset = dataset.convert_longitude()\nnew_dataset.plot(\n    band=0, figsize=(10, 5), title=\"Noah daily Precipitation 1979-01-01\", cbar_label=\"Raindall mm/day\", vmax=30,\n    cbar_length=0.85\n)\n</code></pre> </p>"},{"location":"tutorials/dataset/#resample","title":"resample","text":"<ul> <li><code>resample</code> reproject a raster to any projection (default the WGS84 web mercator projection, without     resampling) The function returns a GDAL in-memory file object, where you can ReadAsArray etc.</li> </ul>"},{"location":"tutorials/dataset/#parameters_3","title":"Parameters","text":"<pre><code>src : [gdal.Dataset]\n     gdal raster (src=gdal.Open(\"dem.tif\"))\ncell_size : [integer]\n     new cell size to resample the raster. (default empty so raster will not be resampled)\nresample_technique : [String]\n    resampling technique default is \"Nearest\"\n    https://gisgeography.com/raster-resampling/\n    \"Nearest\" for nearest neighbour,\"cubic\" for cubic convolution,\n    \"bilinear\" for bilinear\n</code></pre>"},{"location":"tutorials/dataset/#returns_2","title":"Returns","text":"<pre><code>raster : [gdal.Dataset]\n     gdal object (you can read it by ReadAsArray)\n</code></pre> <pre><code>print(\"Original Cell Size =\" + str(geo[1]))\ncell_size = 100\ndst = Raster.resampleRaster(src, cell_size, resample_technique=\"bilinear\")\n\ndst_arr = Raster.read_array(dst)\n_, new_geo = Raster.getProjectionData(dst)\nprint(\"New cell size is \" + str(new_geo[1]))\nMap.plot(dst, title=\"Flow Accumulation\")\n\nOriginal Cell Size =4000.0\nNew cell size is 100.0\n</code></pre>"},{"location":"tutorials/dataset/#to_crs","title":"to_crs","text":"<ul> <li><code>to_crs</code> re-projects a raster to any projection (default the WGS84 web mercator projection, without resampling)     The function returns a GDAL in-memory file object, where you can ReadAsArray etc.</li> </ul>"},{"location":"tutorials/dataset/#parameters_4","title":"Parameters","text":"<pre><code>to_epsg: [integer]\n    reference number to the new projection (https://epsg.io/)\n    (default 3857 the reference no of WGS84 web mercator )\nmethod: [String]\n    resampling technique default is \"Nearest\"\n    https://gisgeography.com/raster-resampling/\n    \"nearest neighbour\" for nearest neighbour,\"cubic\" for cubic convolution,\n    \"bilinear\" for bilinear\nmaintain_alignment : [bool]\n    True to maintain the number of rows and columns of the raster the same after re-projection. Default is False.\n</code></pre>"},{"location":"tutorials/dataset/#returns_3","title":"Returns","text":"<pre><code>raster:\n    gdal dataset (you can read it by ReadAsArray)\n</code></pre> <pre><code>print(\"current EPSG - \" + str(epsg))\nto_epsg = 4326\ndst = Raster.projectRaster(src, to_epsg=to_epsg, option=1)\nnew_epsg, new_geo = Raster.getProjectionData(dst)\nprint(\"New EPSG - \" + str(new_epsg))\nprint(\"New Geotransform - \" + str(new_geo))\n\ncurrent EPSG - 32618\nNew EPSG - 4326\nNew Geotransform - (-75.60441, 0.03606600000000526, 0.0, 4.704305, 0.0, -0.03606600000000526)\n</code></pre> <ul> <li>Option 2</li> </ul> <pre><code>dst = Raster.projectRaster(src, to_epsg=to_epsg, option=2)\nnew_epsg, new_geo = Raster.getProjectionData(dst)\nprint(\"New EPSG - \" + str(new_epsg))\nprint(\"New Geotransform - \" + str(new_geo))\n\nNew EPSG - 4326\nNew Geotransform - (-75.60441003848668, 0.03611587177268461, 0.0, 4.704560448076901, 0.0, -0.03611587177268461)\n</code></pre>"},{"location":"tutorials/dataset/#crop","title":"crop","text":"<ul> <li>If you have an array and you want to clip/crop it using another raster/array.</li> </ul>"},{"location":"tutorials/dataset/#crop-array-using-a-raster","title":"Crop array using a raster","text":"<ul> <li>The <code>crop</code> method clips/crops (matches the location of nodata value from source raster to destination raster).</li> </ul> <p>Parameters \"\"\"\"\"\"\"\"\"\"     src: [gdal.dataset/np.ndarray]         raster you want to clip/store NoDataValue in its cells         exactly the same like mask raster     mask: [gdal.dataset/np.ndarray]         mask raster to get the location of the NoDataValue and         where it is in the array     mask_noval: [numeric]         in case the mask is np.ndarray, the mask_noval have to be given.</p> <p>Returns \"\"\"\"\"\"\"     dst: [gdal.dataset]         the second raster with NoDataValue stored in its cells exactly the same like src raster</p> <p><pre><code>aligned_raster = \"examples/data/Evaporation_ECMWF_ERA-Interim_mm_daily_2009.01.01.tif\"\nband = 1\ndst = Dataset.Open(aligned_raster)\ndst_arr = dst.read_array()\ndst_nodataval = dst.no_data_value[band - 1]\n\nMap.plot(\n    dst_arr,\n    nodataval=dst_nodataval,\n    title=\"Before Cropping-Evapotranspiration\",\n    color_scale=1,\n    ticks_spacing=0.01,\n)\n</code></pre> </p> <p><pre><code>dst_arr_cropped = Raster.cropAlligned(dst_arr, src)\nMap.plot(\n    dst_arr_cropped,\n    nodataval=nodataval,\n    title=\"Cropped array\",\n    color_scale=1,\n    ticks_spacing=0.01,\n)\n</code></pre> </p>"},{"location":"tutorials/dataset/#crop-raster-using-another-raster-while-preserving-the-alignment","title":"Crop raster using another raster while preserving the alignment","text":"<ul> <li>cropping rasters may  change the alignment of the cells and to keep the alignment during cropping a raster we will     crop the same previous raster but will give the input to the function as a gdal.dataset object.</li> </ul> <pre><code>dst_cropped = Raster.cropAlligned(dst, src)\nMap.plot(dst_cropped, title=\"Cropped raster\", color_scale=1, ticks_spacing=0.01)\n</code></pre>"},{"location":"tutorials/dataset/#crop-raster-using-array","title":"Crop raster using array","text":"<p><pre><code>dst_cropped = Raster.cropAlligned(dst, arr, mask_noval=nodataval)\nMap.plot(dst_cropped, title=\"Cropped array\", color_scale=1, ticks_spacing=0.01)\n</code></pre> </p>"},{"location":"tutorials/dataset/#crop_1","title":"crop","text":"<ul> <li><code>crop</code> method crops a raster using another raster/polygon.</li> </ul> <p>Parameters \"\"\"\"\"\"\"\"\"\"     mask: [Polygon GeoDataFrame/Dataset]             GeodataFrame with a polygon geometry, or a Dataset object.     touch: [bool]         To include the cells that touch the polygon not only those that lie entirely inside the polygon mask.         Default is True.     inplace: [bool]         True to make the changes in place.</p> <p>Returns \"\"\"\"\"\"\"     Dataset:         Dataset object.</p> <p><pre><code>RasterA = gdal.Open(aligned_raster)\nepsg, geotransform = Raster.getProjectionData(RasterA)\nprint(\"Raster EPSG = \" + str(epsg))\nprint(\"Raster Geotransform = \" + str(geotransform))\nMap.plot(RasterA, title=\"Raster to be cropped\", color_scale=1, ticks_spacing=1)\n\nRaster EPSG = 32618\nRaster Geotransform = (432968.1206170588, 4000.0, 0.0, 520007.787999178, 0.0, -4000.0)\n</code></pre> </p> <ul> <li>We will use the soil raster from the previous example as a mask so the projection is different between the raster     and the mask and the cell size is also different</li> </ul> <p><pre><code>dst = Raster.crop(RasterA, soil_raster)\ndst_epsg, dst_geotransform = Raster.getProjectionData(dst)\nprint(\"resulted EPSG = \" + str(dst_epsg))\nprint(\"resulted Geotransform = \" + str(dst_geotransform))\nMap.plot(dst, title=\"Cropped Raster\", color_scale=1, ticks_spacing=1)\n\nresulted EPSG = 32618\nresulted Geotransform = (432968.1206170588, 4000.0, 0.0, 520007.787999178, 0.0, -4000.0)\n</code></pre> </p>"},{"location":"tutorials/dataset/#align","title":"align","text":"<ul> <li><code>matchRasterAlignment</code> method matches the coordinate system and the number of of rows &amp; columns between two rasters     alignment_src is the source of the coordinate system, number of rows, number of columns &amp; cell size data_src is the     source of data values in cells the result will be a raster with the same structure like alignment_src but with values     from data_src using Nearest Neighbour interpolation algorithm</li> </ul>"},{"location":"tutorials/dataset/#parameters_5","title":"Parameters","text":"<pre><code>alignment_src : [gdal.dataset/string]\n    spatial information source raster to get the spatial information\n    (coordinate system, no of rows &amp; columns)\ndata_src : [gdal.dataset/string]\n    data values source raster to get the data (values of each cell)\n</code></pre>"},{"location":"tutorials/dataset/#returns_4","title":"Returns","text":"<pre><code>dst : [gdal.dataset]\n    result raster in memory\n</code></pre> <p><pre><code>soil_raster = gdal.Open(soilmappath)\nepsg, geotransform = Raster.getProjectionData(soil_raster)\nprint(\"Before alignment EPSG = \" + str(epsg))\nprint(\"Before alignment Geotransform = \" + str(geotransform))\n# cell_size = geotransform[1]\nMap.plot(soil_raster, title=\"To be aligned\", color_scale=1, ticks_spacing=1)\n\nBefore alignment EPSG = 3116\nBefore alignment Geotransform = (830606.744300001, 30.0, 0.0, 1011325.7178760837, 0.0, -30.0)\n</code></pre> </p> <p><pre><code>soil_aligned = Raster.matchRasterAlignment(src, soil_raster)\nNew_epsg, New_geotransform = Raster.getProjectionData(soil_aligned)\nprint(\"After alignment EPSG = \" + str(New_epsg))\nprint(\"After alignment Geotransform = \" + str(New_geotransform))\nMap.plot(soil_aligned, title=\"After alignment\", color_scale=1, ticks_spacing=1)\n\nAfter alignment EPSG = 32618\nAfter alignment Geotransform = (432968.1206170588, 4000.0, 0.0, 520007.787999178, 0.0, -4000.0)\n</code></pre> </p>"},{"location":"tutorials/dataset/#get_cell_coords","title":"get_cell_coords","text":"<ul> <li><code>getCellCoords</code> returns the coordinates of all cell centres inside the domain (only the cells that         does not have nodatavalue)</li> </ul>"},{"location":"tutorials/dataset/#parameters_6","title":"Parameters","text":"<pre><code>src : [gdal_Dataset]\n    Get the data from the gdal datasetof the DEM\n</code></pre>"},{"location":"tutorials/dataset/#returns_5","title":"Returns","text":"<pre><code>coords : array\n    Array with a list of the coordinates to be interpolated, without the Nan\nmat_range : array\n    Array with all the centres of cells in the domain of the DEM\n</code></pre> <pre><code>coords, centers_coords = Raster.getCellCoords(src)\nprint(coords)\narray([[434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n   [434968.12061706, 520007.78799918],\n\nprint(centers_coords)\narray([[[434968.12061706, 520007.78799918],\n    [438968.12061706, 520007.78799918],\n    [442968.12061706, 520007.78799918],\n    [446968.12061706, 520007.78799918],\n    [450968.12061706, 520007.78799918],\n    [454968.12061706, 520007.78799918],\n    [458968.12061706, 520007.78799918],\n</code></pre> <pre><code>path = \"examples/data/save_raster_test.tif\"\nRaster.saveRaster(src, path)\n</code></pre>"},{"location":"tutorials/dataset/#overlay","title":"overlay","text":"<p>The <code>overlay</code> function takes two ascii files the <code>BaseMap</code> which is the raster/asc file of the polygons and the secon is the asc file you want to extract its values.</p> <pre><code>def overlayMap(\n        path: str,\n        classes_map: Union[str, np.ndarray],\n        exclude_value: Union[float, int],\n        compressed: bool=False,\n        occupied_cells_only: bool=True) -&gt; Tuple[Dict[List[float], List[float]], int]:\n\"\"\"\n\"\"\"overlayMap.\n\n        OverlayMap extracts and return a list of all the values in an ASCII file,\n        if you have two maps one with classes, and the other map contains any type of values,\n        and you want to know the values in each class\n</code></pre>"},{"location":"tutorials/dataset/#parameters_7","title":"Parameters","text":"<p>path: [str]     a path to ascii file. classes_map: [str/array]     a path including the name of the ASCII and extension, or an array     &gt;&gt;&gt; path = \"classes.asc\" exclude_value: [Numeric]     values you want to exclude from extracted values. compressed: [Bool]     if the map you provided is compressed. occupied_cells_only: [Bool]     if you want to count only cells that is not zero.</p>"},{"location":"tutorials/dataset/#returns_6","title":"Returns","text":"<p>ExtractedValues: [Dict]     dictionary with a list of values in the basemap as keys         and for each key a list of all the intersected values in the         maps from the path. NonZeroCells: [dataframe]     the number of cells in the map.</p> <p>To extract the</p> <pre><code>import Hapi.raster as R\n\nPath = \"F:/02Case studies/Hapi Examples/\"\nSavePath  = Path + \"results/ZonalStatistics\"\nBaseMapF = Path + \"data/Polygons.tif\"\nExcludedValue = 0\nCompressed = True\nOccupiedCellsOnly = False\n\nExtractedValues, Cells = R.OverlayMap(Path+\"DepthMax22489.zip\", BaseMapF,ExcludedValue, Compressed,OccupiedCellsOnly)\n</code></pre>"},{"location":"tutorials/dataset/#count_domain_cells","title":"count_domain_cells","text":"<ul> <li>To number of cells in a raster that are not <code>no_data_value</code> value.</li> </ul>"},{"location":"tutorials/dataset/#parameters_8","title":"Parameters","text":"<p>band: [int]     band index. Default is 0.</p>"},{"location":"tutorials/dataset/#returns_7","title":"Returns","text":"<p>int:     Number of cells</p> <p><pre><code>    path = \"examples/data/dem/DEM5km_Rhine_burned_fill.tif\"\n    dataset = Dataset.read_file(path)\n    cells = dataset.count_domain_cells()\n    print(f\"Number of cells = {cells}\")\n\n    Number of cells = 6374\n</code></pre> in case the dataset is a multi-band raster, you can specify the band index.</p> <pre><code>    cells = dataset.count_domain_cells(band=1)\n</code></pre>"},{"location":"tutorials/dataset/#mathematical-operations","title":"Mathematical operations","text":""},{"location":"tutorials/dataset/#apply","title":"apply","text":"<ul> <li><code>apply</code> executes a mathematical operation on raster array and returns the result</li> </ul>"},{"location":"tutorials/dataset/#parameters_9","title":"Parameters","text":"<pre><code>src : [gdal.dataset]\n    source raster to that you want to make some calculation on its values\nfun: [function]\n    defined function that takes one input which is the cell value\n</code></pre>"},{"location":"tutorials/dataset/#returns_8","title":"Returns","text":"<pre><code>Dataset\n    gdal dataset object\n</code></pre> <pre><code>def classify(val):\n    if val &lt; 20:\n        val = 1\n    elif val &lt; 40:\n        val = 2\n    elif val &lt; 60:\n        val = 3\n    elif val &lt; 80:\n        val = 4\n    elif val &lt; 100:\n        val = 5\n    else:\n        val = 0\n    return val\n</code></pre> <pre><code>dst = Raster.mapAlgebra(src, classify)\nMap.plot(dst, title=\"Classes\", color_scale=4, ticks_spacing=1)\n</code></pre>"},{"location":"tutorials/dataset/#fill","title":"fill","text":"<ul> <li><code>fill</code> takes a raster and fill it with one value.</li> </ul>"},{"location":"tutorials/dataset/#parameters_10","title":"Parameters","text":"<pre><code>src : [gdal.dataset]\n    source raster\nval: [numeric]\n    numeric value\nsave_to : [str]\n    path including the extension (.tif)\n</code></pre>"},{"location":"tutorials/dataset/#returns_9","title":"Returns","text":"<pre><code>raster : [saved on disk]\n    the raster will be saved directly to the path you provided.\n</code></pre> <p><pre><code>path = \"examples/data/fill-raster-example.tif\"\nvalue = 20\nRaster.rasterFill(src, value, save_to=path)\n\n\"now the resulted raster is saved to disk\"\ndst = gdal.Open(path)\nMap.plot(dst, title=\"Flow Accumulation\")\n</code></pre> </p>"},{"location":"tutorials/dataset/#nearestneighbour","title":"nearestNeighbour","text":"<ul> <li><code>nearestCell</code> calculates the the indices (row, col) of nearest cell in a given raster to a station coordinate system of     the raster has to be projected to be able to calculate the distance</li> </ul>"},{"location":"tutorials/dataset/#parameters_11","title":"Parameters","text":"<pre><code>Raster: [gdal.dataset]\n    raster to get the spatial information (coordinates of each cell)\nStCoord: [Dataframe]\n    dataframe with two columns \"x\", \"y\" contains the coordinates\n    of each station\n</code></pre>"},{"location":"tutorials/dataset/#returns_10","title":"Returns","text":"<pre><code>StCoord:\n    the same input dataframe with two extra columns \"cellx\",\"celly\"\n</code></pre> <pre><code>points = pd.read_csv(\"examples/data/points.csv\")\nprint(points)\n   id            x            y\n0   1  454795.6728  503143.3264\n1   2  443847.5736  481850.7151\n2   3  454044.6935  481189.4256\n3   4  464533.7067  502683.6482\n4   5  463231.1242  486656.3455\n5   6  487292.5152  478045.5720\n\npoints[\"row\"] = np.nan\npoints[\"col\"] = np.nan\n\npoints.loc[:, [\"row\", \"col\"]] = GC.nearestCell(src, points[[\"x\", \"y\"]][:]).values\nprint(points)\n\n   id            x            y   row   col\n0   1  454795.6728  503143.3264   4.0   5.0\n1   2  443847.5736  481850.7151   9.0   2.0\n2   3  454044.6935  481189.4256   9.0   5.0\n3   4  464533.7067  502683.6482   4.0   7.0\n4   5  463231.1242  486656.3455   8.0   7.0\n5   6  487292.5152  478045.5720  10.0  13.0\n</code></pre>"},{"location":"tutorials/dataset/#plotting","title":"Plotting","text":""},{"location":"tutorials/dataset/#color_table_1","title":"color_table","text":"<ul> <li>The <code>color_table</code> property in the <code>Dataset</code> object can assign a certain symbology to each band in the raster.</li> <li>To assign a certain symbology you have to have to create a <code>Dataframe</code> containing the values and corresponding     colors (hexadecimal number) for each band in the raster.</li> <li>assigning a color_table to the raster file will help when opening the file in GIS software like QGIS or ArcGIS,     the raster will be displayed with the colors you assigned to it.without the need to assign the colors manually.</li> </ul> <p><pre><code>print(df)\n        band  values    color\n&gt;&gt;&gt; 0    1       1  #709959\n&gt;&gt;&gt; 1    1       2  #F2EEA2\n&gt;&gt;&gt; 2    1       3  #F2CE85\n&gt;&gt;&gt; 3    2       1  #C28C7C\n&gt;&gt;&gt; 4    2       2  #D6C19C\n&gt;&gt;&gt; 5    2       3  #D6C19C\n</code></pre> - Assign the DataFrame to the <code>color_table</code> property.</p> <pre><code>dataset.color_table = df\n</code></pre> <ul> <li>When saving the raster to disk, the following file will be created along side the raster file.      <p>.. code:: xml</p> <pre><code>&lt;PAMDataset&gt;\n  &lt;PAMRasterBand band=\"1\"&gt;\n    &lt;ColorInterp&gt;Palette&lt;/ColorInterp&gt;\n    &lt;ColorTable&gt;\n      &lt;Entry c1=\"0\" c2=\"0\" c3=\"0\" c4=\"0\" /&gt;\n      &lt;Entry c1=\"112\" c2=\"153\" c3=\"89\" c4=\"255\" /&gt;\n      &lt;Entry c1=\"0\" c2=\"0\" c3=\"0\" c4=\"0\" /&gt;\n      &lt;Entry c1=\"242\" c2=\"238\" c3=\"162\" c4=\"255\" /&gt;\n      &lt;Entry c1=\"0\" c2=\"0\" c3=\"0\" c4=\"0\" /&gt;\n      &lt;Entry c1=\"242\" c2=\"206\" c3=\"133\" c4=\"255\" /&gt;\n      &lt;Entry c1=\"0\" c2=\"0\" c3=\"0\" c4=\"0\" /&gt;\n      &lt;Entry c1=\"194\" c2=\"140\" c3=\"124\" c4=\"255\" /&gt;\n      &lt;Entry c1=\"0\" c2=\"0\" c3=\"0\" c4=\"0\" /&gt;\n      &lt;Entry c1=\"214\" c2=\"193\" c3=\"156\" c4=\"255\" /&gt;\n    &lt;/ColorTable&gt;\n  &lt;/PAMRasterBand&gt;\n&lt;/PAMDataset&gt;\n</code></pre> <p>.. note::</p> <pre><code>- The values in the `values` column in the `DataFrame` should cover the entir range of  values in the raster.\n- Any value that is not in the `values` column will not be assigned any color.\n</code></pre>"},{"location":"tutorials/dataset/#overviews","title":"Overviews","text":"<p>Overviews are essentially lower resolution versions of a raster dataset. They are used to improve the performance of rendering large raster datasets by providing pyramid layers of the same data at different resolutions. When you view a large image at a small scale (zoomed out), <code>Pyramids</code> can use these overviews instead of the full-resolution data, which speeds up the loading and display of the raster.</p> <p>overviews can be stored in two ways:</p> <pre><code>Internal Overviews:\n    These are overviews that are stored within the same file as the main dataset. Many raster formats support\n    internal overviews.\n\nExternal Overviews:\n    These are overviews stored in separate files, typically with the extension .ovr. External overviews are\n    useful when the raster format does not support internal overviews or when you want to keep the overviews\n    separate from the main dataset file.\n</code></pre> <p>To create an overview for a raster dataset you can use the <code>create_overviews</code> method.</p>"},{"location":"tutorials/dataset/#create_overviews","title":"create_overviews","text":"<p>Parameters \"\"\"\"\"\"\"\"\"\"     resampling_method : str, optional         The resampling method used to create the overviews, by default \"NEAREST\"</p> <pre><code>    possible values are:\n            \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\", \"AVERAGE_MAGPHASE\", \"RMS\",\n            \"BILINEAR\".\noverview_levels : list, optional\n    The overview levels, overview_levels are restricted to the typical power-of-two reduction factors.\n    Default [2, 4, 8, 16, 32]\n</code></pre> <p>Returns \"\"\"\"\"\"\"     internal/external overviews:         The overview (also known as pyramids) could be internal or external depending on the state you read         the dataset with.</p> <pre><code>    - External (.ovr file):\n        If the dataset is read with a`read_only=True` then the overviews' file will be created as an\n        in the same directory of the dataset, with the same name of the dataset and .ovr extension.\n    - Internal:\n        If the dataset is read with a`read_only=False` then the overviews will be created internally in the\n        dataset, and the dataset needs to be saved/flushed to save the new changes to disk.\noverview_count: [list]\n    a list property attribute of the overviews for each band.\n</code></pre> <p><pre><code>path = \"tests\\data\\geotiff\\rhine\\0_Rainfall_MSWEP_mm_daily_1979_1_1.tif\"\ndataset = Dataset.read_file(path)\ndataset.create_overviews(resampling_method=\"nearest\", overview_levels=[2, 4, 8, 16, 32])\n\nprint(dataset.overview_number)\n\n&gt;&gt;&gt; [5]\n</code></pre> The previous code will create 5 overviews for each band in the dataset.</p>"},{"location":"tutorials/dataset/#get_overview","title":"get_overview","text":"<p>To get the overview of a certain band you can use the <code>get_overview</code> method. The overview is a gdal band object.</p> <p>Parameters \"\"\"\"\"\"\"\"\"\"     band : int, optional         The band index, by default 0     overview_index: [int]         index of the overview. Default is 0.</p> <p>Returns \"\"\"\"\"\"\"     gdal.Band         gdal band object</p> <pre><code>overview = dataset.get_overview(band=0, overview_index=0)\nprint(overview)\n\n&gt;&gt;&gt; &lt;osgeo.gdal.Band; proxy of &lt;Swig Object of type 'GDALRasterBandShadow *' at 0x0000020C1F9F9C60&gt; &gt;\n</code></pre>"},{"location":"tutorials/dataset/#recreate_overviews","title":"recreate_overviews","text":"<p>Parameters \"\"\"\"\"\"\"\"\"\"     resampling_method : str, optional         The resampling method used to create the overviews, by default \"nearest\".</p> <pre><code>    possible values are:\n        \"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\", \"MODE\", \"AVERAGE_MAGPHASE\", \"RMS\",\n        \"BILINEAR\".\n</code></pre> <p>Raises \"\"\"\"\"\" ValueError     resampling_method should be one of {\"NEAREST\", \"CUBIC\", \"AVERAGE\", \"GAUSS\", \"CUBICSPLINE\", \"LANCZOS\",     \"MODE\", \"AVERAGE_MAGPHASE\", \"RMS\", \"BILINEAR\"} ReadOnlyError     If the overviews are internal and the Dataset is opened with a read only. Please read the dataset using     read_only=False</p>"},{"location":"tutorials/dataset/#references","title":"References","text":"<p>.. target-notes:: .. <code>Digital-Earth</code>: https://github.com/Serapieum-of-alex/Digital-Earth .. <code>cropAlignedFolder</code>: https://github.com/MAfarrag/pyramids/tree/main/examples/data/crop_aligned_folder</p>"},{"location":"tutorials/raster-basics/","title":"Tutorial: Raster basics","text":"<p>This tutorial demonstrates opening a raster, reading values, and saving to a new file.</p> <p>Prerequisites: install pyramids and have a sample raster (we use a test file path from this repo).</p> <pre><code>from pyramids.dataset import Dataset\n\nsrc = \"tests\\\\data\\\\geotiff\\\\dem.tif\"  # adjust as needed\n\n# Open\nds = Dataset.read_file(src)\nprint(\"Size:\", ds.width, ds.height)\n\n# Read full array\narr = ds.read()\nprint(arr.shape, arr.dtype)\n\n# Save copy\nout = \"tutorial_dem_copy.tif\"\nds.to_file(out)\nprint(\"Saved:\", out)\n</code></pre> <p>Expected outcome: an output GeoTIFF is written and can be opened by common GIS tools.</p>"}]}